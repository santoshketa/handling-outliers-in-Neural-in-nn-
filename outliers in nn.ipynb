{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9NwtCOGGAIw"
   },
   "source": [
    "# Effect of outliers on Neural network's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5hdPG84GAI0"
   },
   "source": [
    "### Why iam doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0djXykEGAI1"
   },
   "source": [
    "\n",
    "> I always wondered how neural network deal with outliers mainly with Relu as activation function because as we know sigmoid in logistic regression is especially used for squashing but relu does have that property of squashing .\n",
    "\n",
    "> So i thought to experiment with NN with relu under various situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4386,
     "status": "ok",
     "timestamp": 1572237513786,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "-L2vfWOpGAI3",
    "outputId": "d09e07bb-aa8b-41b6-de57-51910a33669b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from csv import reader\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLN7Tn8GGAJA"
   },
   "source": [
    "   #### Iam using california housing prediction regression problem \n",
    "   \n",
    "> Data Set Characteristics:\n",
    "\n",
    "Number of Instances:20640\n",
    "\n",
    "Number of Attributes:8 numeric, predictive attributes and the target\n",
    "\n",
    "Attribute Information:\n",
    " \t\n",
    "MedInc: median income in block\n",
    "\n",
    "HouseAge_: median house age in block\n",
    "\n",
    "AveRooms :average number of rooms\n",
    "\n",
    "AveBedrms: average number of bedrooms\n",
    "\n",
    "Population block population\n",
    "\n",
    "AveOccup :average house occupancy\n",
    "\n",
    "Latitude house block latitude\n",
    "\n",
    "Longitude house block longitude \n",
    "\n",
    "https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXDj1v0JGAJB"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "d = fetch_california_housing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3jeybDuGAJG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4320,
     "status": "ok",
     "timestamp": 1572237513801,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "EKvYJmAEGAJJ",
    "outputId": "fb835e33-8b6a-4433-9fbb-949f8124c6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = d['feature_names']\n",
    "print(\"Feature names: {}\\n\".format(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9Dv8vQJGAJM"
   },
   "outputs": [],
   "source": [
    "da=pd.DataFrame(d.data, columns= ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4271,
     "status": "ok",
     "timestamp": 1572237513805,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "UiRWiVADGAJP",
    "outputId": "0ed15ad4-3e09-46dd-8813-4260eef5cacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data : (20640, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  ...  AveOccup  Latitude  Longitude\n",
       "0  8.3252      41.0  6.984127  ...  2.555556     37.88    -122.23\n",
       "1  8.3014      21.0  6.238137  ...  2.109842     37.86    -122.22\n",
       "2  7.2574      52.0  8.288136  ...  2.802260     37.85    -122.24\n",
       "3  5.6431      52.0  5.817352  ...  2.547945     37.85    -122.25\n",
       "4  3.8462      52.0  6.281853  ...  2.181467     37.85    -122.25\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of data :', da.shape)\n",
    "da.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8BpfBGVGAJU"
   },
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0Qg96mgGAJV"
   },
   "source": [
    " Analysis each feature individually by using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6842,
     "status": "ok",
     "timestamp": 1572237516435,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "Pked0T8xGAJX",
    "outputId": "0eda0a8a-79ef-4d59-f2df-493cc8b5c19f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAAM9CAYAAADHAFmqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdfZjdVX3v/feaTBmiUkkgYJGHECvf\nSkOthCJQ1DzR5rSl1lo9QwtibUFSK962Vklt60Nb4ShtrYXKg7XliGXuQlv1tDWahKSAoL0ToRqj\nC04DSQCBwCSeKGRgMuv+Y+94MjEz2cnsmbUn835d11wza/32/Pan10UzzmfWWr9USkGSJEmSJEna\nn67aASRJkiRJkjQ5WCRJkiRJkiSpJRZJkiRJkiRJaolFkiRJkiRJklrSXTuAJEnjZd26dT3ATwHf\nBnZVjiNJnWQa8CPA/zdv3ryB2mFq8GeEJI1qxJ8Tk75IOvroo8vs2bNrx5CkjrNu3bonSymzaueo\n7KeAO2uHkKQO9irgrtohKvFnhCTt3w/8nJj0RdLs2bNZu3Zt7RiS1HFSSptqZ+gA3wY45ZRTOOyw\nw2pnkaSO8eyzz3L//fdD89/JKcqfEZI0gtF+Tkz6IkmSpFHsAjjssMPo6empnUWSOtFU3tLlzwhJ\n2r8f+DnhYduSJEmSJElqiUWSJEmSJEmSWmKRJEmSJEmSpJZYJEmSJEmSJKklFkmSJEmSJElqiUWS\nJEmSJEmSWmKRJEmSJEmSpJZYJEmSJEmSJKklFkmSJEmSJElqiUWSJEmSJEmSWmKRJLXJ+eef//0P\nSZIkSToQ/f39XHHFFWzbtq12FGlUFkmSJEmSJFXW19fHhg0b6Ovrqx1FGpVFktQGe69CclWSJEmS\npFb19/ezatUqSimsXLnSVUnqaN21A0iSpEPb7bffzooVK2rHqG779u0AHHnkkZWT1HXeeeexcOHC\n2jEkqaP09fUxNDQEwNDQEH19fSxdurRyKmnfXJEkSZI0Afr7++nv768dQ5LUgdasWcPg4CAAg4OD\nrF69unIiaWSuSJIkSeNq4cKFrkABli1bBsCVV15ZOYkkqdPMnz+fFStWMDg4SHd3NwsWLKgdSRqR\nRZIkSZKk6iLiauD1wGzgtJzz+r2uvw94/57XIuIs4HpgOvAQcGHO+YmJSy21R29vL6tWrQKgq6uL\n3t7eyomkkbm1TWqD6dOnjzqWJEnSfn0GeDWwae8LEXE6cNae1yKiC7gZeFvO+RTgDuCqiYkqtdfM\nmTNZtGgRKSUWL17MjBkzakeSRmSRJLXB/Pnzh41diipJknRgcs535Zy37D0fET3AtcDeJw/PA3bm\nnO9qjq8D3ji+KaXx09vby6mnnupqJHU8t7ZJbbB7Keqzzz7LYYcd5j/+kiRJ7fNB4Oac80MRsef8\nieyxQinn/GREdEXEzJxzyyfbr1+/fv8vkibIG97wBjZu3Fg7hjQqiySpDWbOnMm5557L7bffzqte\n9SqXokqSJLVBRJwNnAFcMV7vMXfuXHp6esbr9pI0KQ0MDIxYtFskSW1SSqkdQZIk6VDzGuBlwIPN\n1UjHA1+IiF8HNgMn7X5hRBwNDB3IaiRJ0oHzjCSpDfr7+/nSl74EwJ133sm2bdsqJ5IkSZr8cs5X\n5ZyPyznPzjnPBh4Gfjbn/EVgHTA9Is5tvvwy4NZKUSVpyrBIktqgr6+PoaEhAIaGhujr66ucSJIk\naXKJiI9FxMM0Vh2tjIhvjPb6nPMQcBHw8Yh4gMbqpXHbAidJanBrm9QGa9asYXBwEIDBwUFWr17N\n0qV7P1hE0mgi4iFgZ/MD4D055y9ExFnA9cB04CHgwpzzEzUySpLGT875cuDy/bxm9l7ju4HTxjGW\nJGkvrkiS2mD+/PnDxgsWLKgTRJr8fiXn/JPNjy9ERBdwM/C2nPMpwB3AVXUjSpIkSVOXRZLUBmef\nffaw8TnnnFMpiXTImQfszDnf1RxfB7yxYh5JkiRpSnNrm9QGN95447DxDTfcwLXXXlspjTSpfToi\nEnAX8PvAicCm3Rdzzk9GRFdEzDyQp/KM9OhSaSLt2LEDgHXr1lVOIkmSdPAskqQ22LJly7Dx5s2b\nKyWRJrVX5Zy3REQP8FHgGuCf23HjuXPn0tPT045bSQfttttuA2DevHmVk0gwMDBgyS5JOihubZPa\n4EUvetGoY0n7l3Pe0vw8APw18NPAZuCk3a+JiKOBoQNZjSRJkiSpfSySpDYopdSOIE1qEfH8iHhh\n8+sE9AL3AeuA6RFxbvOllwG31kkpSZIkya1tUhs8/vjjw8aPPfZYpSTSpHUs8I8RMQ2YBmwAfivn\nPBQRFwHXR8ThwEPAhfViSpIkSVObRZLUBieccMKwc5JOPPHEimmkySfnvBF4xQjX7gZOm9hEkiRJ\nkvbFrW1SG7zrXe8adSxJkiRJo+nv7+eKK65g27ZttaNIo6pSJKWUPplSeiKl9AOPikgp/W5KqaSU\njq6RTToYc+bM4YQTTgAaq5FOPvnkyokkSZIkTSZ9fX1s2LCBvr6+2lGkUdVakfR3wJK9J1NKJwA/\nQ+MpPdKkcskll9DV1cWll15aO4okSZKkSaS/v59Vq1ZRSmHlypWuSlJHq1IklVLuAPb16Oa/AN4N\n+AgsTTr33HMPpRTuvvvu2lEkSZIkTSJ9fX0MDQ0BMDQ05KokdbSOOSMppfRa4JFSyn+28NpLU0pr\nU0prt27dOgHppNH19/ezcuVKSimsWLHCvyBIkiRJatmaNWsYHBwEYHBwkNWrV1dOJI2sI4qklNLz\ngN8H/qiV15dSbiilnFFKOWPWrFnjG05qQV9fH8899xwAzz33nH9BkCRJktSy+fPn093deKh6d3c3\nCxYsqJxIGllHFEnAS4CTgf9MKT0EHA98NaX0oqqppBbt/ReD22+/vVISSZIkSZNNb28vXV2NX8+7\nurro7e2tnEgaWUcUSaWUr5dSjimlzC6lzAYeBk4vpTxWOZrUkpkzZw4bH3XUUZWSSJIkSZpsZs6c\nyaJFi0gpsXjxYmbMmFE7kjSiKkVSSukW4B4gUkoPp5R+o0YOqV0ef/zxYePHHrMDlSRJktS63t5e\nTj31VFcjqeN113jTUsoF+7k+e4KiSG2RUhp1LEmSJEmjmTlzJldddVXtGNJ+dcTWNmmye8UrXjFs\nfPrpp1dKIkmSJEnS+LFIktrgkUceGXUsSZIkSdKhwCJJaoNHH3102NgiSZIkSZJ0KLJIktrg+c9/\n/rDxC17wgkpJJEmSJEkaPxZJUhs899xzw8bPPvtspSSSJEmSJI0fiySpDaZNmzZs3N1d5YGIkiRJ\nkiSNK4skqQ2eeeaZYeOnn366UhJJkiRJksaPRZIkSZIkSZJaYpEkSZIkSZKkllgkSZIkSZIkqSUW\nSZIkSZIkSWqJRZIkSZIkSZJaYpEkSZIkSZKkllgkSZIkSZIkqSUWSZIkSZIkSWqJRZIkSZIkSZJa\nYpEktcGRRx45bDxjxoxKSSRJkiRJGj8WSVIbbN++fdh427ZtlZJIkiRJkjR+LJIkSZIkSZLUEosk\nSZIkSZIktaS7dgBJkiRJioirgdcDs4HTcs7rI+Io4FPAS4BngQeAt+actza/5yzgemA68BBwYc75\niYlPL0lThyuSJEmSJHWCzwCvBjbtMVeAD+ecI+d8GvBfwFUAEdEF3Ay8Led8CnDH7muSpPHjiiSp\nDaZNm8auXbuGjSVJktS6nPNdABGx51w/sGaPl30ZWNr8eh6wc/f3AdfRWJX0lnGOKklTmiuSpDYY\nGhoadSxJkqSxaa5AWgp8rjl1InusXso5Pwl0RcTMCvEkacpwRZLUBiklSinDxpIkSWqrvwK+C1zT\nzpuuX7++nbeTpEOeRZLUBocddhg7d+4cNpYkSVJ7NA/ifilwfs5599LvzcBJe7zmaGCouR2uZXPn\nzqWnp6dtWSXpUDAwMDBi0e7WNqkN9iyR9jWWJEnSwYmID9E4D+mXcs4De1xaB0yPiHOb48uAWyc6\nnyRNNa5IkiRJklRdRHwM+GXgRcDKiHgKeCOwDLgfuLt5EPeDOefX5ZyHIuIi4PqIOJzGQdsXVgkv\nSVOIRZIkSZKk6nLOlwOX7+PSiIdP5pzvBk4bt1CSpB/g1jZJkiRJkiS1xCJJaoO9n9LmU9skSZIk\nSYciiySpDUopo44lSZIkSToUWCRJkiRJkiSpJRZJkiRJkiRJaolFkiRJkiRJklpikSRJkiRJkqSW\nWCRJkiRJkiSpJRZJkiRJkiRJakmVIiml9MmU0hMppfV7zH0kpfStlNLXUkr/nFI6skY2SZIkSZIk\n7VutFUl/ByzZa24FMLeU8hPA/cCyiQ4lSZIkSZKkkVUpkkopdwD9e819sZQy2Bx+GTh+woNJkiRJ\nkiRpRJ16RtJbgM+PdDGldGlKaW1Kae3WrVsnMJYkSZIkSdLU1XFFUkrpvcAg8OmRXlNKuaGUckYp\n5YxZs2ZNXDhJkiRJkqQprKOKpJTSm4FfAH6tlFIqx5FaNnPmzGHjo446qlISSZIkSZLGT3ftALul\nlJYA7wZeU0p5unYe6UD09w878ounnnqqUhJJkiRJksZPlRVJKaVbgHuASCk9nFL6DeAa4AhgRUrp\nvpTSdTWySZIkSZIkad+qrEgqpVywj+m/mfAgkiRJkiRJalnHbG2TJAkgIt4HvB84Lee8PiLOAq4H\npgMPARfmnJ+ol1CSJEmaujrqsG1J0tQWEacDZwGbmuMu4GbgbTnnU4A7gKvqJZQkSZKmNoskSVJH\niIge4Fpg6R7T84CdOee7muPrgDdOdDZJkiRJDRZJkqRO8UHg5pzzQ3vMnUhzdRJAzvlJoCsiZk5w\nNkmSJEl4RpIkqQNExNnAGcAV43H/9evXj8dtpQOyY8cOANatW1c5iSRJ0sGzSJIkdYLXAC8DHowI\ngOOBLwAfA07a/aKIOBoYyjn3H8jN586dS09PT/vSSgfhtttuA2DevHmVk0gwMDBgyS5JOihubZPa\nYPr06aOOJY0u53xVzvm4nPPsnPNs4GHgZ4GPANMj4tzmSy8Dbq0UU5IkSZryLJKkNnjmmWdGHUs6\nODnnIeAi4OMR8QCNlUvjsv1NkiRJ0v65tU1qg56eHgYGBoaNJR285qqk3V/fDZxWL40kSZKk3VyR\nJLXBniXSvsaSJEmSJB0KLJIkSZIkSZLUEoskSZIkSZIktcQiSZIkSZIkSS2xSJIkSZIkSVJLLJIk\nSZIkSZLUEoskSZIkSZIktcQiSZIkSZIkSS2xSJIkSZIkSVJLLJIkSZIkSZLUEoskSZIkSZIktcQi\nSZIkSZIkSS3prh1AkiRJkiLiauD1wGzgtJzz+ub8KcBNwFHAU8Cbcs4P7O+aJGl8WCRJkjRObrzx\nRjZu3Fg7hjrE7v8Wli1bVjmJOsGcOXO45JJLasfoNJ8B/hK4c6/564Brc843R8SFwPXAwhauSZLG\ngUWSJEnjZOPGjazfkJl2+JG1o6gDDA1OA+CbGx+vnES17dq5vXaEjpRzvgsgIr4/FxHHAKcD5zWn\nbgGuiYhZQBrpWs5560TllqSpxiJJkqRxNO3wI3neSYtqx5DUQZ7etKp2hMnkBOCRnPMugJzzroh4\ntDmfRrnWcpG0fv369qeWpEOYRZIkSZKkKWvu3Ln09PTUjiFJHWVgYGDEot2ntkmSJEnqVFuAF0fE\nNIDm5+Oa86NdkySNE4skSZIkSR0p5/wEcB9wQXPqAuDenPPW0a5NfFJJmjoskiRJkiRVFxEfi4iH\ngeOBlRHxjealy4C3R8T9wNubY1q4JkkaB56RJEmSJKm6nPPlwOX7mP8W8MoRvmfEa5Kk8eGKJEmS\nJEmSJLXEIkmSJEmSJEktsUiSJEmSJElSSyySJEmSJEmS1BKLJEmSJEmSJLXEIkmSJEmSJEktsUiS\nJEmSJElSS6oUSSmlT6aUnkgprd9jbmZKaUVK6YHm5xk1skmSJEmSJGnfaq1I+jtgyV5zVwCrSikv\nBVY1x5IkSZIkSeoQVYqkUsodQP9e068Fbmp+fRPwSxMaSpIkSZIkSaPqpDOSji2lfLv59WPAsSO9\nMKV0aUppbUpp7datWycmnSRJkiRJ0hTXSUXS95VSClBGuX5DKeWMUsoZs2bNmsBkkiRJkiRJU1cn\nFUmPp5R+BKD5+YnKeSRJkiRJkrSHTiqSPgdc3Pz6YuCzFbNIkiRJkiRpL1WKpJTSLcA9QKSUHk4p\n/QZwFXBeSukBYHFzLEmSJEmSpA7RXeNNSykXjHBp0YQGkSRJkiRJUss6aWubJEmSJEmSOphFkiRJ\nkiRJklpikSRJkiRJkqSWWCRJkiRJkiSpJRZJkiRJkiRJaolFkiRJkiRJklpikSRJkiRJkqSWWCRJ\nkiRJkiSpJRZJkiRJkiRJaolFkiRJkiRJklpikSRJkiRJkqSWWCRJkiRJkiSpJRZJkiRJkiRJaolF\nkiRJkiRJklpikSRJkiRJkqSWWCRJkiRJkiSpJRZJkiRJkiRJaolFkiRJkiRJklrSXTuAJEmSpMkv\nIn4HuD3nfF9EnAX8A7AL+NWc8z1100mS2sUVSZIkSZLa4Z3Ag82vrwT+HPgT4KPVEkmS2s4VSZKk\ntoqIE4AX55y/fIDf9xngZGAI+C7w9uZftU8BbgKOAp4C3pRzfqDNsSVJY/fCnPN3IuII4OXA4pzz\nroj4s9rBJEnt44okSVJbRMSJEfEl4FvAyubcr0TEJ1q8xcU555fnnF8BXA18sjl/HXBtzvkU4Frg\n+jZHlyS1x5aIOAfoBe5olkg/TGN7myTpEGGRJElql+uBfwWOAJ5rzq0Azmvlm3PO39lj+EJgKCKO\nAU4HbmnO3wKcHhGz2pJYktROvwfcBrwX+OPm3C8A/1EtkSSp7dzaJklqlzOBn885D0VEgUY5FBEv\nbPUGzdVLPwMkYAlwAvBIznlX8367IuLR5vzWdv8fIEk6eDnnfwOO22v61ubHmEXEL9AoqFLz4wM5\n539yC7QkTSyLJElSuzwO/Chw/+6JiDgV2NzqDXLOv9n8vouAjwB/2I5g69evb8dtDtiOHTuqvK+k\nzrdjxw7WrVtXO0bbRcTzaPwseMFel+4e430T8CngVTnn9RHxE8CXmufr7d4CfXNEXEhjhezCsbyf\nJGlkFkmSpHa5GviXiLgS6I6IC4DfB6460BvlnD8VETcADwMvjohpzdVI02j8tXvLgdxv7ty59PT0\nHGiMMbvttttg69MT/r6SOt8RRxzBvHnzqr3/wMBA20v2iHgTcA3wLPDMHpcKcGIb3mKIxtZngCOB\nbwNH09gCvXsb9S3ANRExK+fsylVJGgcWSZKktsg5fzIingLeSqPouRj4w5zzZ/b3vRHxAmBGznlL\nc3w+0A88AdwHXADc3Px8r78cSFJH+jDw+pzzinbfOOdcIuKNwGcj4ns0zuP7OdqwBbrWqlVJmqws\nkiRJbZNz/izw2YP41ucDt0bE82k83acfOL/5i8NlwE0R8UfANuBNbQssSWqnZ4E143HjiOgGlgGv\nzTl/KSJ+GvgH4KKx3rvWqlVJ6mSjrVy1SJIktUVEvGWESwM0tqh9Oec8sK8X5JwfB84a4dq3gFe2\nJaQkaTz9IfDnEfGBnPOTbb73TwLH5Zy/BNAsk74H7KQNW6AlSa2zSJIktcubgLNpHLr9MHA8cCyw\nFpgNEBGvzTmvrRVwom3bto1dO7fz9KZVtaNI6iC7dm5n27bDascYD/cDHwR+KyJ2zyWg5JynjfHe\nDwPHR0TknHNEvIzGz5gHcAu0JE0oiyRJUrt8A/innPPHdk9ExG8DPwacC7wX+CsaZZMk6dDzKeB/\nAv8vww/bHrOc82MRsRS4LSKGmtNvyTn3uwVakibWmIuklNLJwLdLKTub4+nAsaWUh8Z6b0nSpPKr\nwFF7zX0ceDLn/NsR8RHg9yY+Vj0zZszgsW3P8ryTFtWOIqmDPL1pFTNmzKgdYzwcBfxRzrmMx81z\nzp8GPr2PebdAS9IE6mrDPW6l8SjO3XY15yRJU8vjwPl7zf08jSevARwOPDehiSRJE+lvacPh15Kk\nztaOrW3dpZRndw9KKc+mlA7JTd+SpFFdTuPJa+tpHHJ6AjAXeEPz+itpbG2TJB2azgR+OyLeS+OP\nC9+Xc351nUiSpHZrR5G0NaX0i6WUzwGklF4LtPspDZKkDpdz/mJEzAF+jsYTc/4N+Nec81O7rwNf\nrBhRkjS+bmx+SJIOYe0oki4DPp1SuobGUxm24AF3kjQlNUujT+0eR8SPR8R7cs7vrhhLkjQBcs43\n1c4gSRp/Yy6SSin/BZyVUnpBc/zdMaeSJE1aEXE0jYO3LwZeDiyvm0iSNFEi4tdpnJP0YuAR4FM5\n57+tm0qS1E7teGpbD/B6YDbQnVICoJTywbHeW5I0OUTED9E4aPtiYAmN1anHAWfmnL9aM5skaWI0\nz0Z6E/BnwCbgJODdEXFczvlPq4aTJLVNO7a2fRb4DrAOGBjrzVJK7wR+EyjA14FfL6XsHOt9JUnj\nIyKuBf47jSey3Qq8Juf85Yj4NvBw1XCSpIn0m8D8nPOm3RMR8QXgDsAiSZIOEe0oko4vpSxpw31I\nKb2YxlN/Ti2lPJNS+gegF/i7dtxfkjQuLgP6gfcDfTnn79SNI0mq5PnA1r3mngKmV8giSRon7SiS\n7k4pnVZK+Xob7gWNTNNTSs8BzwMebdN9NU5uv/12VqxYUTtGx1m2bFntCFWcd955LFy4sHYMTayX\n0NjK8HvARyPi34C/B7qqppIkTbTlwKcj4gpgM42tbX8KfKFqKklSW7Xjf+SfC6xLKeWU0tdSSl9P\nKX3tYG5USnkEuJrGD55vA98ppfzAo6JTSpemlNamlNZu3br3Hz0kSRMp5/xQzvmDOecfBX6Gxuqk\nvwFmAX8aEadWDShJmii/DewAvgZ8D/jP5ue31wwlSWqvdqxI+m9tuAcAKaUZwGuBk4HtwK0ppQtL\nKTfv+bpSyg3ADQBnnHFGadf76+AsXLhwyq9AOf/8839g7sorr6yQRKor53wncGdEvB14HfBm4D7g\nsJq5JEnjL+f8f4A3RcSbgaOBJ3POQ3VTSZLa7aBXJKWUZqaUZtL4q8O+Pg7GYuDBUsrWUspzwD8B\n5xxsRmmifPCDwx9S+Cd/8ieVkkidIee8M+d8S875Z2k81VOSNAVExEuBPwD+GPiD5liSdAgZy4qk\ndTSerJb2ca0Acw7inpuBs1JKzwOeARYBaw86oTRBXvGKVwwbv/zlL6+URKonInqAPwIuAI7KOb8w\nIn4GOAW4pmo4SdK4i4jzgU8D/wJsAgJYGxEX5Zw/VzWcJKltDrpIKqWc3M4gzXt+JaV0G/BVYBC4\nl+YWNqnTnXTSSWzatMnVSJrK/gJ4MfBrwOebc99ozlskSdKh70PAa3POq3dPRMR8Gj8DLJIk6RBx\n0EVSSun00a6XUr56MPctpbwPeN9BhZIqOuKII5g7d66rkTSVvQ740Zzz9yJiCCDn/EhEvLhyLknS\nxDgeuHOvubua85KkQ8RYtrb9WfPz4cAZNJ7KkICfoLEd7eyxRZMkTTLPstfPlYiYBTxVJ44kaYLd\nB/wu8D/2mPud5rwk6RAxlq1tCwBSSv8EnF5K+XpzPBd4f1vSSZImk1uBmyLinQAR8SPAR4G+qqkk\nSRNlKfC/IuIdwBbgBOBp4AcfbytJmrQO+qlte4jdJRJAKWU98LI23FeSNLn8PvAg8HXgSOAB4FHg\nAzVDSZImRs75WzR+D3gjjd0LbwRelnP+ZtVgkqS2GsvWtt2+llL6BHBzc/xrwNfacF9J0iSSc34W\neCfwzuaWtidzzqVyLEnSBMo5D9I4FwmAiDgsIt6Wc762YixJUhu1o0j6dRrLWN/RHN8BfLwN95Uk\nTSIRcSrwVM75ceAZ4P3NQ7c/knN+um46SdJ4iohFwE8C/zvn/NmI6AZ+C3gP0A9YJEnSIWLMRVIp\nZWdK6Trg30opuQ2ZJEmT0y00tjE8DlwNBLATuB64qGIuSdI4ioj3AH8IfAP48Yj4a2A+MABcmnP+\n14rxJEltNuYiKaX0i8BHgMOAk1NKPwl8sJTyi2O9tyRpUpmdc84RkYBfBk6lsTLpwbqxJEnj7K3A\na3LO6yLiLOBLwO/mnD9aOZckaRy047Dt9wFnAtsBSin3ASe34b6SpMllZ0QcQeNnwuac85M0/hp9\neN1YkqRxdnTOeR1AzvnLNP7t/8u6kSRJ46UdZyQ9V0r5TkppzzkPV5WkqefvgduBI4BrmnOn44ok\nSTrkNVej7v7Y2Zz7/h+tc85DlaJJktqsHUXSN1JKvwpMSym9FLgcuLsN95UkTSI553dGxM8Az+Wc\nVzenh2g8yU2SdOh6ATC4xzjtMU40/sg8baJDSZLGRzuKpLcD76WxhPUW4AvAH7fhvpKkSSbn/MW9\nxmtrZZEkTRiPtZCkKaQdT217mkaR9N6xx5EkTVYRcScjbG3OOb96guNIkiZIznnTnuPmlrZjc87f\nrhRJkjSODrpISil9brTrPrVNkqacT+w1fhHwG8DNFbJIkiZYRBwJ/DXwK8BzwPMj4heBM3POf1A1\nnCSpbcayIulsYAuN7WxfobH/WZI0ReWcb9p7LiL+Efhb4IMTn0iSNMGuA7YBJwEbmnP3AH8GWCRJ\n0iGia/8vGdGLgN8H5tJ4vOd5wJOllH8vpfx7O8JJkia9R4CfqB1CkjQhFgGXN7e0FYCc81bgmKqp\nJEltddArkkopu4DlwPKUUg9wAbAmpfSBUso1o3+3JOlQExFv2WvqecAvA1+uEEeSNPG+AxwNfP9s\npIg4cc+xJGnyG9Nh280C6edplEizgY8B/zz2WJKkSeiivcbfA+4G/qJCFknSxPsE8I8R8V6gKyLO\nBj5EY8ubJOkQMZbDtv8njW1t/wZ8oJSyvm2pJEmTTs55Qe0MkqSq/gfwDHAt8EPAJ4HraRyDIWk/\n+vv7+fCHP8x73vMeZsyYUTuONKKxrEi6kMZfm98BXJ7S98/aTkAppfzwGLNJkiaZiHgpjVWqL6Zx\nPtItOecH6qaSJE2EnHOhURpZHEkHoa+vjw0bNtDX18fSpUtrx5FGdNCHbZdSukopRzQ/fniPjyMs\nkSRp6omI84F1wI8B/UAAa0w/xMwAACAASURBVJuPfpYkHeIi4j8j4vci4vjaWaTJpr+/n5UrV1JK\nYcWKFWzbtq12JGlEY3lqmyRJe/oQ8Nqc86/mnJflnH8NeG1zXpJ06Hs/8FPAtyLi3yPirRExs3Im\naVLo6+tjcHAQgMHBQfr6+ionkkZmkSRJapfjgTv3mrurOS9JOsTlnP855/xG4EdonI/0OmBLRHyu\nbjKp861evZpSCgClFFavXl05kTQyiyRJUrvcB/zuXnO/05yXJE0ROecdwN8DHwe+Avxc3URS55s1\na9aoY6mTjOWwbUmS9vRbwOci4h3AFuBEGg9l8IwkSZoCIiIBC4FfpbEaaRNwC3BxzVzSZLB169Zh\n4yeeeKJSEmn/LJIkSW2Rc/5mRLwMOAs4DngU+HLOebBuMknSBHkU+C7QB/x0zvmbABHRll0QEXE4\n8BfAYmAncE/O+dKIOAW4CTgKeAp4k08M1WSzYMECPv/5z39/vHDhwopppNFZJEmSxiQi7gTKKNfJ\nOb96AiNJkup4bc75P3YPIuI04E3Ar9H4A8NYfZhGgXRKzrlExLHN+euAa3PON0fEhcD1NFZGSZPG\nkiVLhhVJS5YsqZhGGp1FkiRprD6xx9cJuAZ4W6UskqRKcs7/ERGzaGxtuxh4OY2HLrxjrPeOiBfQ\nKKWOzzmX5vs9HhHHAKcD5zVfegtwTUTMyjlv3ffdpM6zfPlyUkqUUkgpsXz5cpYuXVo7lrRPFkmS\npDHJOd+05zgi/mLvOUnSoSsifojGeXhvBn4W+N80Cp2TgDfknNtx2MtLaGxbe19ELKCxhe4PgGeA\nR3LOuwByzrsi4lHgBKClImn9+vVtiCeNze233z7sqW2rVq3izDPPrJxK2jeLJElSu424zU2SdEh6\nHBgC/g54X875qwAR8VttfI9pwBzg3pzz70XEK4H/BbxhrDeeO3cuPT09Y72NNCYLFy5kxYoVDA4O\n0t3dzaJFi5g3b17tWJrCBgYGRiza23LwnSRJkqQp62vAkcArgZ+KiBnj8B6bgUEaK53IOX8FeJLG\niqQXR8Q0gObn42g8PVSaNHp7e+nqavx63tXVRW9vb+VE0shckSRJGpOI2PtA0+7mtoO0eyLnfPvE\nppIkTZSc8/yIOInGGUbvAj4WEV8Eng/8UJve48mIWE3jLKQvNp/UdgxwP3AfcAFwc/PzvZ6PpMlm\n5syZLFq0iOXLl7N48WJmzBiPPlZqD4skSdJY/c1e46eAT+4xLjS2I0iSDlE5503AHwN/HBHn0iiV\nhoD/jIhP5pzf3Ya3uQz4ZET8GfAccFHOeXtEXAbcFBF/BGxrvrc06fT29rJ582ZXI6njWSRJksYk\n53xy7QySpM6Rc74LuCsiLgdeR5uKnZzzRmD+Pua/RWNbnSRpAnhGkiRJkqS2yznvzDnfknP+b7Wz\nSJNBX18fGzZsoK+vr3YUaVQWSZIkSZIkVdTf38+qVasopbBy5Uq2bdtWO5I0IoskSZIkSZIq6uvr\nY2hoCIChoSFXJamjWSRJkiRJklTRmjVrGBwcBGBwcJDVq1dXTiSNrOOKpJTSkSml21JK30opfTOl\ndHbtTJIkSZIkjZf58+fT3d14FlZ3dzcLFiyonEgaWccVScBfAstLKT8GvBz4ZuU8kiRJkiSNm97e\nXrq6Gr+ed3V10dvbWzmRNLLu2gH2lFJ6IfBq4M0ApZRngWdrZpIkjb+IOAr4FPASGv/uPwC8Nee8\nNSLOAq4HpgMPARfmnJ+olVWSJKndZs6cyaJFi1i+fDmLFy9mxowZtSNJI+q0FUknA1uBv00p3ZtS\n+kRK6fl7vyildGlKaW1Kae3WrVsnPqUkqd0K8OGcc+ScTwP+C7gqIrqAm4G35ZxPAe4ArqqYU5Ik\naVycffbZpJQ455xzakeRRtVpRVI3cDrw8VLKK4DvAVfs/aJSyg2llDNKKWfMmjVrojNKktos59yf\nc16zx9SXgZOAecDOnPNdzfnrgDdOcDxJkqRxd+ONNzI0NMQNN9xQO4o0qk4rkh4GHi6lfKU5vo1G\nsSRJmiKaq5CWAp8DTgQ27b6Wc34S6IqImZXiSZIktd3GjRvZsmULAJs3b+bBBx+snEgaWUedkVRK\neSyltCWlFKWUDCwCNtTOJUmaUH8FfBe4BnhdO264fv36dtzmgO3YsaPK+0rqfDt27GDdunW1Y0jq\nEFdfffUPjK+99tpKaaTRdVSR1PR24NMppcOAjcCvV84jSZogEXE18FLg/JzzUERsprHFbff1o4Gh\nnHP/gdx37ty59PT0tDdsC2677TbY+vSEv6+kznfEEUcwb968au8/MDBQrWSX9IN2r0babfPmzZWS\nSPvXaVvbKKXc1zz/6CdKKb9UStlWO5MkafxFxIdonIn0Sznngeb0OmB6RJzbHF8G3FojnyRJ0ng5\n4YQTho1PPPHESkmk/eu4IkmSNPVExI8Dy4DjgLsj4r6I+Oec8xBwEfDxiHgAeA37eAiDJEnSZPau\nd71r1LHUSTpxa5skaYrJOX8DSCNcuxs4bWITSZIkTZwjjzxy1LHUSVyRJEmSJElSRX19fXR1NX49\n7+rqoq+vr3IiaWQWSZIkSZIkVbRmzRqGhoYAGBoaYvXq1ZUTSSOzSJIkSZIkqaL58+fT3d04eaa7\nu5sFCxZUTiSNzCJJkiRJkqSKent7h21t6+3trZxIGplFkiRJkiRJFc2cOZNFixaRUmLx4sXMmDGj\ndiRpRBZJkiRJkiRVtmTJEqZPn86SJUtqR5FGZZEkSZIkSVJly5cv55lnnmH58uW1o0ij6q4dQJKk\nQ9mundt5etOq2jHUAYYGdwLQ1X145SSqbdfO7cCxtWNI6iD9/f2sWrWKUgorV66kt7fX7W3qWBZJ\nkiSNkzlz5tSOoA6yceNGAObMsUDQsf77IGmYvr4+hoaGABgaGqKvr4+lS5dWTiXtm0WSJEnj5JJL\nLqkdQR1k2bJlAFx55ZWVk0iSOs2aNWsYHBwEYHBwkNWrV1skqWN5RpIkSZIkSRXNnz+f7u7GOo/u\n7m4WLFhQOZE0MoskSZIkSZIq6u3tpaur8et5V1cXvb29lRNJI7NIkiRJkiSpopkzZ7Jo0SJSSixe\nvNiDttXRPCNJkiRJkqTKent72bx5s6uR1PEskiRJkiRJqmzmzJlcddVVtWNI++XWNkmSJEmSJLXE\nIkmSJEmSJEktsUiSJEmSJElSSyySJEmSJEmS1BKLJEmSJEmSKuvv7+eKK65g27ZttaNIo7JIkiRJ\nkiSpsr6+PjZs2EBfX1/tKNKoumsHmOxuvPFGNm7cWDuGOsDu/w6WLVtWOYk6wZw5c7jkkktqx5Ak\nSdIk0N/fz6pVqyilsHLlSnp7e5kxY0btWNI+WSSN0caNG1m/ITPt8CNrR1FlQ4PTAPjmxscrJ1Ft\nu3Zurx1BkiRJk0hfXx9DQ0MADA0N0dfXx9KlSyunkvbNIqkNph1+JM87aVHtGJI6xNObVtWOIEnS\nISsi3ge8Hzgt57w+Is4CrgemAw8BF+acn6iXUDpwa9asYXBwEIDBwUFWr15tkaSO5RlJkiRJkiaF\niDgdOAvY1Bx3ATcDb8s5nwLcAVxVL6F0cObPn09KCYCUEgsWLKicSBqZRZIkSZKkjhcRPcC1wJ7L\nNOYBO3POdzXH1wFvnOhs0lgtWbKEUgoApRSWLFlSOZE0MoskSZIkSZPBB4Gbc84P7TF3Is3VSQA5\n5yeBroiYOcHZpDFZvnz5qGOpk3hGkiRJkqSOFhFnA2cAV7T73uvXr2/3LaUDtmrV8DM2V65cyZln\nnlkpjTQ6iyRJkiRJne41wMuAByMC4HjgC8DHgJN2vygijgaGcs79rd547ty59PT0tDetdICOPfZY\ntmzZ8v3xi170IubNm1cxkaa6gYGBEYt2t7ZJkiRJ6mg556tyzsflnGfnnGcDDwM/C3wEmB4R5zZf\nehlwa6WY0kHbunXrsPETT/jgQXUuiyRJkiRJk1LOeQi4CPh4RDxAY+VS27e/SeNt76e0LVy4sFIS\naf/c2iZJkiRpUmmuStr99d3AafXSSGPX29vL5z//+WFjqVO5IkmSJEmSpIq2b98+6ljqJBZJkiRJ\nkiRV9KEPfWjUsdRJLJIkSZIkSaro8ccfHzZ+7LHHKiWR9s8iSZIkSZIkSS3pyCIppTQtpXRvSulf\nameRJEmSJElSQ0cWScA7gG/WDiFJkiRJ0nibNWvWsPExxxxTKYm0fx1XJKWUjgd+HvhE7SySJEmS\nJI23k046adh49uzZdYJILei4Ign4KPBuYGikF6SULk0prU0prd26devEJZMkSZIkqc3uvffeYeN1\n69ZVSiLtX0cVSSmlXwCeKKWM+v81pZQbSilnlFLO2HsJoCRJkiRJk8nQ0NCoY6mTdFSRBPw08Isp\npYeAPmBhSunmupEkSZIkSRo/XV1do46lTtJR/3WWUpaVUo4vpcwGeoHbSykXVo4lSZIkSdK4efWr\nXz1sPH/+/DpBpBZ0VJEkSZIkSdJU8+Y3v3nY+OKLL64TRGpBd+0AIymlrAHWVI4hSZIkSZKkJlck\nSZIkSZJUUV9f36hjqZNYJEmSJEmSVNHq1auHjW+//fZKSaT9s0iSJEmSJKmiWbNmDRsfc8wxlZJI\n+2eRJEmSJElSRVu3bh11LHWSjj1se7LYtm0bu3Zu5+lNq2pHkdQhdu3czrZth9WOIUmSpEninHPO\nGbad7ZxzzqmYRhqdK5IkSZIkSapo586dw8YDAwOVkkj754qkMZoxYwaPbXuW5520qHYUSR3i6U2r\nmDFjRu0YkiRJmiS+8pWvDBvfc889lZJI++eKJEmSJEmSKtq1a9eoY6mTWCRJkiRJkiSpJRZJkiRJ\nkiRJaolFkiRJkiRJklpikSRJkiRJUkVHHnnksLEPblEns0iSJEmSJKmiWbNmjTqWOkl37QCSJEXE\n1cDrgdnAaTnn9c35U4CbgKOAp4A35ZwfqJVTkiRpPDzwwPD/eXP//fdXSiLtnyuSJEmd4DPAq4FN\ne81fB1ybcz4FuBa4fqKDSZIkSfq/LJIkSdXlnO/KOW/Zcy4ijgFOB25pTt0CnB4RrvWWJEmSKnFr\nmySpU50APJJz3gWQc94VEY8257ceyI3Wr18/DvGkA7Njxw4A1q1bVzmJJKnTnHzyyTz44IPfH8+Z\nM6diGml0FkmSpEPe3Llz6enpqR1DU9xtt90GwLx58yonkWBgYMCSXeogjz766LDxI488UimJtH9u\nbZMkdaotwIsjYhpA8/NxzXlJkqRDxsDAwKhjqZNYJEmSOlLO+QngPuCC5tQFwL055wPa1iZJkiSp\nfSySJEnVRcTHIuJh4HhgZUR8o3npMuDtEXE/8PbmWJIkSVIlnpEkSaou53w5cPk+5r8FvHLiE0mS\nJEnaF1ckSZIkSZJUUUpp1LHUSSySJEmSJEmqqJQy6ljqJBZJkiRJkiRJaolFkiRJkiRJklriYdtt\nsGvndp7etKp2DFU2NLgTgK7uwysnUW27dm4Hjq0dQ5KkQ0ZEHAV8CngJ8CzwAPDWnPPWiDgLuB6Y\nDjwEXJhzfqJWVkk61FkkjdGcOXNqR1CH2LhxIwBz5lgg6Fj/bZAkqb0K8OGc8xqAiPgIcFVEXALc\nDLw553xXRPwBcBXwlmpJJekQZ5E0RpdcckntCOoQy5YtA+DKK6+snESSJOnQknPuB9bsMfVlYCkw\nD9iZc76rOX8djVVJFkmSNE4skiRJkiRNGhHRRaNE+hxwIrBp97Wc85MR0RURM5vl036tX79+fIJK\nY7Ru3braEaR9skiSJEmSNJn8FfBd4BrgdWO92dy5c+np6RlzKKnd5s2bVzuCprCBgYERi3af2iZJ\nkiRpUoiIq4GXAv895zwEbAZO2uP60cBQq6uRJEkHziJJkiRJUseLiA/ROBPpl3LOA83pdcD0iDi3\nOb4MuLVGPkmaKtzaJkmSJKmjRcSPA8uA+4G7IwLgwZzz6yLiIuD6iDicxkHbF1YLKklTgEWSJEmS\npI6Wc/4GkEa4djdw2sQmkqSpy61tkiRJkiRJaolFkiRJkiRJklrSUUVSSumElNLqlNKGlNI3Ukrv\nqJ1JkiRJkiRJDZ12RtIg8LullK+mlI4A1qWUVpRSNtQOJkmSJEmSNNV11IqkUsq3SylfbX69A/gm\n8OK6qSRJkiRJkgQdViTtKaU0G3gF8JV9XLs0pbQ2pbR269atEx1NkiRJkiRpSurIIiml9ALgH4H/\np5Tyf/a+Xkq5oZRyRinljFmzZk18QEmSJEmSpCmo44qklNIP0SiRPl1K+afaeSRJkiRJktTQUUVS\nSikBfwN8s5Ty57XzSJIkSZIk6f/qqCIJ+GngImBhSum+5sfP1Q4lSZIkSZIk6K4dYE+llLuAVDuH\nJEmSJEmSflCnrUiSJEmSJElSh7JIkiRJkiRJUksskiRJkiRJktQSiyRJkiRJkiS1xCJJkiRJkiRJ\nLbFIkiRJkiRJ/z979x5mZ13e+/+9JqMjQsUMJzkX3OTuxgEFPABqzQH8oTVaW6uDG3DrzwOpG9ru\nbZtQD9BaNQb8tSJUUGSXomXtsq2K/mo0Ccm2FAGbCjqk3KAICQchMNGKNoOTWfuPtQYnITN5Zmat\n9ayVvF/XNdea5/muZ617rgvmyXzW93t/pUIMkiRJkiRJklSIQZIkSZIkSZIKMUiSJEmSJElSIQZJ\nkiRJkiRJKqS37AIkSZIkSXuuG2+8kVWrVpVdRse54IILyi6hFKeffjoLFy4suwxNwRlJkiRJkiRJ\nKsQZSZIkSZKk0ixcuHCPn4GyePHip5372Mc+VkIl0q45I0mSJEmSJEmFGCRJkiRJklSir371q1Me\nS53EIEmSJEmSJEmF2CNJkiRJkkrw2c9+lnvvvbfsMtQhnv3sZwNw9NFH77E7tulXjj76aN71rneV\nXcZOGSRJkiRJUgnuvfdehjYkc5713LJLUQcYG50DwL/d+0jJlahs27b+pOwSpmSQJEmSJEkl2LJl\nS9klqIP09D6r7BLUQTr594M9kiRJkiRJklSIQZIkSZIklWDu3Llll6AOMja6lbHRrWWXoQ7Ryb8f\nXNomSZIkSSU4+uijyy5BHWS88frRRx9UciUq30Ed/fvBIEmSJEmSStCpOzKpHG95y1v4xS9+wT77\n7MP73//+ssuRJuXSNkmSJEmSSvaLX/wCgFtuuaXkSqSpGSRJkiRJklSiD3/4w9sdf+QjHympEmnX\nXNomSZIkSSrNjTfeyKpVq8ouo1RDQ0PbHd9yyy1ccMEFJVVTrtNPP52FCxeWXYam4IwkSZIkSZIk\nFeKMJEmSJElSaRYuXLjHz0BZvHjx08597GMfK6ESadcMkjRrTkWtG9+uc0+dgjrOqaiSduR9os77\nRJ33CbVCRMwDrgH2Ax4HzsnMe8qtSpJ2Ty5tk5qkv7+f/v7+ssuQJHUo7xNSS10BXJ6Z84DLgStL\nrkeSdlvOSNKsORVVkjQV7xOSWikiDgROBE5vnLoOuCwiDsjMzeVVJkm7J4MkSZIkSd3scODBzNwG\nkJnbIuKhxvldBkk77pYlleE1r3kNX//61586Xrx4MevXry+xImlyBkmSJEmS9lgDAwP09fWVXYb2\ncCeddNJ2QdK73/3uEquRYGRkZNKg3R5JkqSOFxHzIuLbEXF34/GYsmuSJHWMTcChETEHoPF4SOO8\n1DWWLFkCwHvf+96SK5GmZpAkSeoGNlGVJO1UZj4K3A6c2Th1JvBd+yOp27z2ta/lq1/9KmeccUbZ\npUhTMkiSJHW0CU1Ur2ucug44MSIOKK8qSVKHORc4LyLuBs5rHEuSWsAeSZKkTjerJqpgI1VJ2t1l\n5l3Ay8quQ5L2BAZJkqTdno1UJWl7UzVRlSRpKh23tK1SqZxRqVSyUqn8oFKpLCu7HklS6WyiKkmS\nJHWIjgqSKpXKHOpNVF8DHAucWalUji23KklSmWyiKkmSJHWOjgqSgJcCP6jVavfWarUngSrwhpJr\nkiSVzyaqkiRJUgfotB5Jh7L9UoUH2EnTvEql8m7g3QBHHHFEeyqTJJXGJqqSJElSZ+i0GUmF1Gq1\nz9RqtRfXarUXH3CAuz9LkiRJkiS1Q6cFSQ9S38553GGNc5IkSZIkSSpZpwVJ3wGOqVQqR1UqlWcC\ng8ANJdckSZIkSZIkOqxHUq1WG61UKv8N+AYwB7i6VqvdWXJZkqTuNQfgySefLLsOSeooE34vzimz\njpJ5j5CkSUx1n+ioIAmgVqv9I/CPRZ+/fv36xyqVyv0tLEmajv2Bx8ouQmo4suwCOsDBAHfffXfZ\ndUhSpzoY+GHZRZTEe4Qk7drT7hMdFyRNV61Ws9u2OkalUvmXWq324rLrkPSU7wCvBB4GtpVciyR1\nkjnU/zj4TtmFlMh7hCRNbtL7RKVWq7W/HGk3ZZAkSZIkSdqddVqzbUmSJEmSJHUogySpuT5TdgGS\nJEmSJLWKS9skSZIkSZJUiDOSJEmSJEmSVIhBkiRJkiRJkgoxSJIkSZIkSVIhBkmSJEmSJEkqxCBJ\nkiRJkiRJhRgkSZIkSZIkqRCDJEmSJEmSJBVikCRJkiRJkqRCDJIkSZIkSZJUiEGSJEmSJEmSCjFI\nkiRJkiRJUiEGSZIkSZIkSSrEIEmSJEmSJEmFGCRJkiRJkiSpEIMkSZIkSZIkFWKQJEmSJEmSpEIM\nkiRJkiRJklSIQZIkSZIkSZIKMUiSJEmSJElSIQZJkiRJkiRJKsQgSZIkSZIkSYUYJEmSJEmSJKkQ\ngyRJkiRJkiQVYpAkSZIkSZKkQgySJEmSJEmSVIhBkiRJkiRJkgoxSJIkSZIkSVIhBkmSJEmSJEkq\nxCBJkiRJkiRJhRgkSZIkSZIkqRCDJEmSJEmSJBVikCRJkiRJkqRCDJIkSZIkSZJUiEGSJEmSJEmS\nCjFIkiRJkiRJUiEGSZIkSZIkSSrEIEmSJEmSJEmFGCRJkiRJkiSpEIMkSZIkSZIkFWKQJEmSJEmS\npEIMkiRJkiRJklSIQZIkSZIkSZIKMUiSJEmSJElSIQZJkiRJkiRJKsQgSZIkSZIkSYUYJEmSJEmS\nJKkQgyRJkiRJkiQVYpAkSZIkSZKkQgySJEmSJEmSVIhBkiRJkiRJkgoxSJIkSZIkSVIhBkmSJEmS\nJEkqxCBJkiRJkiRJhRgkSZIkSZIkqRCDJEmSJEmSJBVikCRJkiRJkqRCDJIkSZIkSZJUiEGSJEmS\nJEmSCjFIkiRJkiRJUiEGSZIkSZIkSSrEIEmSJEmSJEmFGCRJkiRJkiSpEIMkSZIkSZIkFWKQJEmS\nJEmSpEIMkiRJkiRJklSIQZIkSZIkSZIKMUiSJEmSJElSIb1lFzAb69ev7wNeAjwMbCu5HEnqJHOA\ng4HvnHTSSSNlF1MW7xOSNKk9/j7hPUKSpjTpfaKrgyTqv/j/qewiJKmDvRK4qewiSuR9QpKmtiff\nJ7xHSNKuPe0+0e1B0sMA8+bN45nPfGbZtUhSx3jyySe5++67ofF7cg/mfUKSdsL7BOA9QpImNdV9\notuDpG0Az3zmM+nr6yu7FknqRHv6VH3vE5I0tT35PuE9QpJ27Wn3CZttS5IkSZIkqZC2zUiKiC8D\nRwFjwBPAeZl5e0TcB2xtfAEszcxvtKsuSZIkSZIkFdPOpW1vy8yfAkTEG4CrgRMbY2/KzKE21iJJ\nkiRJkqRpatvStvEQqWFf6jOTJEmSJEmS1CXa2mw7Iq4CXg1UgDMmDH0hIirUt5T708z8STvrkiRJ\nkiRJ0q61NUjKzHcCRMTZwMXAa4FXZuamiOgD/gq4DDhrOq87NOSqOEmSJEmSpFZra5A0LjOvjYjP\nRMR+mbmpcW4kIv4auGG6rzcwMOCWnZI0wcjIiCG7JEmSpKZrS4+kiNgnIg6fcLwYGAa2RsS+jXMV\nYBC4vR01SZIkSZIkaXraNSNpb+D6iNgb2EY9RFoMHAR8MSLmAHOADcDvt6kmSZIkSZIkTUNbgqTM\nfAQ4eZLhE9pRgyRJkiRJkmanLUvbpD3B8PAwy5YtY8uWLWWXIknqQN4nJElT8T6hbmGQJDVJtVpl\nw4YNVKvVskuRJHUg7xOSpKl4n1C3MEiSmmB4eJg1a9ZQq9VYvXq1nyJIkrbjfUKSNBXvE+omBklS\nE1SrVcbGxgAYGxvzUwRJ0na8T0iSpuJ9Qt3EIElqgnXr1jE6OgrA6Ogoa9euLbkiSVIn8T4hSZqK\n9wl1E4MkqQnmz59Pb299E8Te3l4WLFhQckWSpE7ifUKSNBXvE+omBklSEwwODtLTU//fqaenh8HB\nwZIrkiR1Eu8TkqSpeJ9QN+ktuwBpd9Df38+iRYtYuXIlp512GnPnzi27JKmlIuLLwFHAGPAEcF5m\n3h4R9wFbG18ASzPzG41rTgauBPYC7gPOysxHZzMmdQvvE5KkqXifUDdxRpLUJIODgxx77LF+eqA9\nxdsy84WZeQJwCXD1hLE3ZeaLGl/jIVIP8HngvZk5D/gWsHw2Y1K38T4hSZqK9wl1C2ckSU3S39/P\n8uX+fas9Q2b+dMLhvtRnJk3lJGBrZt7UOL6C+uyid8xiTOoq3ickSVPxPqFuYZAkSZqRiLgKeDVQ\nAc6YMPSFiKgANwF/mpk/AY4A7h9/QmY+FhE9EdE/07HMHC5a69DQ0Mx+SEmSJEnbMUiSJM1IZr4T\nICLOBi4GXgu8MjM3RUQf8FfAZcBZ5VVZNzAwQF9fX9llSFLHGBkZMWSXJM2IPZIkSbOSmdcCCyJi\nv8zc1Dg3Avw18PLG0zYCR45fExH7A2ONWUUzHZMkSZLUZs5IkiRNS0TsA8wdD40iYjEwDGyNiH0z\n86eNpW2DwO2Ny9YDe0XEKxr9js4Frp/lmCSpy0TEJcDvAr8OHJeZQxGxH3At8HzgSeAe4D2Zublx\njTt7SlIHcUaSJGm69gauj4jvR8TtwB8Bi4GDgHUR8T1gCJgH/D5AZo4BZwOfjoh7gFcBy2YzJknq\nSl8GfpMJ/e+AGrAiKfiDpAAAIABJREFUMyMzjwN+iDt7SlLHckaSJGlaMvMR4ORJhk+Y4rqbgeOa\nOSZJ6i7ju3BGxMRzw8C6CU+7BVjS+N6dPSWpwxgkSZIkSeoIjVlGS4AbGqfc2VOSOoxBkiRJkqRO\n8SngCeq7fraFO3tK0tNNtbunPZIkSZIkla7RiPsY4C2NHnngzp6S1HEMkiRJkiSVKiI+Sr3n0W9n\n5siEoad272wc73Rnz52MSZJaxKVtkiRJktoiIi4Ffgd4HrA6Ih4H3gxcANwN3NxoxP2jzHxjZo5F\nxNnAlRHxLOrNtM+C+s6ek41JklrHIEmSJElSW2Tm+cD5OxmqTHGNO3tKUgdxaZskSZIkSZIKMUiS\nJEmSJElSIQZJkiRJkiRJKsQgSZIkSZIkSYUYJEmSJEmSJKkQgyRJkiRJkiQVYpAkSZIkSZKkQgyS\nJEmSJEmSVIhBkiRJkiRJkgoxSJIkSZIkSVIhBkmSJEmSJEkqxCBJkiRJkiRJhRgkSZIkSZIkqRCD\nJEmSJEmSJBVikCRJkiRJkqRCDJIkSZIkSZJUiEGSJEmSJEmSCjFIkiRJkiRJUiEGSZIkSZIkSSqk\nt11vFBFfBo4CxoAngPMy8/aImAdcA+wHPA6ck5n3tKsuSZIkSZIkFdPOGUlvy8wXZuYJwCXA1Y3z\nVwCXZ+Y84HLgyjbWJEmSJEmSpILaFiRl5k8nHO4LjEXEgcCJwHWN89cBJ0bEAe2qS5IkSZIkScW0\nbWkbQERcBbwaqABnAIcDD2bmNoDM3BYRDzXOb25nbZIkSZIkSZpaW4OkzHwnQEScDVwMfLAZrzs0\nNNSMl5EkSZIkSdIU2hokjcvMayPiM8ADwKERMacxG2kOcAiwaTqvNzAwQF9fXytKlaSuNDIyYsgu\nSZIkqena0iMpIvaJiMMnHC8GhoFHgduBMxtDZwLfzUyXtUmSJEmSJHWYds1I2hu4PiL2BrZRD5EW\nZ2YtIs4FromIDwFbgHPaVJMkSZIkSZKmoS1BUmY+Apw8ydhdwMvaUYckSZIkSZJmri1L2yRJkiRJ\nktT9DJIkSZIkSZJUiEGSJEmSJEmSCjFIkiRJkiRJUiHt2rVNkrQbiYgvA0cBY8ATwHmZeXtEzAOu\nAfYDHgfOycx7Gtc0fUySJElSezkjSZI0E2/LzBdm5gnAJcDVjfNXAJdn5jzgcuDKCde0YkySJElS\nGzkjSZI0bZn50wmH+wJjEXEgcCJweuP8dcBlEXEAUGn2WGZubskPJ0mSJGlSzkiSJM1IRFwVERuB\njwBvAw4HHszMbQCNx4ca51sxJkmSJKnNnJEkSZqRzHwnQEScDVwMfLDciiY3NDRUdgmSJEnSbsEg\nSZI0K5l5bUR8BngAODQi5mTmtoiYAxwCbKK+RK3ZY4UNDAzQ19fXrB9ZkrreyMiIIbskaUZc2iZJ\nmpaI2CciDp9wvBgYBh4FbgfObAydCXw3MzdnZtPHWvcTSpIkSZqMM5IkSdO1N3B9ROwNbKMeIi3O\nzFpEnAtcExEfArYA50y4rhVjkiRJktrIIEmSNC2Z+Qhw8iRjdwEva9eYJEmSpPZyaZskSZIkSZIK\nMUiSJEmSJElSIQZJkiRJkiRJKsQgSZIkSZIkSYXYbFuSJKkNhoeHWbFiBUuXLmXu3LlllyOVIiIu\nAX4X+HXguMwcapyfB1wD7Ac8DpyTmffMZkyS1BrOSJIkSWqDarXKhg0bqFarZZcilenLwG8C9+9w\n/grg8sycB1wOXNmEMUlSCxgkSZIktdjw8DBr1qyhVquxevVqtmzZUnZJUiky86bM3DTxXEQcCJwI\nXNc4dR1wYkQcMNOxVv8ckrQnc2mbJElSi1WrVcbGxgAYGxujWq2yZMmSkquSOsbhwIOZuQ0gM7dF\nxEON85UZjm0u+uZDQ0NN/WEkaXdnkCRJktRi69atY3R0FIDR0VHWrl1rkCR1iIGBAfr6+souQ5I6\nysjIyKRBu0vbJEmSWmz+/Pn09tY/v+vt7WXBggUlVyR1lE3AoRExB6DxeEjj/EzHJEktYpAkSZLU\nYoODg/T01P/Z1dPTw+DgYMkVSZ0jMx8FbgfObJw6E/huZm6e6Vj7qpekPY9BkiRJUov19/ezaNEi\nKpUKp512GnPnzi27JKkUEXFpRDwAHAasjog7G0PnAudFxN3AeY1jZjkmSWoBeyRJkiS1weDgIBs3\nbnQ2kvZomXk+cP5Ozt8FvGySa2Y0JklqDYMkSZKkNujv72f58uVllyFJkjQrLm2TJEmSJElSIQZJ\nkiRJkiRJKsQgSZIkSZIkSYUYJEmSJEmSJKkQgyRJkiRJkiQVYpAkSZIkSZKkQgySJEmSJEmSVIhB\nktQkw8PDLFu2jC1btpRdiiRJkiRJLWGQJDVJtVplw4YNVKvVskuRJEmSJKklDJKkJhgeHmbNmjXU\najVWr17trCRJkiRJ0m7JIElqgmq1ytjYGABjY2POSpIkSZIk7ZYMkqQmWLduHaOjowCMjo6ydu3a\nkiuSJEmSJKn5DJKkJpg/fz69vb0A9Pb2smDBgpIrkiRJkiSp+QySpCYYHBykp6f+v1NPTw+Dg4Ml\nVyRJkiRJUvP1tuNNImI/4Frg+cCTwD3AezJzc0TUgO8DY42nn52Z329HXVKz9Pf3s2jRIlauXMlp\np53G3Llzyy5JkiRJkqSma0uQBNSAFZm5DiAiLgaWA/9vY/zUzHyiTbVILTE4OMjGjRudjSRJkiRJ\n2m21JUjKzGFg3YRTtwBL2vHeUrv09/ezfPnyssuQJEmSJKll2jUj6SkR0UM9RLphwul1EdELfB24\nKDNH2l2XJEmSJEmSptb2IAn4FPAEcFnj+IjM3BQRz6HeR+mDwAem84JDQ0PNrVCSJEmSJElP09Yg\nKSIuAY4BFmfmGEBmbmo8/ntEXAX89+m+7sDAAH19fU2tVZK62cjIiCG7JEmSpKbradcbRcRHgZOA\n3x5fuhYRcyNir8b3vcCbgNvbVZMkSZIkSZKKa8uMpIh4AXABcDdwc0QA/AhYAVwZETXgGcDN1Je2\nSZIkSZIkqcO0a9e2O4HKJMPHt6MGSZIkSZIkzU7blrZJkiRJkiSpuxkkSZIkSZIkqZC27tomSep+\nEbEfcC3wfOBJ4B7gPZm5udHz7vvAWOPpZ2fm9xvXLQYupn7vWQ+8PTN/MZsxSZIkSe3ljCRJ0nTV\ngBWZGZl5HPBDYPmE8VMz80WNr/EQaR/gs8DizPxPwM+A981mTJIkSVL7GSRJkqYlM4czc92EU7cA\nR+7istcA/5KZ9zSOrwDeMssxSZIkSW3m0jZJ0oxFRA+wBLhhwul1EdELfB24KDNHgCOA+yc8ZyNw\neOP7mY4VNjQ0NN1LJEmSJO2EQZIkaTY+BTwBXNY4PiIzN0XEc6j3Ufog8IGyihs3MDBAX19f2WVI\nUscYGRkxZJckzYhL2yRJMxIRlwDHAG/JzDGAzNzUePx34Crg5Y2nb2T75W9HAJtmOSZJkiSpzQyS\nJEnTFhEfBU4CfruxdI2ImBsRezW+7wXeBNzeuGQl8JKIOKZxfC7w97MckyRJktRmBkmSpGmJiBcA\nFwCHADdHxO0R8SXgN4BbI+IO4HvAL6kvbSMzfwa8G/haRPwA2Be4ZDZjkiRJktrPHkmSpGnJzDuB\nyiTDx09x3VeArzRzTJIkSVJ7OSNJkiRJkiRJhRgkSZIkSZIkqRCDJEmSJEmSJBVikCRJkiRJkqRC\nDJIkSZIkSZJUiEGSJEmSJEmSCjFIkiRJkiRJUiEGSZIkSZIkSSrEIEmSJEmSJEmFGCRJkiRJkiSp\nkN6yC5AkSZIkgIh4HfBhoNL4+rPM/IeImAdcA+wHPA6ck5n3NK6ZdEyS1HzOSJIkSZJUuoioANcC\nZ2fmi4CzgWsioge4Arg8M+cBlwNXTrh0qjFJUpMZJEmSJEnqFGPAvo3vnws8DOwPnAhc1zh/HXBi\nRBwQEQdONta+kiVpz+LSNkmSJEmly8xaRLwZ+EpE/Bz4NeC1wOHAg5m5rfG8bRHxUON8ZYqxzUXe\nd2hoqPk/jCTtxgySJEmSJJUuInqBC4A3ZOY/R8TLgb+nvsStZQYGBujr62vlW0hS1xkZGZk0aHdp\nmyRJkqRO8CLgkMz8Z4DG48+BrcChETEHoPF4CLCp8TXZmCSpBQySJEmSJHWCB4DDIiIAIuI/AwcB\n9wC3A2c2nncm8N3M3JyZj0421tbKJWkPYpAkSZIkqXSZ+WNgCfC/I+IOoAq8IzOHgXOB8yLibuC8\nxvG4qcYkSU1mjyRJkiRJHSEzvwB8YSfn7wJeNsk1k45JkprPGUmSJEmSJEkqxCBJkiRJkiRJhRgk\nSZIkSZIkqRCDJEmSJEmSJBVikCRJkiRJkqRCDJIkSZIkSZJUiEGSJEmSJEmSCjFIkiRJkiRJUiEG\nSZIkSZIkSSrEIEmSJEmSJEmFGCRJkiRJkiSpkN4iT4qIA4D/yMwnImIOcA4wBlybmWMFrt8PuBZ4\nPvAkcA/wnszcHBEnA1cCewH3AWdl5qMz+WEkSZIkSZLUOkVnJH0NOKbx/UeA9wF/BHyi4PU1YEVm\nRmYeB/wQWB4RPcDngfdm5jzgW8DyosVLkiRJkiSpfYoGSfOA2xvfnwW8BlgIDBa5ODOHM3PdhFO3\nAEcCJwFbM/OmxvkrgDcXrEmSJEmSJEltVDRI2gY8MyKOA36amRuBnwD7TPcNG7OQlgA3AEcA94+P\nZeZjQE9E9E/3dSVJkiRJktRahXokAV8H/h7YD6g2zh0LPDiD9/wU8ARwGfDGGVz/NENDQ814GUna\no0TEq4EXscOHApn5oXIqkiR1g4hYCJwJHAI8BFQzc025VUmS2qVokPRO4G3AL6k3zQbYH7hoOm8W\nEZdQ77W0ODPHImIj9SVu4+P7A2OZOTyd1x0YGKCvr286l0jSbm1kZGTKkD0iLqO+lHgt8IsJQ7UW\nlyZJ6mIR8T+ApcD/BL5LfYXB30XEisws2j9VktTFCgVJmTkCfGaHc+um80YR8VHqPZF+q/F6AOuB\nvSLiFY0+SecC10/ndSVJM/JW4IWZuansQiRJXeW/Awsz86lPKyLiWmAVxTfikSR1sUJBUkTsC5wP\nnMDTl0C8usD1LwAuAO4Gbo4IgB9l5hsj4mzgyoh4FnAf9WbekqTWeox6rztJkqbrBzsc34szWiVp\nj1F0adv1wBzgS8B/TPdNMvNOoDLJ2M3AcdN9TUnSrHwC+EJEfAx4ZOJAZt471YURsR/1Zc7PB54E\n7gHek5mbI+Jk4EpgLxofDmTmo43rmj4mSWq7i4DPRcRFwAPA4cAHgQsbm+oAkJljpVQnSWq5okHS\nycD+mflkK4uRJLXNpxuPr9vhfI36BwdTqQErxpc4R8TFwPKIeBfweeC/ZuZNEfEBYDnwjsYfF00d\nm9VPL0maqSsbj2dSvx+Mf1j8XxpjFYrdSyRJXapokHQT8BvA91pYiySpTTKzZ9fPmvTaYWDdhFO3\nAEuo98Hb2uh5B3AF9RlE72jRmCSp/Y4quwBJUrmKBkn/FfjHiLiVpy+B+PNmFyVJao+IOAI4FHhg\nJo23GzOGlgA3UN+55/7xscx8LCJ6IqK/FWPT2eFzqh3sJEnFZeb9u36WJGl3VjRI+gj19c/3Ac+Z\ncN6mepLUhSLiYKAKnAI8DuwXEbcAg5n50DRe6lPAE8BlwBubXmiTDAwM0NfXV3YZktQxRkZGZhSy\nN3Zo2+nfAJl5zmzrkiR1vqJB0iAwLzMfbmUxkqS2+TRwB/DazPx5ROwNfJT60rHXF3mBiLgEOAZY\nnJljEbEROHLC+P7AWGYOt2Jsxj+5JGk2dtyx7XnAm4AvlFCLJKkERYOke4FftrIQSVJbvQI4ODN/\nCdAIk/4EeLDIxRHxUer9i34rM0cap9cDe0XEKxo9jc6lvutnq8YkSW2WmX+247mI+BxwYQnlSJJK\nUDRIuha4ISI+xdN7JN3Y9KokSa22BTiW+qykcQH8ZFcXRsQLgAuAu4GbIwLgR5n5xog4G7gyIp5F\nfTn0WVDfBrrZY5KkjnE78Kqyi5AktUfRIOm9jceP7nC+BhzdvHIkSW2yAljd+BT5furLx94OfHBX\nF2bmnfxqu+cdx24GjmvXmCSpvSJi4Q6nnk29DcaGEsqRJJWgaJD0nzJzW0srkSS1TWZ+NiJ+CLwV\nOB54CHhrZq4ptzJJUof73A7HP6c+I+nMEmqRJJVgl0FSRMwBnoiI507ogyFJ6nKNpckuT5YkFZaZ\nR5VdgySpXLsMkjJzW0TcDexH/RNrSVIXioj3Z+ZHGt//+WTPy8wPta8qSVI3iYhXA/dl5t0TzgVw\nRGauKq8ySVK7FF3a9gXgaxHxSeAB6r2RAJttS1IXOWzC94eXVoUkqZtdDvzmDud+1jg/r/3lSJLa\nrWiQtKTxeNEO5222LUldIjOXTPj+7WXWIknqWgdm5sM7nHsYeF4ZxUiS2q9QkORaaEnavUTEcGb2\n7+T8o5l5YBk1SZK6wr0RsXCHVQnzgR+VVI8kqc2KzkiSJO1enrHjiYh4BjCnhFokSd3jIuAfIuJz\nwA+B5wNvb3xJkvYAkwZJEbGJCb2QJpOZRzS1IklSy0TEP1H/3f6siPjWDsOHATe3vypJUrfIzK80\nGm6/A/gtYBPw/2Tmd8qtTJLULlPNSDprwvcvAd4GXArcDxwJ/Dfgb1tXmiSpBa4CKtR/r39uwvka\n8AjgBgqSpCll5m3AbWXXIUkqx6RBUmb+n/HvI+Jy6p80PDjh3NeBlcAnWlqhJKlpMvMagIi4JTPv\nKrseSVJ3iYh/AP4yM/9pwrlXAn+QmW8qrzJJUrsU7ZF0CPDEDueeAA5tbjmSpHbIzLsi4iDgpcD+\n1GcpjY9dXVphkqRO9yrg93Y4923gyyXUIkkqQdEg6Qbghoj4C+AB4HDggsZ5SVKXiYjfBj4P3AO8\nALgTGABuAgySJEmT2QrsDfz7hHP7AL8spxxJUrv1FHzeudQ/abgC+Ffg08CtjfOSpO7zF8DbM/ME\n4OeNx3cD68stS5LU4b4BXBkRzwFoPF5GveWFJGkPUGhGUmZuBZY1viRJ3e+IzLx+h3PXAD8G3ldC\nPZKk7vA/qM9o3RIRjwP9wNeBs0utSpLUNkWXthERpwODwIGZuTgiXgw8JzPd4UeSus+jEXFQZj4C\n3BcRpwCPAXNKrkuS1MEycwvwWxHxPOrtLjZl5o9LLkuS1EaFgqSIOA/4A+rbRo/vxvAfwKXAqa0p\nTZLUQp8FXgF8EfhLYC0whjtxSpKmEBFHA4uob9TwGPB4uRVJktqtaI+kPwROy8zl1P/QALgLiJZU\nJUlqqcz8eGZ+sfH93wLzgJMy84PlViZJ6lQR8UngbuD9wOsbj3dHxKWlFiZJaquiS9t+DdjU+L7W\neHwG8GTTK5IktV1mbiy7BklS54qI9wGvAU7NzNsmnH8ZcG1E/HFmXtyE93kW9Zmyp1HfIe7bmfnu\niJhHvZffftRnQZ2Tmfc0rpl0TOomw8PDrFixgqVLlzJ37tyyy5EmVTRI+hb1RtsfmXDufOpLISRJ\nXSAiNvGrDwMmlZlHtKEcSVJ3eRdw1sQQCSAzb42Ic4C/BWYdJAErqAdI8zKzFhEHNc5fAVyemZ+P\niLOAK4GFBcakrlGtVtmwYQPVapUlS5aUXY40qaJB0nnAVyPiXcCvRUQCPwNe17LKJEnNdlbZBUiS\nutZhwL9MMnYbcOhs3yAi9gHOAQ7LzBpAZj4SEQcCJwKnN556HXBZRBwAVCYby8zNs61Japfh4WHW\nrFlDrVZj9erVDA4OOitJHatQkJSZD0fES4CXAEdSX+Z2W2aOTX2lJKlTZOb/KbsGSVLXeox6f9R/\n28nYb9CcptvPb7zOhRGxAHgC+AD1TX4ezMxtAJm5LSIeor5rXGWKsUJB0tDQUBNKl2bna1/7Gtu2\nbQNg27ZtXHrppbzudc7bUGcqOiOJxqcCtwG3RUS/IZIkda+I+PPJxjLzQ+2sRZLUFf4O+J8R8TuZ\n+dD4yYg4FLga+EIT3mMOcDTw3cz840b/pa8Cv9eE157UwMAAfX19rXwLaZc+/vGPbxck3XnnnVx4\n4YUlV6U92cjIyKRB+5RBUkTMAd4LHAt8m/o20d8EXhYRPwZen5nrm1uuJKkNDt/h+HnAq4AvlVCL\nJKnzXQi8APhBRNwKPAwcDLwMWNMYn62NwCj15Wnj/Zceoz4j6dCImNOYcTQHOIT6KonKFGNS15g/\nfz6rVq1idHSU3t5eFixYUHZJ0qR2NSPpUuAUYBXwR9Sb7N0EvJP6+uVLAP8Ll6Quk5lv3/FcRJwB\nnFlCOZKkDpeZTwKvj4hF1HdU2x+4BfiLzFzTpPd4LCLWUu939M3GbmwHAncDt1O/R32+8fjd8R5I\nETHpmNQtBgcHWbOm/r9ST08Pg4ODJVckTW5XQdLvAMc1fql/knqyvygzRyLiQuqfREiSdg/fBP5X\n2UVIkjpXIzRaExE9wEGZ2ey/B84Fro6ITwC/BM7OzJ9ExLnANRHxIWAL9Q+1J14z2ZjUFfr7+1m0\naBErV67ktNNOs9G2OtqugqS9M/MxgMx8KCL+PTNHGscjEVG4x5IkqXNExNE7nHo28FZcCiC1zPDw\nMCtWrGDp0qX+gaCuFRHPBS6n3rfol8DeEfF64KWZ+YHZvn5m3gvM38n5u6gvo9vZNZOOSd1kcHCQ\njRs3OhtJHW+XQVBEVKivPa4AtR2OJUnd6QdAjV/9Lv8F8F3gbaVVJO3mqtUqGzZsoFqtsmTJkrLL\nkWbqCuqzfo4ENjTOfRv4BPUd1iTNUH9/P8uXLy+7DGmXdhUk7UO94d24yoTjCvU/QiRJXSYze8qu\nQdqTDA8Ps3r1amq1GqtWrWJwcNBZSepWi4BDMvOXEVEDyMzNEXFgyXVJktpkV0HSUW2pQpLUdo2d\nbU6mvrvNg8Ctmbmt3Kqk3VO1WmV0tP5Z3OjoqLOS1M1+Sr3R9lO9kSLiCOydKkl7jCk/kc7M+yd+\nUe+d8eQO5yRR/7R52bJlbNmypexSpF2KiOOBe4DrgT8G/jdwT0S8sNTCpN3U2rVrqdXqE7lrtRpr\n164tuSJpxq4CvhgRC4CeiDgFuIb6kjdJ0h6g0NKGiHhuRPwdsJV6Xw0i4vUR8RetLE7qJhN7X0hd\n4GrqzVIPzcyXAocClzXOS2qyAw44YMpjqYt8nPoOn5cDz6B+3/gK8Mkyi5IktU/RXddsqidNYXh4\nmDVr1lCr1Vi9erW9L9QN5gF/lZnj/S1qEfFJ4KJSq5J2U5s3b97u+NFHHy2pEml2GveNT2JwJEl7\nrKLNVhcB52fmwzQabGfmZsCmehL12UhjY2MAjI2NOStJ3eAfgdfvcG4x8P+XUIu021uwYAGVSn2T\nxEqlwsKFC0uuSJqZiLgjIv44Ig4ruxZJUjmKzkiadVO9iLgE+F3g14HjMnOocf4+6kvmtjaeujQz\nv1H0daVOsG7duu2aqK5du9Ymqup0c4BqRKyn3v/ucOAk4CsR8bfjT8rMc0qqT9qtDA4OsmrVKkZH\nR5kzZw6Dg4NllyTN1EXAmcCFjXvI3wHXZ+ZwqVVJktqmaJA03lTv/fyqqd5HmV5TvS9TnwL7TzsZ\ne9N4sCR1o/nz5z/1B0Jvby8LFiwouyRpV4YaX+M2AIb4Uov09/dz8MEHs2nTJg455BCXP6trZeaX\ngC9FxK8Bv0M9VPr/ImJNZu4401WStBsqGiR9HPgPtm+qdyXTWBudmTcBRMQ0S5Q63+DgIGvWrAGg\np6fHT5rV8TLzz8quQdqTDA8P8+Mf/xiAhx9+mC1bthgmqatl5s8am/H8BHgm8NqSS5IktUmhIKkN\nTfW+EBEV4CbgTzPzJ9O5eGjIyUwq3/HHH8/69es5/vjjuffee8suR9qliJgPnEN9x7YHgWsz0z3J\npRaoVqvUajUAarUa1WrVJdDqSo1/sy8E3gq8EbgfuA54W5l1SZLap1CQFBF3AJ8HrsvMB5pcwysz\nc1NE9AF/RX376bOm8wIDAwP09fU1uSxpeo466ihWrFjB+eef76fMKt3IyMiUIXtEvJP6EuWrgFuB\nI4DrIuKDmfnZ9lQp7TnspafdyEPAE0AVeHlm/htARBTdxEeS1OWKLm27iBY11cvMTY3HkYj4a+CG\n2b6mVIb+/n6WL19edhlSUX8CnJ6Zd4yfiIj/BXwRmDJImsnmCRFxMvUl0XsB9wFnZeajsxmTusn8\n+fNZuXIltVqNSqViLz11szdk5m3jBxFxHPXZrf8FOKS0qiRJbVPok4PM/FJmvhk4mHp/pDcCmyJi\nVqFPROwdEfs2vq8Ag8Dts3lNSVIh+1FvsD1RAv0Frv0y8JvUlzPs6E2Z+aLG13iI1EN9Vut7M3Me\n8C1g+WzGpG5zxhlnbLe07Ywzzii5ImlmMvO2iDggIv4gIv6V+r/dXwr8QcmlSZLaZFpTUDPzZ9Rn\nI32a+lKIwk31IuLSiHgAOAxYHRF3AgcB6yLie9R3D5oH/P50apIkzcg/A38ZEc+GerAPXAzcvKsL\nM/Om8dmkBZ0EbB3fdIH6jp9vnuWY1FVWrlxJpVIBoFKpsHLlypIrkqYnIp4REb8bEV+l3lfvPcCX\nqDfb/r3MvL7UAiVJbVO0R9Ksm+pl5vnA+TsZOqHoa0idbHh4mBUrVrB06VJ7JKkbvId6f4ufRsQw\n9ZlIN1NfxjwbO9s84QgmzF7KzMcioici+mc6Nt2l1W7KoLLdeOON281IWrNmDS996UtLrkqalkeA\nMeBvgAsz818BIsIPgSVpD1O0R5JN9aRdqFarbNiwwZ141NEaM5A+AAwA6/hVT4uHmrCZwqw3T2gV\nN2VQ2RYuXMiqVasYHR2lt7eXRYsWcdJJJ5VdlvZgu9qUYSe+B7wCeBlwT0T8KDO3tKQ4SVJHKxoE\nvSEzj8nMD2bmv0XEcRFxMdDsHdykrjQ8PMyaNWuo1WqsXr2aLVv8d5U61uXAYuAu6g2zl2bmbc3Y\nkXPi5gnAXwMZAetJAAAWwklEQVQvbwxtBI4cf15E7A+MNWYVzXRM6iqDg4P09NT/2dXT08Pg4GDJ\nFUnTk5nzgecD3wTeB/y4scxtb+AZJZYm7TaGh4dZtmyZf0uo4xVttm1TPWkK1WqVsbExAMbGxqhW\nqyVXJE3qDODVmfknwGuA1zXjRXexecJ6YK+IeEXj+Fzg+lmOSV2lv7+fRYsWUalUOO2001wCra6U\nmfdn5ocz8xhgEfAw9eVud0TEinKrk7rfxBUOUiebMkiyqZ5UzLp16xgdHQVgdHSUtWvXllyRNKm9\nM/NheGoG0b7TfYHpbp6QmWPA2cCnI+Ie4FXAstmMSd1ocHCQY4891tlI2i00Nl54N/A84DzguJJL\nkrqaKxzUTXbVI8mmelIB8+fP3673xYIFC8ouSZpMb0QsACqTHJOZN071AjPZPCEzb2aSPzJmOiZ1\nm/7+fpYvX152GVJTZeZW6pvwXFd2LVI329kKB/uuqlPtamnb94DnUm+q95KIcB62tBP2vlAXeRS4\nGvhc4+vxHY6vKq80SZKkPZMrHNRNppyRlJnzI+JI4BzqTfUujYhvYlM9aTvjvS9Wrlxp7wt1tMz8\n9bJrkCRJ0vZc4aBusstm2zbVk4qx94UkSZKkmXCFg7pJoV3bxtlUT5rceO8LZyNJkiRJmg5391Q3\n2VWz7Z2yqZ4kSZIkSc0zODjIxo0bnY2kjjejIEmSJEmSJDWPu3uqW0xraZskSZIkSZL2XAZJkiRJ\nbTA8PMyyZcvYsmVL2aVIkiTNmEGSJElSG1SrVTZs2EC1Wi27FEmSpBkzSJIkSWqx4eFh1qxZQ61W\nY/Xq1c5KkiRJXcsgSZIkqcWq1SpjY2MAjI2NOStJkiR1LYMkSZKkFlu3bh2jo6MAjI6Osnbt2pIr\nkiRJmhmDJEmSpBY75ZRTtjs+9dRTS6pEkiRpdgySJEmSWqxWq5VdgiRJUlMYJEmSJLXYLbfcst3x\nzTffXFIlkiRJs2OQJEmS1GLz58+np6f+z66enh4WLFhQckWSJEkzY5AkSZLUYoODg1MeS5IkdQuD\nJEmSJEmSJBVikCRJktRi1WqVsbExAMbGxqhWqyVXJEmSNDMGSZIkSS22du3a7Y5vvPHGkiqRJEma\nHYMkSZKkFpszZ852x729vSVVIkmSNDsGSVKTDA8Ps2zZMrZs2VJ2KZKkDvPzn/98u+MnnniipEok\nSZJmxyBJapJqtcqGDRvseyFJeppKpTLlsSRJUrcwSJKaYHh4mDVr1lCr1Vi9erWzkiRJ26nValMe\nS5IkdQsX6EtNsLPdeJYsWVJyVZKkTrH33ntvt7xtn332KbEaqfNFxIXARcBxmTkUEScDVwJ7AfcB\nZ2Xmo43nTjomSWo+ZyRJTbBu3TpGR0cBGB0dfdruPJKkPdvSpUu3O162bFlJlUidLyJOBE4G7m8c\n9wCfB96bmfOAbwHLdzUmSWoNgySpCU455ZTtjk899dSSKpEkdaITTjiBZz/72UB9dtILX/jCkiuS\nOlNE9AGXAxOndp8EbM3MmxrHVwBvLjAmSWoBgySpCbZu3brd8cjISEmVSJI61THHHLPdo6Sd+nPg\n85l534RzR9CYnQSQmY8BPRHRv4sxSVIL2CNJaoJbb711u+Nvf/vbJVUiSepEw8PDfO973wPgjjvu\nYMuWLcydO7fkqqTOEhGnAC8G2rr2c2hoqJ1vJ0ldzyBJagK3dZYkTeVv/uZvntqprVarcc011/CH\nf/iHJVcldZxXAf8Z+FFEABwGfAO4FDhy/EkRsT8wlpnDEbFxsrGibzowMMD/be+OYus8yzyB/21H\nm7ZzwSQhSafVbGlW0++iRgKCNCTMtHag2t5U2dmtWI9EMVpUQWcF3EGzGhWJi23Z5WKXDUurXkXb\nFYYJ2ulyE2gbG5ami0ZRA8p25ms1yZRuukCJXQSkSWUf74WdTE4Sn7j2OX6/Y/9+UmU/3xfLf6fx\neeMnz/u9mzdv7s5XALBOXLhwYclGu61t0AV33XVXW3333XcXSgJAE/3whz9sq6empsoEgQar6/qx\nuq5vqev6PXVdvyfJ/03yz5P8xyQ3VlX1J4u/9DNJ/mrx/eMd7gHQAxpJ0AXj4+MZHFz4dhocHMz4\n+HjhRAA0ydzcXMcaWFpd160kDyT5RlVVr2Rhcunh690DoDdsbYMu2Lp1a0ZGRnL06NGMjo567gUA\nwCotTiVdfP9Ykvcu8euWvAdA95lIgi7Zv39/brrppuzfv790FAAaZmhoqGMNANAvNJKgS55++umc\nO3cuTz/9dOkoADTMlc/OGxkZKRMEAGCVNJKgC6anpy89OHVycjIzMzNlAwHQKLt3726rP/jBDxZK\nAgCwOmvSSKqq6qtVVZ2uqmq+qqrhy67fUVXVC1VVvbz49o/WIg9026FDh9JqtZIkrVYrhw4dKpwI\ngCY5ePBgW/21r32tUBIAgNVZq4mkv05yV5JXr7j+eJKv13V9R5KvJ3lijfJAV115rPMPfvCDQkkA\naKK33nqrYw0A0C/W5NS2uq5/lCRVVV26VlXVjiQfSHLP4qVvJjlYVdX2uq7fWItc0C3z8/Mda1hP\nqqr6apJ/leQ9Sd5b1/XJxet3JDmUZFuSs0k+Udf1K726BwAArL01aSQt4Q+TnKnrei5J6rqeq6rq\n9cXr76iRdPLkyR7Eg+W7884789Of/vRSPTw8nOPHjxdMBD3110n+c5L/dcX1i1OmT1VV9fEsTJnu\n6+E96Bvbt2/PG2/8419vduzYUTANAMDKlWwkdc3w8HA2b95cOgYb2JYtW/L5z3/+Uv2pT30qt99+\ne8FEbHQXLlzoWZP9nU6ZJhno9j2Tq/Sbz372s3nkkUcu1Z/73OcKpgEAWLmSp7a9luTWqqqGkmTx\n7S2L16GvHDlypGMNG8BVU6ZJLk6Z9uIe9JXvfOc7bfXhw4cLJQEAWJ1iE0l1Xf+yqqoTSf48yVOL\nb1/0r8z0o6mpqbZ6cnIyDz30UJkwwFVsgaa0n/zkJ231iRMnbIEGAPrSmjSSqqr6WpJ/meTmJM9W\nVXW2rus7k3wmyaGqqh5JMpPkE2uRB7ptZGQk3//+9zM3N5ehoaGMjo6WjgRr7dKU6eIz7y6fMh3o\nwb13xBZommj37t2lI7CB9XILNADr21qd2va5JFc9DKCu679L8sdrkQF6aWxsLN/73veSLJzYNjY2\nVjgRrK3rTZn24h4AALD21sXDtgFYOyucMu3FPQAAYI1pJEEXTExMZHBwMK1WK4ODg5mYmPCMJNat\nlUyZ9uIeAACw9kqe2gbrxtTUVGZnZ5Mks7OzmZycLJwIAAAAuk8jCbpgZGQkAwMDSZKBgQEP2wYA\nAGBd0kiCLrj33nszPz+fZOFh2/fee2/hRAA0ydatW9vqbdu2FUoCALA6GknQBUeOHGmbSDpy5Ejh\nRAA0yfT0dFt99uzZQkkAAFZHIwm6YGpqqm0iyTOSAAAAWI80kqAL9uzZ01bv3bu3UBIAAADoHY0k\n6ILz58+31RcuXCiUBAAAAHpHIwm64Mc//nFb/cILLxRKAgAAAL2jkQQAAADAsmgkQRfs3Lmzrb75\n5psLJQEAAIDe0UiCLrjyGOdf/epXhZIAAABA72gkQRds27atrX73u99dKAkAAAD0jkYSdMEvfvGL\ntvrnP/95oSQAAADQOxpJAAAAACyLRhJ0wV133dVWj4yMlAkCAAAAPaSRBF3wyU9+MoODC99Og4OD\nGR8fL5wIAAAAuk8jCbpg69atufvuu5Mko6Oj2bJlS+FEAAAA0H0aSdAlo6OjGRwczOjoaOkoAAAA\n0BMaSdAljz/+eFqtVr7xjW+UjgIAAAA9oZEEXXDq1Km8/vrrSZIzZ87k9OnThRMBAABA92kkQRd8\n5Stf6VgDAADAeqCRBF1wcRrpojNnzhRKAgAAAL2jkQQAAADAsmgkQRfs3Lmzrb755psLJQEAAPrR\n9PR0Hn744czMzJSOAh1pJEEX/PrXv26r33zzzUJJAACAfjQxMZGXXnopExMTpaNARxpJ0AV79uxp\nqz/84Q8XSgIAAPSb6enpPPfcc5mfn8+zzz5rKolG00iCLhgYGCgdAQAA6FMTExNptVpJklarZSqJ\nRtNIgi44duxYW/38888XSgIAAPSbqampzM7OJklmZ2czOTlZOBEsTSMJumBoaKit3rRpU6EkAABA\nvxkZGbn0M8SmTZsyOjpaOBEsTSMJuuB3v/tdW/3b3/62UBIAAKDfjI2NZXBw4cfzwcHBjI2NFU4E\nS9NIgi7YsmVLxxoAAGApW7duzUc+8pEMDAzkox/9qJ8naDT7b6ALrjxVwSkLAADAOzE2Npaf/exn\nppFoPI0kAACguKqqtiX5b0n+WZK3k7yS5NN1Xb9RVdWHkjyR5MYk/5Dk43Vd/3Lx45a8B/1k69at\neeyxx0rHgOuytQ0AAGiC+ST/oa7rqq7r9yb5+ySPVVU1mOSpJP+2rus7kvwwyWNJ0ukeAL2hkQRd\nMDAw0LEGAKCzuq6n67qeuuzS/05yW5LdSc7Xdf2jxeuPJ/nY4vud7gHQA7a2QRfccMMNeeutt9pq\nAABWZnHS6KEk/zPJP03y6sV7dV3/qqqqwaqqtna6V9f19HI+18mTJ7sbHmCd00iCLri8iXStGgCA\nd+S/JPltkoNJ/qyXn2h4eDibN2/u5acA6DsXLlxYstFuaxsAANAYVVV9NckfJfnXdV23kvwsC1vc\nLt5/d5LW4sRRp3sA9IBGEgAA0AhVVf37LDz36F/UdX1h8fLxJDdWVfUni/VnkvzVMu4B0AON2NpW\nVdU/JDm/+F+SfLGu6+8VCwTAii31mr7So5sd6wywMVRVdWeSA0leTnKsqqokOV3X9Z9VVfVAkieq\nqrohi2tBktR13VrqHgC90YhG0qL767r2pDuA9aHtNf2y45k/Wdf1j6qq+sssHM/8b1Z6b62/IAB6\nq67r/5Pkmkff1nV9LMl73+k9ALrP1jYA1sJKj252rDMAADRIkyaS/ntVVQNJfpTk39V1/eZyP9CR\nnTTR8ePHS0eAktpe07PCo5s73XsnD1K1TtBE1gkAoB81pZH0p3Vdv1ZV1eYk/ykLx3wue2+zIztp\not27d5eOwAbW6bjONXCt1/T/USpMYp2gmawTlFR4nQCgjzVia1td168tvr2Q5L8m+XDZRACs1BKv\n6Ss9utmxzgAA0CDFG0lVVf1eVVXvWnx/IMlYkhNlUwGwEh1e01d6dLNjnQEAoEGasLVtZ5LvVFU1\nlGQoyUtJ/qJsJABW6Jqv6Z2OZ17pPQAAYO0VbyTVdX0qyftL5wBg9Tq9pq/06GbHOgMAQHMU39oG\nAAAAQH/QSIIuGBgY6FgDAADAeqCRBF0wNDTUsQYAAID1QCMJumB2drZjDQAAAOuBRhIAAAAAy6KR\nBAAAAMCyaCQBAAAAsCwaSQAAAAAsi0YSAAAAAMuikQQAAADAsmwqHQAAWN+OHj2aZ555pnSMxjlw\n4EDpCEXcc8892bdvX+kYAMAKmUgCAAAAYFlMJAEAPbVv374NP4Fy3333XXXt0UcfLZAEAGB1TCQB\nAAAAsCwaSQAAPfbd7363Yw0A0C80kgAAAKCw6enpPPzww5mZmSkdBTrSSAIAWAPDw8MZHh42jQTA\nNU1MTOSll17KxMRE6SjQkUYSAAAAFDQ9PZ3nnnsu8/PzefbZZ00l0WgaSQAAAFDQxMREWq1WkqTV\naplKotE0kgAAAKCgqampzM7OJklmZ2czOTlZOBEsTSMJAAAAChoZGcnQ0FCSZGhoKKOjo4UTwdI0\nkgAAAKCgsbGxzM/PJ0nm5+czNjZWOBEsTSMJAAAAgGXRSAIAAICCJiYmMjAwkCQZGBjwsG0abVPp\nAACwXj355JM5depU6Rg0xMU/CwcOHCichCbYtWtXHnzwwdIxgIaYmprK3NxckmRubi6Tk5N56KGH\nCqeCa9NIAoAeOXXqVE6+VGfoht8vHYUGaM0uPET1b0/9onASSps7/2bpCEDDfOhDH2o7qW3v3r0F\n00BnGkkA0ENDN/x+brrtI6VjAA1y7tXnSkcAGubitjboB56RBAAAAAW98MILbfWxY8cKJYHr00gC\nAACAgvbs2dNW29pGk9naBgA9MjMzk7nzb9rGArSZO/9mZmb+SekYQIPMz8+XjgDLZiIJAAAACrpy\na9vzzz9fKAlcn4kkVu3o0aN55plnSsdonI16vPM999yTffv2lY4BjbBly5b8fOZtD9sG2px79bls\n2bKldAygQbZv357XXnvtUr1jx46CaaAzjaRVevLJJ3Pq1KnSMYqamZnJzMxM6RiNs1H/XHz729/W\nWEyya9euPPjgg6VjAADQB954442ONTSJRtIqHT9+PGfOnCkdgwY6d+5c6QhFnDt3zvdEFhqsGkkk\n8YwkLmnNnk+SDG66oXASSps7/2aSnaVjAA2yd+/eHD16tK2GptJIWqUdO3aYxuGaTaObbrqpQBKa\nwjgyycJkGlx0cVJ11y4NBHZ6fQDaXL6t7Vo1NIlG0ip9+ctfLh2hOM9ISk6ePHnVtY36F0TPSIJ/\nZCqNy118dt6jjz5aOAkATfPKK6+01S+//HKhJHB9Tm0DAAAAYFlMJLFq+/bt2/ATKPfdd99V1/yL\nMwAAAOuNRhIAjVdV1R1JDiXZluRskk/Udf1K548CYKOwTgCsHY0kAPrB40m+Xtf1U1VVfTzJE0k2\n9igkAJfry3Xi6NGjeeKJJ0rHKG52djazs7OlYzTO/v37S0coYtOmTdm0Savi05/+dGN3/vi/A0Cj\nVVW1I8kHktyzeOmbSQ5WVbW9rus3yiVjuRzKsODiqW0XH7q9UTmUgW6zTvS/VquVVqtVOkbjbNTf\nk436dfeTRjSSjKIC0MEfJjlT1/VcktR1PVdV1euL15f1A8K1TlZk7Zw+fTq/+c1vSsco7sYbb0yS\nDf97cfr06Rw/frx0DNaXVa0TJdeId73rXfnCF75Q7PM3xYkTJ/Liiy+WjlHUq6++etW12267rUCS\n8t7//vfnfe97X+kYjdDU9bIRjaT06SgqXHTLLbfk9ddfv1TfeuutBdMAVxoeHs7mzZtLx9iwdu/e\nXToCcIULFy5osi+yRpRnnbj24T0HDx4skAQWdFonBtc4y1UuG0X95uKlbyb5QFVV28ulgnfmi1/8\nYscaWJXXktxaVdVQkiy+vWXxOgBYJ+h7999/f1v9sY99rFASuL4mTCTZssC6sG3btpw9ezbbtm3L\n9PR0pqenS0eCdaGu619WVXUiyZ8neWrx7YueewFAYp1gfRgfH8/hw4cv1Q888EDBNNBZExpJq2Yc\nlSZ45JFHcuDAgXzpS1/K7bffXjoOG9w63LLwmSSHqqp6JMlMkk8UzgNAs1gn6Hv3339/Dh8+bBqJ\nxmtCI+nSKOriNJJRVPrSrl278q1vfat0DFiX6rr+uyR/XDoHAM1knWA9GB8fz/j4eOkYcF3Fn5FU\n1/Uvk1wcRU2MogIAAAA0UhMmkhKjqAAAAACN14hGklFUAAAAgOYrvrUNAAAAgP6gkQQAAADAsmgk\nAQAAALAsGkkAAAAALItGEgAAAADLopEEAAAAwLJoJAEAAACwLJtKB1iloSR5++23S+cAaJTLXheH\nSuZoAOsEwDVYJ5JYIwCW1Gmd6PdG0h8kycsvv1w6B0BT/UGSvy8doiDrBEBnG3mdsEYAXN9V60S/\nN5L+JsmfJvl/SeYKZwFokqEsvOj/TekghVknAK7NOmGNAOhkyXViYH5+fu3jAAAAANB3PGwbAAAA\ngGXRSAIAAABgWTSSAAAAAFgWjSQAAAAAluX/AxcSkKmhsszQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2,3,1)\n",
    "sns.set(style=\"whitegrid\") \n",
    "ax = sns.boxplot(y=da['MedInc'])\n",
    "plt.subplot(2,3,2)\n",
    "ax = sns.boxplot(y=da['HouseAge'])\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "ax = sns.boxplot(y=da['AveRooms'])\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "ax = sns.boxplot(y=da['AveBedrms'])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "ax = sns.boxplot(y=da['Population'])\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "ax = sns.boxplot(y=da['AveOccup'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOPHBtRkGAJb"
   },
   "source": [
    "After watching plots of 6 features i can say that there are some extremities/outliers especially in AveRooms ,AveBedrooms ,population and Average occupation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lidzZHBPGAJc"
   },
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eh8PD0M1GAJe"
   },
   "outputs": [],
   "source": [
    "# splitting data without removing extremities\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(da, d.target,test_size = 0.30,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Gct660OGAJh"
   },
   "source": [
    "##### Standardising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6803,
     "status": "ok",
     "timestamp": 1572237516440,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "m7XMVWA4GAJi",
    "outputId": "b8d206d8-3234-4f71-d84c-45ebe10a91e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of train : (14448, 8)\n",
      "dimension of test:  (6192, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc=preprocessing.StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "print('dimension of train :',X_train.shape)\n",
    "print('dimension of test: ',X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaqW0RvvGAJm"
   },
   "source": [
    " Just training with linear regression to see resultant error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Js4lu1E0GAJn"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P53WGQ0HGAJq"
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X=X_train, y=y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "valid_pred = lm.predict(X_test)\n",
    "train_pred = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6755,
     "status": "ok",
     "timestamp": 1572237516447,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "lvNOFffrGAJt",
    "outputId": "0d2271cf-da1e-44bf-edad-580c6a912f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on train by using linear regression 0.5217441346520719\n",
      "error on test by using linear regression 0.5306706395698603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('error on train by using linear regression',mean_squared_error(y_train, train_pred))\n",
    "print('error on test by using linear regression',mean_squared_error(y_test,valid_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMPjnKaIGAJw"
   },
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNxS_RqfGAJz"
   },
   "source": [
    "#### TRAINING NN with RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1 with relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 380055,
     "status": "ok",
     "timestamp": 1572237889770,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "8vAr-bYTGAJ1",
    "outputId": "8c467478-a452-4cd4-b955-82d38432b2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 14448 samples, validate on 6192 samples\n",
      "Epoch 1/600\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 5.3265 - val_loss: 4.2313\n",
      "Epoch 2/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 3.6638 - val_loss: 3.1046\n",
      "Epoch 3/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 2.7310 - val_loss: 2.3511\n",
      "Epoch 4/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 2.1220 - val_loss: 1.8738\n",
      "Epoch 5/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 1.7524 - val_loss: 1.5986\n",
      "Epoch 6/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 1.5435 - val_loss: 1.4359\n",
      "Epoch 7/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 1.3954 - val_loss: 1.3064\n",
      "Epoch 8/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 1.2598 - val_loss: 1.1561\n",
      "Epoch 9/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 1.1098 - val_loss: 1.0256\n",
      "Epoch 10/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.9951 - val_loss: 0.9266\n",
      "Epoch 11/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.9057 - val_loss: 0.8470\n",
      "Epoch 12/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.8335 - val_loss: 0.7851\n",
      "Epoch 13/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.7761 - val_loss: 0.7347\n",
      "Epoch 14/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.7298 - val_loss: 0.6949\n",
      "Epoch 15/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.6926 - val_loss: 0.6622\n",
      "Epoch 16/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.6618 - val_loss: 0.6354\n",
      "Epoch 17/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.6352 - val_loss: 0.6124\n",
      "Epoch 18/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.6126 - val_loss: 0.5941\n",
      "Epoch 19/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5949 - val_loss: 0.5783\n",
      "Epoch 20/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5791 - val_loss: 0.5663\n",
      "Epoch 21/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5667 - val_loss: 0.5566\n",
      "Epoch 22/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5559 - val_loss: 0.5486\n",
      "Epoch 23/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5465 - val_loss: 0.5419\n",
      "Epoch 24/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5390 - val_loss: 0.5358\n",
      "Epoch 25/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5323 - val_loss: 0.5313\n",
      "Epoch 26/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5267 - val_loss: 0.5272\n",
      "Epoch 27/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5223 - val_loss: 0.5245\n",
      "Epoch 28/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5187 - val_loss: 0.5216\n",
      "Epoch 29/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5154 - val_loss: 0.5195\n",
      "Epoch 30/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5135 - val_loss: 0.5185\n",
      "Epoch 31/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5111 - val_loss: 0.5175\n",
      "Epoch 32/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5099 - val_loss: 0.5155\n",
      "Epoch 33/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5084 - val_loss: 0.5143\n",
      "Epoch 34/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5073 - val_loss: 0.5139\n",
      "Epoch 35/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5064 - val_loss: 0.5141\n",
      "Epoch 36/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5058 - val_loss: 0.5124\n",
      "Epoch 37/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5052 - val_loss: 0.5122\n",
      "Epoch 38/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5048 - val_loss: 0.5122\n",
      "Epoch 39/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5041 - val_loss: 0.5122\n",
      "Epoch 40/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5037 - val_loss: 0.5118\n",
      "Epoch 41/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5036 - val_loss: 0.5112\n",
      "Epoch 42/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5032 - val_loss: 0.5111\n",
      "Epoch 43/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5026 - val_loss: 0.5110\n",
      "Epoch 44/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5028 - val_loss: 0.5110\n",
      "Epoch 45/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5025 - val_loss: 0.5098\n",
      "Epoch 46/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5019 - val_loss: 0.5111\n",
      "Epoch 47/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5019 - val_loss: 0.5099\n",
      "Epoch 48/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5019 - val_loss: 0.5110\n",
      "Epoch 49/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5016 - val_loss: 0.5099\n",
      "Epoch 50/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5013 - val_loss: 0.5094\n",
      "Epoch 51/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5014 - val_loss: 0.5097\n",
      "Epoch 52/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5009 - val_loss: 0.5106\n",
      "Epoch 53/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5008 - val_loss: 0.5109\n",
      "Epoch 54/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5016 - val_loss: 0.5103\n",
      "Epoch 55/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5009 - val_loss: 0.5097\n",
      "Epoch 56/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5010 - val_loss: 0.5093\n",
      "Epoch 57/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5009 - val_loss: 0.5102\n",
      "Epoch 58/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5006 - val_loss: 0.5098\n",
      "Epoch 59/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5008 - val_loss: 0.5096\n",
      "Epoch 60/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5009 - val_loss: 0.5116\n",
      "Epoch 61/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5005 - val_loss: 0.5101\n",
      "Epoch 62/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5106\n",
      "Epoch 63/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5012 - val_loss: 0.5095\n",
      "Epoch 64/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5108\n",
      "Epoch 65/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 66/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5005 - val_loss: 0.5098\n",
      "Epoch 67/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5004 - val_loss: 0.5096\n",
      "Epoch 68/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5099\n",
      "Epoch 69/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5003 - val_loss: 0.5103\n",
      "Epoch 70/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 71/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5112\n",
      "Epoch 72/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5007 - val_loss: 0.5097\n",
      "Epoch 73/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 74/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 75/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5111\n",
      "Epoch 76/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5004 - val_loss: 0.5101\n",
      "Epoch 77/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 78/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5094\n",
      "Epoch 79/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4998 - val_loss: 0.5100\n",
      "Epoch 80/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5007 - val_loss: 0.5096\n",
      "Epoch 81/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 82/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5093\n",
      "Epoch 83/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 84/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5102\n",
      "Epoch 85/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5115\n",
      "Epoch 86/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5094\n",
      "Epoch 87/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 88/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5103\n",
      "Epoch 89/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5104\n",
      "Epoch 90/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 91/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 92/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 93/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5102\n",
      "Epoch 94/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 95/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 96/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5095\n",
      "Epoch 97/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5097\n",
      "Epoch 98/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5002 - val_loss: 0.5103\n",
      "Epoch 99/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5102\n",
      "Epoch 100/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5092\n",
      "Epoch 101/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5103\n",
      "Epoch 102/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5003 - val_loss: 0.5097\n",
      "Epoch 103/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5108\n",
      "Epoch 104/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5102\n",
      "Epoch 105/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5100\n",
      "Epoch 106/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 107/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 108/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5102\n",
      "Epoch 109/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5104\n",
      "Epoch 110/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 111/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5105\n",
      "Epoch 112/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5116\n",
      "Epoch 113/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5106\n",
      "Epoch 114/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 115/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5096\n",
      "Epoch 116/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 117/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 118/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 119/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5117\n",
      "Epoch 120/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5101\n",
      "Epoch 121/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5005 - val_loss: 0.5100\n",
      "Epoch 122/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5128\n",
      "Epoch 123/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 124/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5118\n",
      "Epoch 125/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5099\n",
      "Epoch 126/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5004 - val_loss: 0.5098\n",
      "Epoch 127/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5098\n",
      "Epoch 128/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5111\n",
      "Epoch 129/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5107\n",
      "Epoch 130/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5115\n",
      "Epoch 131/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 132/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4996 - val_loss: 0.5098\n",
      "Epoch 133/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5005 - val_loss: 0.5106\n",
      "Epoch 134/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5005 - val_loss: 0.5096\n",
      "Epoch 135/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5109\n",
      "Epoch 136/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5093\n",
      "Epoch 137/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 138/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5095\n",
      "Epoch 139/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5111\n",
      "Epoch 140/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5110\n",
      "Epoch 141/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 142/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5108\n",
      "Epoch 143/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5097\n",
      "Epoch 144/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5103\n",
      "Epoch 145/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 146/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 147/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5097\n",
      "Epoch 148/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5093\n",
      "Epoch 149/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5102\n",
      "Epoch 150/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5108\n",
      "Epoch 151/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5004 - val_loss: 0.5099\n",
      "Epoch 152/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5095\n",
      "Epoch 153/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5115\n",
      "Epoch 154/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4998 - val_loss: 0.5100\n",
      "Epoch 155/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 156/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 157/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5129\n",
      "Epoch 158/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4998 - val_loss: 0.5108\n",
      "Epoch 159/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5005 - val_loss: 0.5096\n",
      "Epoch 160/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 161/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 162/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5108\n",
      "Epoch 163/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 164/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5096\n",
      "Epoch 165/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5109\n",
      "Epoch 166/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5103\n",
      "Epoch 167/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 168/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 169/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5107\n",
      "Epoch 170/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5002 - val_loss: 0.5099\n",
      "Epoch 171/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5113\n",
      "Epoch 172/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 173/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5105\n",
      "Epoch 174/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5097\n",
      "Epoch 175/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5095\n",
      "Epoch 176/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 177/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5104\n",
      "Epoch 178/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5109\n",
      "Epoch 179/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 180/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5107\n",
      "Epoch 181/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 182/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4998 - val_loss: 0.5100\n",
      "Epoch 183/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5011 - val_loss: 0.5103\n",
      "Epoch 184/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 185/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5107\n",
      "Epoch 186/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 187/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 188/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 189/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5104\n",
      "Epoch 190/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5092\n",
      "Epoch 191/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 192/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 193/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5005 - val_loss: 0.5097\n",
      "Epoch 194/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4997 - val_loss: 0.5099\n",
      "Epoch 195/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 196/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 197/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 198/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 199/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5110\n",
      "Epoch 200/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5106\n",
      "Epoch 201/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 202/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5107\n",
      "Epoch 203/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 204/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5105\n",
      "Epoch 205/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5002 - val_loss: 0.5128\n",
      "Epoch 206/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5101\n",
      "Epoch 207/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 208/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5128\n",
      "Epoch 209/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 210/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5120\n",
      "Epoch 211/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5112\n",
      "Epoch 212/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 213/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5122\n",
      "Epoch 214/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 215/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5101\n",
      "Epoch 216/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 217/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5093\n",
      "Epoch 218/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5101\n",
      "Epoch 219/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5109\n",
      "Epoch 220/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 221/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 222/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 223/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 224/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5106\n",
      "Epoch 225/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 226/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5094\n",
      "Epoch 227/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 228/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5106\n",
      "Epoch 229/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 230/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 231/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5105\n",
      "Epoch 232/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5095\n",
      "Epoch 233/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5001 - val_loss: 0.5094\n",
      "Epoch 234/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5099\n",
      "Epoch 235/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5108\n",
      "Epoch 236/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5094\n",
      "Epoch 237/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5095\n",
      "Epoch 238/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5005 - val_loss: 0.5101\n",
      "Epoch 239/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4997 - val_loss: 0.5101\n",
      "Epoch 240/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5001 - val_loss: 0.5096\n",
      "Epoch 241/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5125\n",
      "Epoch 242/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 243/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5118\n",
      "Epoch 244/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 245/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 246/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 247/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5097\n",
      "Epoch 248/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5111\n",
      "Epoch 249/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 250/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5113\n",
      "Epoch 251/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 252/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5095\n",
      "Epoch 253/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 254/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5002 - val_loss: 0.5115\n",
      "Epoch 255/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5103\n",
      "Epoch 256/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5101\n",
      "Epoch 257/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 258/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 259/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5104\n",
      "Epoch 260/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 261/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4998 - val_loss: 0.5096\n",
      "Epoch 262/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 263/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5000 - val_loss: 0.5102\n",
      "Epoch 264/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5001 - val_loss: 0.5095\n",
      "Epoch 265/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.4998 - val_loss: 0.5106\n",
      "Epoch 266/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 267/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4996 - val_loss: 0.5109\n",
      "Epoch 268/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5002 - val_loss: 0.5103\n",
      "Epoch 269/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5005 - val_loss: 0.5106\n",
      "Epoch 270/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 271/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 272/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5002 - val_loss: 0.5099\n",
      "Epoch 273/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 274/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5106\n",
      "Epoch 275/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 276/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 277/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5126\n",
      "Epoch 278/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 279/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 280/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5112\n",
      "Epoch 281/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5106\n",
      "Epoch 282/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5094\n",
      "Epoch 283/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 284/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5110\n",
      "Epoch 285/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 286/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 287/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5101\n",
      "Epoch 288/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 289/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5119\n",
      "Epoch 290/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 291/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 292/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5104\n",
      "Epoch 293/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 294/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4995 - val_loss: 0.5110\n",
      "Epoch 295/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5112\n",
      "Epoch 296/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5094\n",
      "Epoch 297/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4997 - val_loss: 0.5107\n",
      "Epoch 298/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 299/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5144\n",
      "Epoch 300/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5102\n",
      "Epoch 301/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 302/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5103\n",
      "Epoch 303/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5103\n",
      "Epoch 304/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 305/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 306/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5097\n",
      "Epoch 307/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 308/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4997 - val_loss: 0.5110\n",
      "Epoch 309/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 310/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4998 - val_loss: 0.5097\n",
      "Epoch 311/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5108\n",
      "Epoch 312/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5106\n",
      "Epoch 313/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5099\n",
      "Epoch 314/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4995 - val_loss: 0.5107\n",
      "Epoch 315/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 316/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 317/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 318/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 319/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5114\n",
      "Epoch 320/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5116\n",
      "Epoch 321/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 322/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5006 - val_loss: 0.5097\n",
      "Epoch 323/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 324/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4996 - val_loss: 0.5102\n",
      "Epoch 325/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5110\n",
      "Epoch 326/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 327/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5096\n",
      "Epoch 328/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5126\n",
      "Epoch 329/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5101\n",
      "Epoch 330/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 331/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5112\n",
      "Epoch 332/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5133\n",
      "Epoch 333/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 334/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 335/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 336/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4999 - val_loss: 0.5095\n",
      "Epoch 337/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 338/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 339/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 340/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 341/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 342/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5101\n",
      "Epoch 343/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5095\n",
      "Epoch 344/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 345/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 346/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5127\n",
      "Epoch 347/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5102\n",
      "Epoch 348/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5109\n",
      "Epoch 349/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 350/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 351/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5111\n",
      "Epoch 352/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 353/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5097\n",
      "Epoch 354/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5096\n",
      "Epoch 355/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5095\n",
      "Epoch 356/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5122\n",
      "Epoch 357/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.4998 - val_loss: 0.5115\n",
      "Epoch 358/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5110\n",
      "Epoch 359/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 360/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 361/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 362/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 363/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5104\n",
      "Epoch 364/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5102\n",
      "Epoch 365/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 366/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5095\n",
      "Epoch 367/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 368/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5099\n",
      "Epoch 369/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5094\n",
      "Epoch 370/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 371/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 372/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 373/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5111\n",
      "Epoch 374/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5106\n",
      "Epoch 375/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.4998 - val_loss: 0.5108\n",
      "Epoch 376/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5099\n",
      "Epoch 377/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 378/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4995 - val_loss: 0.5117\n",
      "Epoch 379/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 380/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 381/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5116\n",
      "Epoch 382/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5096\n",
      "Epoch 383/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5108\n",
      "Epoch 384/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 385/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 386/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5005 - val_loss: 0.5100\n",
      "Epoch 387/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 388/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5101\n",
      "Epoch 389/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4998 - val_loss: 0.5101\n",
      "Epoch 390/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5122\n",
      "Epoch 391/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5108\n",
      "Epoch 392/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4998 - val_loss: 0.5112\n",
      "Epoch 393/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5112\n",
      "Epoch 394/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5105\n",
      "Epoch 395/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 396/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5106\n",
      "Epoch 397/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5002 - val_loss: 0.5098\n",
      "Epoch 398/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 399/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 400/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5116\n",
      "Epoch 401/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5107\n",
      "Epoch 402/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5103\n",
      "Epoch 403/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 404/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 405/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 406/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5099\n",
      "Epoch 407/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5112\n",
      "Epoch 408/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5112\n",
      "Epoch 409/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5109\n",
      "Epoch 410/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 411/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4997 - val_loss: 0.5103\n",
      "Epoch 412/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4998 - val_loss: 0.5114\n",
      "Epoch 413/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 414/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 415/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 416/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5103\n",
      "Epoch 417/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5107\n",
      "Epoch 418/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 419/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 420/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5095\n",
      "Epoch 421/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5105\n",
      "Epoch 422/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 423/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 424/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 425/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5112\n",
      "Epoch 426/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 427/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4996 - val_loss: 0.5097\n",
      "Epoch 428/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 429/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 430/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5103\n",
      "Epoch 431/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5096\n",
      "Epoch 432/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 433/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5097\n",
      "Epoch 434/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 435/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5096\n",
      "Epoch 436/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5103\n",
      "Epoch 437/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5004 - val_loss: 0.5102\n",
      "Epoch 438/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5102\n",
      "Epoch 439/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5108\n",
      "Epoch 440/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5114\n",
      "Epoch 441/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 442/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5106\n",
      "Epoch 443/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4996 - val_loss: 0.5103\n",
      "Epoch 444/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 445/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4995 - val_loss: 0.5121\n",
      "Epoch 446/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5100\n",
      "Epoch 447/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4999 - val_loss: 0.5120\n",
      "Epoch 448/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 449/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5105\n",
      "Epoch 450/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5104\n",
      "Epoch 451/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5106\n",
      "Epoch 452/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5105\n",
      "Epoch 453/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4999 - val_loss: 0.5123\n",
      "Epoch 454/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5006 - val_loss: 0.5109\n",
      "Epoch 455/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5106\n",
      "Epoch 456/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 457/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 458/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 459/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5105\n",
      "Epoch 460/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 461/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5003 - val_loss: 0.5103\n",
      "Epoch 462/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4997 - val_loss: 0.5102\n",
      "Epoch 463/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5003 - val_loss: 0.5108\n",
      "Epoch 464/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4997 - val_loss: 0.5107\n",
      "Epoch 465/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 466/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.4997 - val_loss: 0.5107\n",
      "Epoch 467/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 468/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 469/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.4998 - val_loss: 0.5105\n",
      "Epoch 470/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5121\n",
      "Epoch 471/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 472/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5105\n",
      "Epoch 473/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5100\n",
      "Epoch 474/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5109\n",
      "Epoch 475/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 476/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4997 - val_loss: 0.5107\n",
      "Epoch 477/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5099\n",
      "Epoch 478/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5109\n",
      "Epoch 479/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5003 - val_loss: 0.5103\n",
      "Epoch 480/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 481/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4995 - val_loss: 0.5103\n",
      "Epoch 482/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5110\n",
      "Epoch 483/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5102\n",
      "Epoch 484/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 485/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 486/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4996 - val_loss: 0.5110\n",
      "Epoch 487/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5099\n",
      "Epoch 488/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 489/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5096\n",
      "Epoch 490/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 491/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5097\n",
      "Epoch 492/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4996 - val_loss: 0.5102\n",
      "Epoch 493/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 494/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5100\n",
      "Epoch 495/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 496/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 497/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 498/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4995 - val_loss: 0.5109\n",
      "Epoch 499/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5097\n",
      "Epoch 500/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 501/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5112\n",
      "Epoch 502/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 503/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5099\n",
      "Epoch 504/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5106\n",
      "Epoch 505/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 506/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5112\n",
      "Epoch 507/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 508/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5110\n",
      "Epoch 509/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 510/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 511/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5111\n",
      "Epoch 512/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5106\n",
      "Epoch 513/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5113\n",
      "Epoch 514/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5104\n",
      "Epoch 515/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 516/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5094\n",
      "Epoch 517/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5104\n",
      "Epoch 518/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5106\n",
      "Epoch 519/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4998 - val_loss: 0.5111\n",
      "Epoch 520/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5109\n",
      "Epoch 521/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5105\n",
      "Epoch 522/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5110\n",
      "Epoch 523/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.5003 - val_loss: 0.5103\n",
      "Epoch 524/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5099\n",
      "Epoch 525/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 526/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 527/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 528/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5098\n",
      "Epoch 529/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 530/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5100\n",
      "Epoch 531/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4999 - val_loss: 0.5106\n",
      "Epoch 532/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5101\n",
      "Epoch 533/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5111\n",
      "Epoch 534/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5102\n",
      "Epoch 535/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5109\n",
      "Epoch 536/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5109\n",
      "Epoch 537/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4996 - val_loss: 0.5121\n",
      "Epoch 538/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5101\n",
      "Epoch 539/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 540/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5105\n",
      "Epoch 541/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5001 - val_loss: 0.5128\n",
      "Epoch 542/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5102\n",
      "Epoch 543/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5003 - val_loss: 0.5107\n",
      "Epoch 544/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4997 - val_loss: 0.5116\n",
      "Epoch 545/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 546/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5098\n",
      "Epoch 547/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5099\n",
      "Epoch 548/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5004 - val_loss: 0.5095\n",
      "Epoch 549/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4998 - val_loss: 0.5101\n",
      "Epoch 550/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5117\n",
      "Epoch 551/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5000 - val_loss: 0.5107\n",
      "Epoch 552/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5103\n",
      "Epoch 553/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5096\n",
      "Epoch 554/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4999 - val_loss: 0.5098\n",
      "Epoch 555/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 556/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5098\n",
      "Epoch 557/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4997 - val_loss: 0.5111\n",
      "Epoch 558/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5002 - val_loss: 0.5099\n",
      "Epoch 559/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4994 - val_loss: 0.5111\n",
      "Epoch 560/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4993 - val_loss: 0.5139\n",
      "Epoch 561/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5100\n",
      "Epoch 562/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5112\n",
      "Epoch 563/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 564/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5104\n",
      "Epoch 565/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5101\n",
      "Epoch 566/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4999 - val_loss: 0.5102\n",
      "Epoch 567/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5107\n",
      "Epoch 568/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4998 - val_loss: 0.5096\n",
      "Epoch 569/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5112\n",
      "Epoch 570/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5097\n",
      "Epoch 571/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5002 - val_loss: 0.5100\n",
      "Epoch 572/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5117\n",
      "Epoch 573/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4995 - val_loss: 0.5098\n",
      "Epoch 574/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5097\n",
      "Epoch 575/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4998 - val_loss: 0.5106\n",
      "Epoch 576/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.5001 - val_loss: 0.5109\n",
      "Epoch 577/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5103\n",
      "Epoch 578/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 579/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4996 - val_loss: 0.5101\n",
      "Epoch 580/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 581/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.5000 - val_loss: 0.5105\n",
      "Epoch 582/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5001 - val_loss: 0.5098\n",
      "Epoch 583/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.5000 - val_loss: 0.5098\n",
      "Epoch 584/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 585/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5097\n",
      "Epoch 586/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5004 - val_loss: 0.5097\n",
      "Epoch 587/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4998 - val_loss: 0.5102\n",
      "Epoch 588/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5096\n",
      "Epoch 589/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5000 - val_loss: 0.5107\n",
      "Epoch 590/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4997 - val_loss: 0.5099\n",
      "Epoch 591/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5000 - val_loss: 0.5100\n",
      "Epoch 592/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 593/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4995 - val_loss: 0.5097\n",
      "Epoch 594/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4999 - val_loss: 0.5101\n",
      "Epoch 595/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.5001 - val_loss: 0.5104\n",
      "Epoch 596/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5095\n",
      "Epoch 597/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4998 - val_loss: 0.5096\n",
      "Epoch 598/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.4999 - val_loss: 0.5099\n",
      "Epoch 599/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4998 - val_loss: 0.5104\n",
      "Epoch 600/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.5000 - val_loss: 0.5098\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAELCAYAAAASrNdlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGNpERAby/wFGEDUoHcX3QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(1 ,activation='relu', input_shape=(8,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1 with sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 759687,
     "status": "ok",
     "timestamp": 1572238269420,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "B2dtqkQrGAJ9",
    "outputId": "334bcccc-cd46-4e6a-d26a-4209199f1462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14448 samples, validate on 6192 samples\n",
      "Epoch 1/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 2.3008 - val_loss: 1.5599\n",
      "Epoch 2/600\n",
      "14448/14448 [==============================] - 1s 39us/step - loss: 1.1612 - val_loss: 0.9684\n",
      "Epoch 3/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.8577 - val_loss: 0.8053\n",
      "Epoch 4/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.7479 - val_loss: 0.7193\n",
      "Epoch 5/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.6781 - val_loss: 0.6586\n",
      "Epoch 6/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.6256 - val_loss: 0.6120\n",
      "Epoch 7/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5832 - val_loss: 0.5740\n",
      "Epoch 8/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5477 - val_loss: 0.5422\n",
      "Epoch 9/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.5180 - val_loss: 0.5162\n",
      "Epoch 10/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4936 - val_loss: 0.4936\n",
      "Epoch 11/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4735 - val_loss: 0.4762\n",
      "Epoch 12/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4573 - val_loss: 0.4624\n",
      "Epoch 13/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4442 - val_loss: 0.4503\n",
      "Epoch 14/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4339 - val_loss: 0.4416\n",
      "Epoch 15/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4256 - val_loss: 0.4350\n",
      "Epoch 16/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.4194 - val_loss: 0.4294\n",
      "Epoch 17/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4144 - val_loss: 0.4252\n",
      "Epoch 18/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.4104 - val_loss: 0.4221\n",
      "Epoch 19/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4073 - val_loss: 0.4196\n",
      "Epoch 20/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.4044 - val_loss: 0.4172\n",
      "Epoch 21/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.4027 - val_loss: 0.4154\n",
      "Epoch 22/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.4008 - val_loss: 0.4138\n",
      "Epoch 23/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3993 - val_loss: 0.4121\n",
      "Epoch 24/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3980 - val_loss: 0.4117\n",
      "Epoch 25/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3969 - val_loss: 0.4107\n",
      "Epoch 26/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3962 - val_loss: 0.4092\n",
      "Epoch 27/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3954 - val_loss: 0.4098\n",
      "Epoch 28/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3949 - val_loss: 0.4082\n",
      "Epoch 29/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3942 - val_loss: 0.4071\n",
      "Epoch 30/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3937 - val_loss: 0.4073\n",
      "Epoch 31/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3934 - val_loss: 0.4068\n",
      "Epoch 32/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3929 - val_loss: 0.4066\n",
      "Epoch 33/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3926 - val_loss: 0.4070\n",
      "Epoch 34/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3923 - val_loss: 0.4055\n",
      "Epoch 35/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3922 - val_loss: 0.4046\n",
      "Epoch 36/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3920 - val_loss: 0.4047\n",
      "Epoch 37/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3920 - val_loss: 0.4048\n",
      "Epoch 38/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3915 - val_loss: 0.4046\n",
      "Epoch 39/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3913 - val_loss: 0.4045\n",
      "Epoch 40/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3914 - val_loss: 0.4044\n",
      "Epoch 41/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3912 - val_loss: 0.4038\n",
      "Epoch 42/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3913 - val_loss: 0.4039\n",
      "Epoch 43/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3910 - val_loss: 0.4042\n",
      "Epoch 44/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3913 - val_loss: 0.4032\n",
      "Epoch 45/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3909 - val_loss: 0.4034\n",
      "Epoch 46/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3908 - val_loss: 0.4031\n",
      "Epoch 47/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3907 - val_loss: 0.4034\n",
      "Epoch 48/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3906 - val_loss: 0.4041\n",
      "Epoch 49/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.3906 - val_loss: 0.4034\n",
      "Epoch 50/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3907 - val_loss: 0.4031\n",
      "Epoch 51/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.3906 - val_loss: 0.4038\n",
      "Epoch 52/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3906 - val_loss: 0.4031\n",
      "Epoch 53/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3905 - val_loss: 0.4031\n",
      "Epoch 54/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3904 - val_loss: 0.4031\n",
      "Epoch 55/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3905 - val_loss: 0.4031\n",
      "Epoch 56/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3905 - val_loss: 0.4027\n",
      "Epoch 57/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3904 - val_loss: 0.4027\n",
      "Epoch 58/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3904 - val_loss: 0.4029\n",
      "Epoch 59/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3905 - val_loss: 0.4023\n",
      "Epoch 60/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4027\n",
      "Epoch 61/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4037\n",
      "Epoch 62/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3904 - val_loss: 0.4033\n",
      "Epoch 63/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 64/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4035\n",
      "Epoch 65/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4033\n",
      "Epoch 66/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4023\n",
      "Epoch 67/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4031\n",
      "Epoch 68/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3903 - val_loss: 0.4029\n",
      "Epoch 69/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.3902 - val_loss: 0.4021\n",
      "Epoch 70/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3902 - val_loss: 0.4027\n",
      "Epoch 71/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3904 - val_loss: 0.4029\n",
      "Epoch 72/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3902 - val_loss: 0.4031\n",
      "Epoch 73/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4030\n",
      "Epoch 74/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3902 - val_loss: 0.4030\n",
      "Epoch 75/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 76/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4022\n",
      "Epoch 77/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4027\n",
      "Epoch 78/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4022\n",
      "Epoch 79/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3902 - val_loss: 0.4022\n",
      "Epoch 80/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 81/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 82/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 83/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4026\n",
      "Epoch 84/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 85/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 86/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4021\n",
      "Epoch 87/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 88/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 89/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 90/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 91/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 92/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 93/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 94/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 95/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 96/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4020\n",
      "Epoch 97/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 98/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4030\n",
      "Epoch 99/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4031\n",
      "Epoch 100/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 101/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4021\n",
      "Epoch 102/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4033\n",
      "Epoch 103/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3903 - val_loss: 0.4029\n",
      "Epoch 104/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4023\n",
      "Epoch 105/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4027\n",
      "Epoch 106/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 107/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 108/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4023\n",
      "Epoch 109/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 110/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 111/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 112/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 113/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 114/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 115/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 116/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 117/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4022\n",
      "Epoch 118/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 119/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 120/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 121/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 122/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 123/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4023\n",
      "Epoch 124/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4019\n",
      "Epoch 125/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 126/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4032\n",
      "Epoch 127/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4031\n",
      "Epoch 128/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 129/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4023\n",
      "Epoch 130/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 131/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 132/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4025\n",
      "Epoch 133/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 134/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 135/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4028\n",
      "Epoch 136/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 137/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 138/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 139/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 140/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4035\n",
      "Epoch 141/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 142/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 143/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 144/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4025\n",
      "Epoch 145/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4028\n",
      "Epoch 146/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 147/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 148/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 149/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 150/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4033\n",
      "Epoch 151/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 152/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 153/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 154/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3898 - val_loss: 0.4023\n",
      "Epoch 155/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 156/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 157/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 158/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 159/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 160/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3902 - val_loss: 0.4027\n",
      "Epoch 161/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 162/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 163/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3901 - val_loss: 0.4020\n",
      "Epoch 164/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 165/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3901 - val_loss: 0.4029\n",
      "Epoch 166/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3902 - val_loss: 0.4022\n",
      "Epoch 167/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 168/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 169/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3902 - val_loss: 0.4026\n",
      "Epoch 170/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 171/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 172/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 173/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 174/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 175/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 176/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4027\n",
      "Epoch 177/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 178/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 179/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 180/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4033\n",
      "Epoch 181/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 182/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 183/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4035\n",
      "Epoch 184/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 185/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3902 - val_loss: 0.4020\n",
      "Epoch 186/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 187/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 188/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4030\n",
      "Epoch 189/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4031\n",
      "Epoch 190/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 191/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 192/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 193/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 194/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 195/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 196/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 197/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 198/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4023\n",
      "Epoch 199/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 200/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 201/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 202/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4016\n",
      "Epoch 203/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 204/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 205/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3903 - val_loss: 0.4024\n",
      "Epoch 206/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 207/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 208/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 209/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 210/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 211/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 212/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 213/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4030\n",
      "Epoch 214/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4025\n",
      "Epoch 215/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4030\n",
      "Epoch 216/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 217/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 218/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 219/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 220/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4034\n",
      "Epoch 221/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 222/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4022\n",
      "Epoch 223/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 224/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4035\n",
      "Epoch 225/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 226/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 227/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4033\n",
      "Epoch 228/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4024\n",
      "Epoch 229/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 230/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3899 - val_loss: 0.4036\n",
      "Epoch 231/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4033\n",
      "Epoch 232/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 233/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 234/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 235/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 236/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 237/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 238/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 239/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 240/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 241/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 242/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 243/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 244/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 245/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4022\n",
      "Epoch 246/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3897 - val_loss: 0.4040\n",
      "Epoch 247/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 248/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 249/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 250/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 251/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 252/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 253/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 254/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 255/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 256/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 257/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 258/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 259/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 260/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 261/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 262/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 263/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 264/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4021\n",
      "Epoch 265/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 266/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 267/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3903 - val_loss: 0.4023\n",
      "Epoch 268/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 269/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 270/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 271/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 272/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 273/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 274/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 275/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4022\n",
      "Epoch 276/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 277/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 278/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 279/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 280/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4027\n",
      "Epoch 281/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 282/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 283/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 284/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 285/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4020\n",
      "Epoch 286/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 287/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 288/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 289/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 290/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 291/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 292/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3897 - val_loss: 0.4031\n",
      "Epoch 293/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4019\n",
      "Epoch 294/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 295/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 296/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 297/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 298/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4022\n",
      "Epoch 299/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 300/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 301/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 302/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 303/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 304/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 305/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3898 - val_loss: 0.4035\n",
      "Epoch 306/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4025\n",
      "Epoch 307/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 308/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 309/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 310/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 311/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 312/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3896 - val_loss: 0.4039\n",
      "Epoch 313/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 314/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 315/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 316/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 317/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 318/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 319/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 320/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 321/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 322/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 323/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 324/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 325/600\n",
      "14448/14448 [==============================] - 1s 41us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 326/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 327/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 328/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 329/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 330/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 331/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 332/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4023\n",
      "Epoch 333/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 334/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 335/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4035\n",
      "Epoch 336/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 337/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4035\n",
      "Epoch 338/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 339/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4035\n",
      "Epoch 340/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 341/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4021\n",
      "Epoch 342/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 343/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 344/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 345/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 346/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3898 - val_loss: 0.4032\n",
      "Epoch 347/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 348/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 349/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.3899 - val_loss: 0.4036\n",
      "Epoch 350/600\n",
      "14448/14448 [==============================] - 1s 48us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 351/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3902 - val_loss: 0.4022\n",
      "Epoch 352/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 353/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 354/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 355/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 356/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 357/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 358/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 359/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 360/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 361/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 362/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3900 - val_loss: 0.4033\n",
      "Epoch 363/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4033\n",
      "Epoch 364/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4042\n",
      "Epoch 365/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4034\n",
      "Epoch 366/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 367/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 368/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 369/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 370/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 371/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4034\n",
      "Epoch 372/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 373/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 374/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 375/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 376/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 377/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 378/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 379/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 380/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 381/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 382/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 383/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 384/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4034\n",
      "Epoch 385/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 386/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 387/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 388/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4037\n",
      "Epoch 389/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 390/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 391/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 392/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 393/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4030\n",
      "Epoch 394/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 395/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 396/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3897 - val_loss: 0.4035\n",
      "Epoch 397/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4023\n",
      "Epoch 398/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 399/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4031\n",
      "Epoch 400/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4034\n",
      "Epoch 401/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4026\n",
      "Epoch 402/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 403/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 404/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4034\n",
      "Epoch 405/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 406/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4029\n",
      "Epoch 407/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4033\n",
      "Epoch 408/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4030\n",
      "Epoch 409/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4030\n",
      "Epoch 410/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 411/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4016\n",
      "Epoch 412/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 413/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 414/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 415/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4023\n",
      "Epoch 416/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4033\n",
      "Epoch 417/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 418/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 419/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4034\n",
      "Epoch 420/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4033\n",
      "Epoch 421/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4034\n",
      "Epoch 422/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 423/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 424/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 425/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 426/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 427/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 428/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 429/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 430/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 431/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4025\n",
      "Epoch 432/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 433/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 434/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 435/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 436/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3898 - val_loss: 0.4030\n",
      "Epoch 437/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 438/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 439/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4032\n",
      "Epoch 440/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4029\n",
      "Epoch 441/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 442/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4029\n",
      "Epoch 443/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 444/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4025\n",
      "Epoch 445/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 446/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 447/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4024\n",
      "Epoch 448/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 449/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 450/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 451/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 452/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 453/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 454/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 455/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 456/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 457/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4024\n",
      "Epoch 458/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 459/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 460/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 461/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 462/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4032\n",
      "Epoch 463/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 464/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3902 - val_loss: 0.4025\n",
      "Epoch 465/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 466/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 467/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 468/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 469/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 470/600\n",
      "14448/14448 [==============================] - 1s 47us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 471/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 472/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4023\n",
      "Epoch 473/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 474/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3901 - val_loss: 0.4031\n",
      "Epoch 475/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 476/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 477/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 478/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 479/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 480/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 481/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 482/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 483/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4018\n",
      "Epoch 484/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 485/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 486/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4021\n",
      "Epoch 487/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 488/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 489/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3898 - val_loss: 0.4026\n",
      "Epoch 490/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 491/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 492/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4019\n",
      "Epoch 493/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 494/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4020\n",
      "Epoch 495/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4019\n",
      "Epoch 496/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 497/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 498/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 499/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 500/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3902 - val_loss: 0.4024\n",
      "Epoch 501/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 502/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3897 - val_loss: 0.4026\n",
      "Epoch 503/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 504/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3897 - val_loss: 0.4031\n",
      "Epoch 505/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 506/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4032\n",
      "Epoch 507/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 508/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 509/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4028\n",
      "Epoch 510/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 511/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 512/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4029\n",
      "Epoch 513/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 514/600\n",
      "14448/14448 [==============================] - 1s 46us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 515/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 516/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 517/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 518/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 519/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 520/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 521/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4023\n",
      "Epoch 522/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4028\n",
      "Epoch 523/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 524/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 525/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3897 - val_loss: 0.4025\n",
      "Epoch 526/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4035\n",
      "Epoch 527/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 528/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 529/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 530/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 531/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 532/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 533/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 534/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 535/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4027\n",
      "Epoch 536/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 537/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 538/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3898 - val_loss: 0.4029\n",
      "Epoch 539/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 540/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 541/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4039\n",
      "Epoch 542/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3903 - val_loss: 0.4031\n",
      "Epoch 543/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3900 - val_loss: 0.4019\n",
      "Epoch 544/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 545/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 546/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 547/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 548/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4029\n",
      "Epoch 549/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4033\n",
      "Epoch 550/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 551/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 552/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 553/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 554/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 555/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4026\n",
      "Epoch 556/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 557/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 558/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4026\n",
      "Epoch 559/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4024\n",
      "Epoch 560/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4031\n",
      "Epoch 561/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4031\n",
      "Epoch 562/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4029\n",
      "Epoch 563/600\n",
      "14448/14448 [==============================] - 1s 45us/step - loss: 0.3901 - val_loss: 0.4020\n",
      "Epoch 564/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 565/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3899 - val_loss: 0.4036\n",
      "Epoch 566/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4024\n",
      "Epoch 567/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4034\n",
      "Epoch 568/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4032\n",
      "Epoch 569/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 570/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 571/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4030\n",
      "Epoch 572/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 573/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4034\n",
      "Epoch 574/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 575/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4028\n",
      "Epoch 576/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4024\n",
      "Epoch 577/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4023\n",
      "Epoch 578/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3901 - val_loss: 0.4025\n",
      "Epoch 579/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 580/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4029\n",
      "Epoch 581/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4022\n",
      "Epoch 582/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 583/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 584/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4023\n",
      "Epoch 585/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 586/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3897 - val_loss: 0.4023\n",
      "Epoch 587/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3899 - val_loss: 0.4030\n",
      "Epoch 588/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4025\n",
      "Epoch 589/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4029\n",
      "Epoch 590/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4020\n",
      "Epoch 591/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4026\n",
      "Epoch 592/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4028\n",
      "Epoch 593/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4031\n",
      "Epoch 594/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3901 - val_loss: 0.4027\n",
      "Epoch 595/600\n",
      "14448/14448 [==============================] - 1s 42us/step - loss: 0.3900 - val_loss: 0.4027\n",
      "Epoch 596/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4025\n",
      "Epoch 597/600\n",
      "14448/14448 [==============================] - 1s 44us/step - loss: 0.3900 - val_loss: 0.4022\n",
      "Epoch 598/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3900 - val_loss: 0.4031\n",
      "Epoch 599/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3898 - val_loss: 0.4022\n",
      "Epoch 600/600\n",
      "14448/14448 [==============================] - 1s 43us/step - loss: 0.3899 - val_loss: 0.4024\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAELCAYAAAAx94awAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEXkhhj8RkRdi+BMReaH/D3QZhU65sQYaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(8,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 with relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1212447,
     "status": "ok",
     "timestamp": 1572238722197,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "zSMxZcBhGAJ5",
    "outputId": "2e07e6e7-3b75-429a-9977-501079f6cfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14448 samples, validate on 6192 samples\n",
      "Epoch 1/600\n",
      "14448/14448 [==============================] - 1s 79us/step - loss: 0.7133 - val_loss: 0.5034\n",
      "Epoch 2/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.4216 - val_loss: 0.6245\n",
      "Epoch 3/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.8555 - val_loss: 0.3949\n",
      "Epoch 4/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.4053 - val_loss: 0.3830\n",
      "Epoch 5/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3864 - val_loss: 0.3675\n",
      "Epoch 6/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3545 - val_loss: 0.3663\n",
      "Epoch 7/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3605 - val_loss: 0.3544\n",
      "Epoch 8/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3421 - val_loss: 0.3634\n",
      "Epoch 9/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3596 - val_loss: 0.5425\n",
      "Epoch 10/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.4561 - val_loss: 0.3595\n",
      "Epoch 11/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3354 - val_loss: 0.3482\n",
      "Epoch 12/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3287 - val_loss: 0.3428\n",
      "Epoch 13/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3263 - val_loss: 0.3407\n",
      "Epoch 14/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3312 - val_loss: 0.3643\n",
      "Epoch 15/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.4076 - val_loss: 0.3388\n",
      "Epoch 16/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3232 - val_loss: 0.3427\n",
      "Epoch 17/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3203 - val_loss: 0.3348\n",
      "Epoch 18/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3199 - val_loss: 0.3343\n",
      "Epoch 19/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3347 - val_loss: 0.3673\n",
      "Epoch 20/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3129 - val_loss: 0.3304\n",
      "Epoch 21/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3136 - val_loss: 0.3333\n",
      "Epoch 22/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3060 - val_loss: 0.3290\n",
      "Epoch 23/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3538 - val_loss: 0.3345\n",
      "Epoch 24/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3144 - val_loss: 0.3285\n",
      "Epoch 25/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3050 - val_loss: 0.3294\n",
      "Epoch 26/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3052 - val_loss: 0.3240\n",
      "Epoch 27/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.3155 - val_loss: 0.3262\n",
      "Epoch 28/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3014 - val_loss: 0.3194\n",
      "Epoch 29/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3108 - val_loss: 0.3198\n",
      "Epoch 30/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3033 - val_loss: 0.3316\n",
      "Epoch 31/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3150 - val_loss: 0.3290\n",
      "Epoch 32/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3030 - val_loss: 0.3167\n",
      "Epoch 33/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3081 - val_loss: 0.3151\n",
      "Epoch 34/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2975 - val_loss: 0.3175\n",
      "Epoch 35/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2964 - val_loss: 0.3195\n",
      "Epoch 36/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2973 - val_loss: 0.3183\n",
      "Epoch 37/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.3160 - val_loss: 0.3267\n",
      "Epoch 38/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2976 - val_loss: 0.3216\n",
      "Epoch 39/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2925 - val_loss: 0.3114\n",
      "Epoch 40/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2915 - val_loss: 0.3122\n",
      "Epoch 41/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2927 - val_loss: 0.3152\n",
      "Epoch 42/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2907 - val_loss: 0.3091\n",
      "Epoch 43/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2985 - val_loss: 0.3275\n",
      "Epoch 44/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2894 - val_loss: 0.3198\n",
      "Epoch 45/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2886 - val_loss: 0.3099\n",
      "Epoch 46/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2871 - val_loss: 0.3147\n",
      "Epoch 47/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2913 - val_loss: 0.3251\n",
      "Epoch 48/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.3008 - val_loss: 0.3099\n",
      "Epoch 49/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2981 - val_loss: 0.3134\n",
      "Epoch 50/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2881 - val_loss: 0.3126\n",
      "Epoch 51/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2972 - val_loss: 0.3195\n",
      "Epoch 52/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2942 - val_loss: 0.3147\n",
      "Epoch 53/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2884 - val_loss: 0.3084\n",
      "Epoch 54/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3083 - val_loss: 0.3161\n",
      "Epoch 55/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2868 - val_loss: 0.3097\n",
      "Epoch 56/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2867 - val_loss: 0.3073\n",
      "Epoch 57/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2861 - val_loss: 0.3198\n",
      "Epoch 58/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2890 - val_loss: 0.3140\n",
      "Epoch 59/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2878 - val_loss: 0.3398\n",
      "Epoch 60/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2925 - val_loss: 0.3034\n",
      "Epoch 61/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2845 - val_loss: 0.3039\n",
      "Epoch 62/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2809 - val_loss: 0.3044\n",
      "Epoch 63/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2875 - val_loss: 0.3164\n",
      "Epoch 64/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2801 - val_loss: 0.3081\n",
      "Epoch 65/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2819 - val_loss: 0.3007\n",
      "Epoch 66/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.3026 - val_loss: 0.3156\n",
      "Epoch 67/600\n",
      "14448/14448 [==============================] - 1s 66us/step - loss: 0.3052 - val_loss: 0.3063\n",
      "Epoch 68/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2889 - val_loss: 0.3092\n",
      "Epoch 69/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2814 - val_loss: 0.3059\n",
      "Epoch 70/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2795 - val_loss: 0.3027\n",
      "Epoch 71/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2825 - val_loss: 0.3033\n",
      "Epoch 72/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2803 - val_loss: 0.3004\n",
      "Epoch 73/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2903 - val_loss: 0.3122\n",
      "Epoch 74/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2913 - val_loss: 0.3040\n",
      "Epoch 75/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2812 - val_loss: 0.3019\n",
      "Epoch 76/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2752 - val_loss: 0.3015\n",
      "Epoch 77/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2769 - val_loss: 0.3030\n",
      "Epoch 78/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2749 - val_loss: 0.3129\n",
      "Epoch 79/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2751 - val_loss: 0.2981\n",
      "Epoch 80/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2759 - val_loss: 0.2985\n",
      "Epoch 81/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2730 - val_loss: 0.3026\n",
      "Epoch 82/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2792 - val_loss: 0.2976\n",
      "Epoch 83/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2832 - val_loss: 0.3009\n",
      "Epoch 84/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2825 - val_loss: 0.3010\n",
      "Epoch 85/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2735 - val_loss: 0.2968\n",
      "Epoch 86/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2757 - val_loss: 0.2998\n",
      "Epoch 87/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2743 - val_loss: 0.3088\n",
      "Epoch 88/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2748 - val_loss: 0.2978\n",
      "Epoch 89/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2717 - val_loss: 0.3066\n",
      "Epoch 90/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2769 - val_loss: 0.3038\n",
      "Epoch 91/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2892 - val_loss: 0.3020\n",
      "Epoch 92/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2845 - val_loss: 0.3097\n",
      "Epoch 93/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2789 - val_loss: 0.2969\n",
      "Epoch 94/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2727 - val_loss: 0.3080\n",
      "Epoch 95/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2692 - val_loss: 0.2990\n",
      "Epoch 96/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2705 - val_loss: 0.3018\n",
      "Epoch 97/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2696 - val_loss: 0.2994\n",
      "Epoch 98/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2721 - val_loss: 0.2994\n",
      "Epoch 99/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2755 - val_loss: 0.2981\n",
      "Epoch 100/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2728 - val_loss: 0.2991\n",
      "Epoch 101/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2703 - val_loss: 0.3041\n",
      "Epoch 102/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2796 - val_loss: 0.2994\n",
      "Epoch 103/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2773 - val_loss: 0.2983\n",
      "Epoch 104/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2674 - val_loss: 0.2997\n",
      "Epoch 105/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2671 - val_loss: 0.3013\n",
      "Epoch 106/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2689 - val_loss: 0.2999\n",
      "Epoch 107/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2680 - val_loss: 0.3521\n",
      "Epoch 108/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2802 - val_loss: 0.3070\n",
      "Epoch 109/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2760 - val_loss: 0.3154\n",
      "Epoch 110/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2823 - val_loss: 0.2935\n",
      "Epoch 111/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2685 - val_loss: 0.3027\n",
      "Epoch 112/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2676 - val_loss: 0.2983\n",
      "Epoch 113/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2673 - val_loss: 0.2944\n",
      "Epoch 114/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2671 - val_loss: 0.2971\n",
      "Epoch 115/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2695 - val_loss: 0.3017\n",
      "Epoch 116/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2650 - val_loss: 0.3020\n",
      "Epoch 117/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2675 - val_loss: 0.3010\n",
      "Epoch 118/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2661 - val_loss: 0.2934\n",
      "Epoch 119/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2655 - val_loss: 0.2926\n",
      "Epoch 120/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2641 - val_loss: 0.2913\n",
      "Epoch 121/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2670 - val_loss: 0.2933\n",
      "Epoch 122/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2713 - val_loss: 0.2999\n",
      "Epoch 123/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2663 - val_loss: 0.2931\n",
      "Epoch 124/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2694 - val_loss: 0.2956\n",
      "Epoch 125/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2676 - val_loss: 0.3134\n",
      "Epoch 126/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2640 - val_loss: 0.2983\n",
      "Epoch 127/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2653 - val_loss: 0.2990\n",
      "Epoch 128/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2646 - val_loss: 0.3054\n",
      "Epoch 129/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2797 - val_loss: 0.2900\n",
      "Epoch 130/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2626 - val_loss: 0.2951\n",
      "Epoch 131/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2634 - val_loss: 0.2914\n",
      "Epoch 132/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2611 - val_loss: 0.2964\n",
      "Epoch 133/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2626 - val_loss: 0.3094\n",
      "Epoch 134/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2596 - val_loss: 0.2907\n",
      "Epoch 135/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2622 - val_loss: 0.2973\n",
      "Epoch 136/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2763 - val_loss: 0.2943\n",
      "Epoch 137/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2615 - val_loss: 0.2944\n",
      "Epoch 138/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2618 - val_loss: 0.2906\n",
      "Epoch 139/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2674 - val_loss: 0.2907\n",
      "Epoch 140/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2579 - val_loss: 0.2925\n",
      "Epoch 141/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2587 - val_loss: 0.2965\n",
      "Epoch 142/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2631 - val_loss: 0.3004\n",
      "Epoch 143/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2599 - val_loss: 0.2989\n",
      "Epoch 144/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2629 - val_loss: 0.2926\n",
      "Epoch 145/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2636 - val_loss: 0.2941\n",
      "Epoch 146/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2623 - val_loss: 0.2886\n",
      "Epoch 147/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2707 - val_loss: 0.2946\n",
      "Epoch 148/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2604 - val_loss: 0.3051\n",
      "Epoch 149/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2647 - val_loss: 0.2883\n",
      "Epoch 150/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2577 - val_loss: 0.2932\n",
      "Epoch 151/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2590 - val_loss: 0.2924\n",
      "Epoch 152/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2576 - val_loss: 0.2903\n",
      "Epoch 153/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2625 - val_loss: 0.2924\n",
      "Epoch 154/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2611 - val_loss: 0.2937\n",
      "Epoch 155/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2586 - val_loss: 0.2937\n",
      "Epoch 156/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2607 - val_loss: 0.2979\n",
      "Epoch 157/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2586 - val_loss: 0.2903\n",
      "Epoch 158/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2598 - val_loss: 0.2947\n",
      "Epoch 159/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2595 - val_loss: 0.2911\n",
      "Epoch 160/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2580 - val_loss: 0.2932\n",
      "Epoch 161/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2599 - val_loss: 0.2892\n",
      "Epoch 162/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2574 - val_loss: 0.2916\n",
      "Epoch 163/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2581 - val_loss: 0.2847\n",
      "Epoch 164/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2581 - val_loss: 0.2868\n",
      "Epoch 165/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2582 - val_loss: 0.2839\n",
      "Epoch 166/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2587 - val_loss: 0.2925\n",
      "Epoch 167/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2565 - val_loss: 0.2903\n",
      "Epoch 168/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2558 - val_loss: 0.2938\n",
      "Epoch 169/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2609 - val_loss: 0.2900\n",
      "Epoch 170/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2572 - val_loss: 0.2932\n",
      "Epoch 171/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2578 - val_loss: 0.2846\n",
      "Epoch 172/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2565 - val_loss: 0.2926\n",
      "Epoch 173/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2544 - val_loss: 0.2917\n",
      "Epoch 174/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2532 - val_loss: 0.2888\n",
      "Epoch 175/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2610 - val_loss: 0.2929\n",
      "Epoch 176/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2595 - val_loss: 0.2891\n",
      "Epoch 177/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2569 - val_loss: 0.2962\n",
      "Epoch 178/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2554 - val_loss: 0.2903\n",
      "Epoch 179/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2550 - val_loss: 0.2931\n",
      "Epoch 180/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2583 - val_loss: 0.2898\n",
      "Epoch 181/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2588 - val_loss: 0.2895\n",
      "Epoch 182/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2533 - val_loss: 0.2920\n",
      "Epoch 183/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2590 - val_loss: 0.2884\n",
      "Epoch 184/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2541 - val_loss: 0.2884\n",
      "Epoch 185/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2542 - val_loss: 0.2907\n",
      "Epoch 186/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2585 - val_loss: 0.2871\n",
      "Epoch 187/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2541 - val_loss: 0.2865\n",
      "Epoch 188/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2552 - val_loss: 0.2916\n",
      "Epoch 189/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2575 - val_loss: 0.2938\n",
      "Epoch 190/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2565 - val_loss: 0.2944\n",
      "Epoch 191/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2722 - val_loss: 0.2952\n",
      "Epoch 192/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2659 - val_loss: 0.2871\n",
      "Epoch 193/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2527 - val_loss: 0.2932\n",
      "Epoch 194/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.2531 - val_loss: 0.2861\n",
      "Epoch 195/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2564 - val_loss: 0.2863\n",
      "Epoch 196/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2536 - val_loss: 0.2988\n",
      "Epoch 197/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2540 - val_loss: 0.2860\n",
      "Epoch 198/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2521 - val_loss: 0.2948\n",
      "Epoch 199/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2537 - val_loss: 0.2854\n",
      "Epoch 200/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2531 - val_loss: 0.2849\n",
      "Epoch 201/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2634 - val_loss: 0.2890\n",
      "Epoch 202/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2537 - val_loss: 0.2897\n",
      "Epoch 203/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2526 - val_loss: 0.2873\n",
      "Epoch 204/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2536 - val_loss: 0.2883\n",
      "Epoch 205/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2546 - val_loss: 0.2883\n",
      "Epoch 206/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2534 - val_loss: 0.2904\n",
      "Epoch 207/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2524 - val_loss: 0.2855\n",
      "Epoch 208/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2514 - val_loss: 0.2908\n",
      "Epoch 209/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2557 - val_loss: 0.2938\n",
      "Epoch 210/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2559 - val_loss: 0.2901\n",
      "Epoch 211/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2537 - val_loss: 0.2909\n",
      "Epoch 212/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2559 - val_loss: 0.2924\n",
      "Epoch 213/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2564 - val_loss: 0.2878\n",
      "Epoch 214/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2512 - val_loss: 0.2868\n",
      "Epoch 215/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2529 - val_loss: 0.2932\n",
      "Epoch 216/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2557 - val_loss: 0.2965\n",
      "Epoch 217/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2636 - val_loss: 0.2835\n",
      "Epoch 218/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2549 - val_loss: 0.2910\n",
      "Epoch 219/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2515 - val_loss: 0.2963\n",
      "Epoch 220/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2522 - val_loss: 0.2883\n",
      "Epoch 221/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2509 - val_loss: 0.2912\n",
      "Epoch 222/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2518 - val_loss: 0.2875\n",
      "Epoch 223/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2500 - val_loss: 0.2857\n",
      "Epoch 224/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2515 - val_loss: 0.2883\n",
      "Epoch 225/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2511 - val_loss: 0.2930\n",
      "Epoch 226/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2510 - val_loss: 0.2903\n",
      "Epoch 227/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2564 - val_loss: 0.2934\n",
      "Epoch 228/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2540 - val_loss: 0.2940\n",
      "Epoch 229/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2527 - val_loss: 0.2899\n",
      "Epoch 230/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2517 - val_loss: 0.2906\n",
      "Epoch 231/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2520 - val_loss: 0.2915\n",
      "Epoch 232/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2525 - val_loss: 0.2856\n",
      "Epoch 233/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2529 - val_loss: 0.2884\n",
      "Epoch 234/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2539 - val_loss: 0.2972\n",
      "Epoch 235/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2692 - val_loss: 0.2826\n",
      "Epoch 236/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2491 - val_loss: 0.2823\n",
      "Epoch 237/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2488 - val_loss: 0.2858\n",
      "Epoch 238/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2519 - val_loss: 0.2853\n",
      "Epoch 239/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2507 - val_loss: 0.2870\n",
      "Epoch 240/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2513 - val_loss: 0.2975\n",
      "Epoch 241/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2511 - val_loss: 0.2879\n",
      "Epoch 242/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2529 - val_loss: 0.2871\n",
      "Epoch 243/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2547 - val_loss: 0.2891\n",
      "Epoch 244/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2543 - val_loss: 0.2871\n",
      "Epoch 245/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2484 - val_loss: 0.2845\n",
      "Epoch 246/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2478 - val_loss: 0.2853\n",
      "Epoch 247/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2550 - val_loss: 0.2847\n",
      "Epoch 248/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2538 - val_loss: 0.2876\n",
      "Epoch 249/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2481 - val_loss: 0.2845\n",
      "Epoch 250/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2496 - val_loss: 0.2975\n",
      "Epoch 251/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2518 - val_loss: 0.2872\n",
      "Epoch 252/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2548 - val_loss: 0.2846\n",
      "Epoch 253/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2511 - val_loss: 0.2879\n",
      "Epoch 254/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2486 - val_loss: 0.2860\n",
      "Epoch 255/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2589 - val_loss: 0.2909\n",
      "Epoch 256/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2469 - val_loss: 0.2942\n",
      "Epoch 257/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2509 - val_loss: 0.2866\n",
      "Epoch 258/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2517 - val_loss: 0.2832\n",
      "Epoch 259/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2537 - val_loss: 0.2820\n",
      "Epoch 260/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2466 - val_loss: 0.2950\n",
      "Epoch 261/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2531 - val_loss: 0.2938\n",
      "Epoch 262/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2573 - val_loss: 0.2871\n",
      "Epoch 263/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2524 - val_loss: 0.2931\n",
      "Epoch 264/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2486 - val_loss: 0.2820\n",
      "Epoch 265/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2476 - val_loss: 0.2823\n",
      "Epoch 266/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2507 - val_loss: 0.2822\n",
      "Epoch 267/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2474 - val_loss: 0.2837\n",
      "Epoch 268/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2526 - val_loss: 0.2839\n",
      "Epoch 269/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2499 - val_loss: 0.2805\n",
      "Epoch 270/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2528 - val_loss: 0.2857\n",
      "Epoch 271/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2535 - val_loss: 0.2844\n",
      "Epoch 272/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2468 - val_loss: 0.2811\n",
      "Epoch 273/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2470 - val_loss: 0.2843\n",
      "Epoch 274/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2497 - val_loss: 0.2797\n",
      "Epoch 275/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2552 - val_loss: 0.2914\n",
      "Epoch 276/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2506 - val_loss: 0.2821\n",
      "Epoch 277/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2459 - val_loss: 0.2850\n",
      "Epoch 278/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2505 - val_loss: 0.2822\n",
      "Epoch 279/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2524 - val_loss: 0.2841\n",
      "Epoch 280/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2475 - val_loss: 0.2954\n",
      "Epoch 281/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2612 - val_loss: 0.2857\n",
      "Epoch 282/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2537 - val_loss: 0.2852\n",
      "Epoch 283/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2491 - val_loss: 0.2842\n",
      "Epoch 284/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2472 - val_loss: 0.2847\n",
      "Epoch 285/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2480 - val_loss: 0.2807\n",
      "Epoch 286/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2493 - val_loss: 0.2860\n",
      "Epoch 287/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2472 - val_loss: 0.2939\n",
      "Epoch 288/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2471 - val_loss: 0.2793\n",
      "Epoch 289/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2425 - val_loss: 0.2828\n",
      "Epoch 290/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2440 - val_loss: 0.2899\n",
      "Epoch 291/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2470 - val_loss: 0.2812\n",
      "Epoch 292/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2484 - val_loss: 0.2894\n",
      "Epoch 293/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2552 - val_loss: 0.2775\n",
      "Epoch 294/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2475 - val_loss: 0.2816\n",
      "Epoch 295/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2440 - val_loss: 0.2840\n",
      "Epoch 296/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2473 - val_loss: 0.2788\n",
      "Epoch 297/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2466 - val_loss: 0.2813\n",
      "Epoch 298/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2446 - val_loss: 0.2877\n",
      "Epoch 299/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2470 - val_loss: 0.2861\n",
      "Epoch 300/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2450 - val_loss: 0.2898\n",
      "Epoch 301/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2455 - val_loss: 0.2833\n",
      "Epoch 302/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2487 - val_loss: 0.2779\n",
      "Epoch 303/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2438 - val_loss: 0.2853\n",
      "Epoch 304/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2465 - val_loss: 0.2883\n",
      "Epoch 305/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2486 - val_loss: 0.2858\n",
      "Epoch 306/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2501 - val_loss: 0.2802\n",
      "Epoch 307/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2462 - val_loss: 0.2941\n",
      "Epoch 308/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2550 - val_loss: 0.2863\n",
      "Epoch 309/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2506 - val_loss: 0.2808\n",
      "Epoch 310/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2466 - val_loss: 0.2821\n",
      "Epoch 311/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2512 - val_loss: 0.2842\n",
      "Epoch 312/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2515 - val_loss: 0.2815\n",
      "Epoch 313/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2507 - val_loss: 0.2808\n",
      "Epoch 314/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2514 - val_loss: 0.2932\n",
      "Epoch 315/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2443 - val_loss: 0.2840\n",
      "Epoch 316/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2457 - val_loss: 0.2790\n",
      "Epoch 317/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2445 - val_loss: 0.2897\n",
      "Epoch 318/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2516 - val_loss: 0.2843\n",
      "Epoch 319/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2504 - val_loss: 0.2844\n",
      "Epoch 320/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2466 - val_loss: 0.2870\n",
      "Epoch 321/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2472 - val_loss: 0.2887\n",
      "Epoch 322/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2550 - val_loss: 0.2824\n",
      "Epoch 323/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2470 - val_loss: 0.2825\n",
      "Epoch 324/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2426 - val_loss: 0.2831\n",
      "Epoch 325/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2431 - val_loss: 0.2817\n",
      "Epoch 326/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2429 - val_loss: 0.2923\n",
      "Epoch 327/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2413 - val_loss: 0.2901\n",
      "Epoch 328/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2436 - val_loss: 0.2778\n",
      "Epoch 329/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2417 - val_loss: 0.2839\n",
      "Epoch 330/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2431 - val_loss: 0.2789\n",
      "Epoch 331/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2450 - val_loss: 0.2851\n",
      "Epoch 332/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2450 - val_loss: 0.2836\n",
      "Epoch 333/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2453 - val_loss: 0.2797\n",
      "Epoch 334/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2446 - val_loss: 0.2839\n",
      "Epoch 335/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2517 - val_loss: 0.2839\n",
      "Epoch 336/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2464 - val_loss: 0.2839\n",
      "Epoch 337/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2452 - val_loss: 0.2819\n",
      "Epoch 338/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2435 - val_loss: 0.2877\n",
      "Epoch 339/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2422 - val_loss: 0.2848\n",
      "Epoch 340/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2449 - val_loss: 0.3017\n",
      "Epoch 341/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2560 - val_loss: 0.2870\n",
      "Epoch 342/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2576 - val_loss: 0.2777\n",
      "Epoch 343/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2461 - val_loss: 0.2808\n",
      "Epoch 344/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2433 - val_loss: 0.2762\n",
      "Epoch 345/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2446 - val_loss: 0.2787\n",
      "Epoch 346/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2434 - val_loss: 0.2822\n",
      "Epoch 347/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2432 - val_loss: 0.2836\n",
      "Epoch 348/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2431 - val_loss: 0.2799\n",
      "Epoch 349/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2502 - val_loss: 0.2799\n",
      "Epoch 350/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2489 - val_loss: 0.2844\n",
      "Epoch 351/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2414 - val_loss: 0.2802\n",
      "Epoch 352/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2407 - val_loss: 0.2815\n",
      "Epoch 353/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2430 - val_loss: 0.2821\n",
      "Epoch 354/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2468 - val_loss: 0.2834\n",
      "Epoch 355/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2445 - val_loss: 0.2836\n",
      "Epoch 356/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2426 - val_loss: 0.2833\n",
      "Epoch 357/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2418 - val_loss: 0.2822\n",
      "Epoch 358/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2412 - val_loss: 0.2822\n",
      "Epoch 359/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2410 - val_loss: 0.2946\n",
      "Epoch 360/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2428 - val_loss: 0.2805\n",
      "Epoch 361/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2444 - val_loss: 0.2836\n",
      "Epoch 362/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2434 - val_loss: 0.2803\n",
      "Epoch 363/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2447 - val_loss: 0.2853\n",
      "Epoch 364/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2428 - val_loss: 0.2762\n",
      "Epoch 365/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2440 - val_loss: 0.2790\n",
      "Epoch 366/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2413 - val_loss: 0.2803\n",
      "Epoch 367/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2414 - val_loss: 0.2812\n",
      "Epoch 368/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2450 - val_loss: 0.2807\n",
      "Epoch 369/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2440 - val_loss: 0.2908\n",
      "Epoch 370/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2436 - val_loss: 0.2764\n",
      "Epoch 371/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2442 - val_loss: 0.2826\n",
      "Epoch 372/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2425 - val_loss: 0.2761\n",
      "Epoch 373/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2440 - val_loss: 0.2830\n",
      "Epoch 374/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2442 - val_loss: 0.2801\n",
      "Epoch 375/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2429 - val_loss: 0.2810\n",
      "Epoch 376/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2413 - val_loss: 0.2799\n",
      "Epoch 377/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2427 - val_loss: 0.2824\n",
      "Epoch 378/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2432 - val_loss: 0.2801\n",
      "Epoch 379/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2446 - val_loss: 0.2864\n",
      "Epoch 380/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2446 - val_loss: 0.2773\n",
      "Epoch 381/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2446 - val_loss: 0.2888\n",
      "Epoch 382/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2424 - val_loss: 0.2877\n",
      "Epoch 383/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2412 - val_loss: 0.2847\n",
      "Epoch 384/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2446 - val_loss: 0.2838\n",
      "Epoch 385/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2472 - val_loss: 0.2938\n",
      "Epoch 386/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2457 - val_loss: 0.2797\n",
      "Epoch 387/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2468 - val_loss: 0.2829\n",
      "Epoch 388/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2505 - val_loss: 0.2818\n",
      "Epoch 389/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2426 - val_loss: 0.2842\n",
      "Epoch 390/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2448 - val_loss: 0.2787\n",
      "Epoch 391/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2402 - val_loss: 0.2802\n",
      "Epoch 392/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2429 - val_loss: 0.2802\n",
      "Epoch 393/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2505 - val_loss: 0.2788\n",
      "Epoch 394/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2393 - val_loss: 0.2853\n",
      "Epoch 395/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2402 - val_loss: 0.2851\n",
      "Epoch 396/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2389 - val_loss: 0.2869\n",
      "Epoch 397/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2395 - val_loss: 0.2828\n",
      "Epoch 398/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2411 - val_loss: 0.2873\n",
      "Epoch 399/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2656 - val_loss: 0.2770\n",
      "Epoch 400/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2411 - val_loss: 0.2845\n",
      "Epoch 401/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2407 - val_loss: 0.2804\n",
      "Epoch 402/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2416 - val_loss: 0.2782\n",
      "Epoch 403/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2383 - val_loss: 0.2911\n",
      "Epoch 404/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2384 - val_loss: 0.2799\n",
      "Epoch 405/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2390 - val_loss: 0.2876\n",
      "Epoch 406/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2425 - val_loss: 0.2873\n",
      "Epoch 407/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2520 - val_loss: 0.2787\n",
      "Epoch 408/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2447 - val_loss: 0.2788\n",
      "Epoch 409/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2402 - val_loss: 0.2785\n",
      "Epoch 410/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2381 - val_loss: 0.2862\n",
      "Epoch 411/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2378 - val_loss: 0.2818\n",
      "Epoch 412/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2386 - val_loss: 0.2789\n",
      "Epoch 413/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2412 - val_loss: 0.2772\n",
      "Epoch 414/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2400 - val_loss: 0.2779\n",
      "Epoch 415/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2402 - val_loss: 0.2775\n",
      "Epoch 416/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2399 - val_loss: 0.2827\n",
      "Epoch 417/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2473 - val_loss: 0.2774\n",
      "Epoch 418/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2380 - val_loss: 0.2788\n",
      "Epoch 419/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2403 - val_loss: 0.2871\n",
      "Epoch 420/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2413 - val_loss: 0.2776\n",
      "Epoch 421/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2379 - val_loss: 0.2791\n",
      "Epoch 422/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2382 - val_loss: 0.2763\n",
      "Epoch 423/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2373 - val_loss: 0.2799\n",
      "Epoch 424/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2376 - val_loss: 0.2780\n",
      "Epoch 425/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2394 - val_loss: 0.2809\n",
      "Epoch 426/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2398 - val_loss: 0.2826\n",
      "Epoch 427/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2496 - val_loss: 0.2778\n",
      "Epoch 428/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2377 - val_loss: 0.2788\n",
      "Epoch 429/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2401 - val_loss: 0.2835\n",
      "Epoch 430/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2376 - val_loss: 0.2858\n",
      "Epoch 431/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2372 - val_loss: 0.2935\n",
      "Epoch 432/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2482 - val_loss: 0.2855\n",
      "Epoch 433/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2414 - val_loss: 0.2756\n",
      "Epoch 434/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2382 - val_loss: 0.2780\n",
      "Epoch 435/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2373 - val_loss: 0.2794\n",
      "Epoch 436/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2371 - val_loss: 0.2848\n",
      "Epoch 437/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2359 - val_loss: 0.2789\n",
      "Epoch 438/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2367 - val_loss: 0.2804\n",
      "Epoch 439/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2376 - val_loss: 0.2804\n",
      "Epoch 440/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2417 - val_loss: 0.2820\n",
      "Epoch 441/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2371 - val_loss: 0.2827\n",
      "Epoch 442/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2363 - val_loss: 0.2883\n",
      "Epoch 443/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2365 - val_loss: 0.2834\n",
      "Epoch 444/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2369 - val_loss: 0.2820\n",
      "Epoch 445/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2375 - val_loss: 0.2802\n",
      "Epoch 446/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2386 - val_loss: 0.2869\n",
      "Epoch 447/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2385 - val_loss: 0.2781\n",
      "Epoch 448/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2377 - val_loss: 0.2800\n",
      "Epoch 449/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2351 - val_loss: 0.2808\n",
      "Epoch 450/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2353 - val_loss: 0.2795\n",
      "Epoch 451/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2382 - val_loss: 0.3026\n",
      "Epoch 452/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2548 - val_loss: 0.2848\n",
      "Epoch 453/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2421 - val_loss: 0.2932\n",
      "Epoch 454/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2376 - val_loss: 0.2816\n",
      "Epoch 455/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2370 - val_loss: 0.2944\n",
      "Epoch 456/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2372 - val_loss: 0.2865\n",
      "Epoch 457/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2365 - val_loss: 0.2933\n",
      "Epoch 458/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2377 - val_loss: 0.2785\n",
      "Epoch 459/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2375 - val_loss: 0.2805\n",
      "Epoch 460/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2374 - val_loss: 0.2809\n",
      "Epoch 461/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2373 - val_loss: 0.2787\n",
      "Epoch 462/600\n",
      "14448/14448 [==============================] - 1s 63us/step - loss: 0.2355 - val_loss: 0.2826\n",
      "Epoch 463/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2379 - val_loss: 0.2817\n",
      "Epoch 464/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2351 - val_loss: 0.2818\n",
      "Epoch 465/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2364 - val_loss: 0.2845\n",
      "Epoch 466/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2348 - val_loss: 0.2875\n",
      "Epoch 467/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2361 - val_loss: 0.2812\n",
      "Epoch 468/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2357 - val_loss: 0.2869\n",
      "Epoch 469/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2360 - val_loss: 0.2795\n",
      "Epoch 470/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2381 - val_loss: 0.2830\n",
      "Epoch 471/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2360 - val_loss: 0.2777\n",
      "Epoch 472/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2368 - val_loss: 0.2772\n",
      "Epoch 473/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2365 - val_loss: 0.2777\n",
      "Epoch 474/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2348 - val_loss: 0.2779\n",
      "Epoch 475/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2366 - val_loss: 0.2820\n",
      "Epoch 476/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2378 - val_loss: 0.2813\n",
      "Epoch 477/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2368 - val_loss: 0.2789\n",
      "Epoch 478/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2371 - val_loss: 0.2889\n",
      "Epoch 479/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2396 - val_loss: 0.2840\n",
      "Epoch 480/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2361 - val_loss: 0.2997\n",
      "Epoch 481/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2652 - val_loss: 0.2822\n",
      "Epoch 482/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2361 - val_loss: 0.2807\n",
      "Epoch 483/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2382 - val_loss: 0.2822\n",
      "Epoch 484/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2344 - val_loss: 0.2864\n",
      "Epoch 485/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2346 - val_loss: 0.2819\n",
      "Epoch 486/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2356 - val_loss: 0.2887\n",
      "Epoch 487/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2422 - val_loss: 0.2747\n",
      "Epoch 488/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2379 - val_loss: 0.2837\n",
      "Epoch 489/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2417 - val_loss: 0.2784\n",
      "Epoch 490/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2392 - val_loss: 0.2830\n",
      "Epoch 491/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2366 - val_loss: 0.2800\n",
      "Epoch 492/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2378 - val_loss: 0.2809\n",
      "Epoch 493/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2343 - val_loss: 0.2840\n",
      "Epoch 494/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2359 - val_loss: 0.2806\n",
      "Epoch 495/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2348 - val_loss: 0.2782\n",
      "Epoch 496/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2349 - val_loss: 0.2809\n",
      "Epoch 497/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2355 - val_loss: 0.2796\n",
      "Epoch 498/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2348 - val_loss: 0.2787\n",
      "Epoch 499/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2408 - val_loss: 0.2764\n",
      "Epoch 500/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2377 - val_loss: 0.2866\n",
      "Epoch 501/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2367 - val_loss: 0.2794\n",
      "Epoch 502/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2351 - val_loss: 0.2812\n",
      "Epoch 503/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2346 - val_loss: 0.2837\n",
      "Epoch 504/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2352 - val_loss: 0.2804\n",
      "Epoch 505/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2344 - val_loss: 0.2790\n",
      "Epoch 506/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2343 - val_loss: 0.2914\n",
      "Epoch 507/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2453 - val_loss: 0.2797\n",
      "Epoch 508/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2416 - val_loss: 0.2773\n",
      "Epoch 509/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2384 - val_loss: 0.2882\n",
      "Epoch 510/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2345 - val_loss: 0.2801\n",
      "Epoch 511/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2366 - val_loss: 0.2824\n",
      "Epoch 512/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2341 - val_loss: 0.2831\n",
      "Epoch 513/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2373 - val_loss: 0.2828\n",
      "Epoch 514/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2359 - val_loss: 0.2805\n",
      "Epoch 515/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2353 - val_loss: 0.2808\n",
      "Epoch 516/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2381 - val_loss: 0.2858\n",
      "Epoch 517/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2404 - val_loss: 0.2795\n",
      "Epoch 518/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2384 - val_loss: 0.2805\n",
      "Epoch 519/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2347 - val_loss: 0.2904\n",
      "Epoch 520/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2389 - val_loss: 0.2861\n",
      "Epoch 521/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2365 - val_loss: 0.2819\n",
      "Epoch 522/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2383 - val_loss: 0.2785\n",
      "Epoch 523/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2333 - val_loss: 0.2786\n",
      "Epoch 524/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2350 - val_loss: 0.2797\n",
      "Epoch 525/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2327 - val_loss: 0.2766\n",
      "Epoch 526/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2344 - val_loss: 0.2768\n",
      "Epoch 527/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2359 - val_loss: 0.2792\n",
      "Epoch 528/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2348 - val_loss: 0.2792\n",
      "Epoch 529/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2342 - val_loss: 0.2862\n",
      "Epoch 530/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2346 - val_loss: 0.2768\n",
      "Epoch 531/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2345 - val_loss: 0.2794\n",
      "Epoch 532/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2351 - val_loss: 0.2824\n",
      "Epoch 533/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2371 - val_loss: 0.2859\n",
      "Epoch 534/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2353 - val_loss: 0.2755\n",
      "Epoch 535/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2339 - val_loss: 0.2770\n",
      "Epoch 536/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2388 - val_loss: 0.2846\n",
      "Epoch 537/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2354 - val_loss: 0.2936\n",
      "Epoch 538/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2346 - val_loss: 0.2817\n",
      "Epoch 539/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2347 - val_loss: 0.2850\n",
      "Epoch 540/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2368 - val_loss: 0.2781\n",
      "Epoch 541/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2377 - val_loss: 0.2778\n",
      "Epoch 542/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2343 - val_loss: 0.2839\n",
      "Epoch 543/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2336 - val_loss: 0.2771\n",
      "Epoch 544/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2350 - val_loss: 0.2880\n",
      "Epoch 545/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2342 - val_loss: 0.2817\n",
      "Epoch 546/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2348 - val_loss: 0.2762\n",
      "Epoch 547/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2343 - val_loss: 0.2778\n",
      "Epoch 548/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2323 - val_loss: 0.2843\n",
      "Epoch 549/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2346 - val_loss: 0.2763\n",
      "Epoch 550/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2339 - val_loss: 0.2784\n",
      "Epoch 551/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2350 - val_loss: 0.2790\n",
      "Epoch 552/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2351 - val_loss: 0.2789\n",
      "Epoch 553/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2345 - val_loss: 0.2969\n",
      "Epoch 554/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2352 - val_loss: 0.2777\n",
      "Epoch 555/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2332 - val_loss: 0.2835\n",
      "Epoch 556/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2364 - val_loss: 0.2799\n",
      "Epoch 557/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2350 - val_loss: 0.2769\n",
      "Epoch 558/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2347 - val_loss: 0.2855\n",
      "Epoch 559/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2362 - val_loss: 0.2839\n",
      "Epoch 560/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2347 - val_loss: 0.2844\n",
      "Epoch 561/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2355 - val_loss: 0.2773\n",
      "Epoch 562/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2365 - val_loss: 0.2778\n",
      "Epoch 563/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2344 - val_loss: 0.2844\n",
      "Epoch 564/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2327 - val_loss: 0.2956\n",
      "Epoch 565/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2484 - val_loss: 0.2787\n",
      "Epoch 566/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2380 - val_loss: 0.2900\n",
      "Epoch 567/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2350 - val_loss: 0.2803\n",
      "Epoch 568/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2343 - val_loss: 0.2855\n",
      "Epoch 569/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2343 - val_loss: 0.2790\n",
      "Epoch 570/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2332 - val_loss: 0.2856\n",
      "Epoch 571/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2348 - val_loss: 0.2815\n",
      "Epoch 572/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2371 - val_loss: 0.2820\n",
      "Epoch 573/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2348 - val_loss: 0.2806\n",
      "Epoch 574/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2343 - val_loss: 0.2892\n",
      "Epoch 575/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2338 - val_loss: 0.2887\n",
      "Epoch 576/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2355 - val_loss: 0.2867\n",
      "Epoch 577/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2335 - val_loss: 0.2825\n",
      "Epoch 578/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2334 - val_loss: 0.2898\n",
      "Epoch 579/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.2331 - val_loss: 0.2831\n",
      "Epoch 580/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2317 - val_loss: 0.2880\n",
      "Epoch 581/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2336 - val_loss: 0.2789\n",
      "Epoch 582/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2364 - val_loss: 0.2797\n",
      "Epoch 583/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2345 - val_loss: 0.2778\n",
      "Epoch 584/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2323 - val_loss: 0.2818\n",
      "Epoch 585/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2328 - val_loss: 0.2840\n",
      "Epoch 586/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2353 - val_loss: 0.2781\n",
      "Epoch 587/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2333 - val_loss: 0.2806\n",
      "Epoch 588/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2362 - val_loss: 0.2767\n",
      "Epoch 589/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2325 - val_loss: 0.2789\n",
      "Epoch 590/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2345 - val_loss: 0.2788\n",
      "Epoch 591/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2343 - val_loss: 0.2794\n",
      "Epoch 592/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2323 - val_loss: 0.2804\n",
      "Epoch 593/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2337 - val_loss: 0.2814\n",
      "Epoch 594/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2334 - val_loss: 0.2798\n",
      "Epoch 595/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2342 - val_loss: 0.2864\n",
      "Epoch 596/600\n",
      "14448/14448 [==============================] - 1s 49us/step - loss: 0.2354 - val_loss: 0.2868\n",
      "Epoch 597/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2347 - val_loss: 0.2828\n",
      "Epoch 598/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2344 - val_loss: 0.2790\n",
      "Epoch 599/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2330 - val_loss: 0.2843\n",
      "Epoch 600/600\n",
      "14448/14448 [==============================] - 1s 50us/step - loss: 0.2343 - val_loss: 0.2795\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAELCAYAAADTK53JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxphEcYBnjDGJ4gDPGGMS9f8hO6PUwMsV0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(8,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600 ,validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 with sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684813,
     "status": "ok",
     "timestamp": 1572239194578,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "CEl-Qw-6GAKN",
    "outputId": "53fe3d33-72af-45dc-db97-a62d8e5ad2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14448 samples, validate on 6192 samples\n",
      "Epoch 1/600\n",
      "14448/14448 [==============================] - 1s 87us/step - loss: 0.7330 - val_loss: 0.5199\n",
      "Epoch 2/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.4993 - val_loss: 0.5114\n",
      "Epoch 3/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.4813 - val_loss: 0.4863\n",
      "Epoch 4/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.4749 - val_loss: 0.5178\n",
      "Epoch 5/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.4738 - val_loss: 0.4809\n",
      "Epoch 6/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.4625 - val_loss: 0.4833\n",
      "Epoch 7/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.4606 - val_loss: 0.4762\n",
      "Epoch 8/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.4570 - val_loss: 0.4635\n",
      "Epoch 9/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.4492 - val_loss: 0.4788\n",
      "Epoch 10/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.4499 - val_loss: 0.4529\n",
      "Epoch 11/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.4447 - val_loss: 0.4506\n",
      "Epoch 12/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.4396 - val_loss: 0.4491\n",
      "Epoch 13/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.4372 - val_loss: 0.5037\n",
      "Epoch 14/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.4388 - val_loss: 0.4410\n",
      "Epoch 15/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.4287 - val_loss: 0.4578\n",
      "Epoch 16/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.4247 - val_loss: 0.4421\n",
      "Epoch 17/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.4230 - val_loss: 0.4258\n",
      "Epoch 18/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.4189 - val_loss: 0.4305\n",
      "Epoch 19/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.4131 - val_loss: 0.4269\n",
      "Epoch 20/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.4070 - val_loss: 0.4143\n",
      "Epoch 21/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.4048 - val_loss: 0.4270\n",
      "Epoch 22/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.4022 - val_loss: 0.4311\n",
      "Epoch 23/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.4000 - val_loss: 0.4143\n",
      "Epoch 24/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3965 - val_loss: 0.4058\n",
      "Epoch 25/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3919 - val_loss: 0.4123\n",
      "Epoch 26/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.3894 - val_loss: 0.3988\n",
      "Epoch 27/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3887 - val_loss: 0.3980\n",
      "Epoch 28/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3894 - val_loss: 0.4111\n",
      "Epoch 29/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3849 - val_loss: 0.4097\n",
      "Epoch 30/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3856 - val_loss: 0.4195\n",
      "Epoch 31/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3857 - val_loss: 0.3930\n",
      "Epoch 32/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3805 - val_loss: 0.3988\n",
      "Epoch 33/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3781 - val_loss: 0.4034\n",
      "Epoch 34/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3807 - val_loss: 0.3860\n",
      "Epoch 35/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3766 - val_loss: 0.3867\n",
      "Epoch 36/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3744 - val_loss: 0.3860\n",
      "Epoch 37/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3739 - val_loss: 0.3880\n",
      "Epoch 38/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3737 - val_loss: 0.3849\n",
      "Epoch 39/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3710 - val_loss: 0.3863\n",
      "Epoch 40/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3697 - val_loss: 0.3800\n",
      "Epoch 41/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3648 - val_loss: 0.3809\n",
      "Epoch 42/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3652 - val_loss: 0.3759\n",
      "Epoch 43/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3643 - val_loss: 0.3754\n",
      "Epoch 44/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3624 - val_loss: 0.3706\n",
      "Epoch 45/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.3636 - val_loss: 0.3876\n",
      "Epoch 46/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.3615 - val_loss: 0.3761\n",
      "Epoch 47/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3632 - val_loss: 0.3788\n",
      "Epoch 48/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3584 - val_loss: 0.3785\n",
      "Epoch 49/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3607 - val_loss: 0.3676\n",
      "Epoch 50/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3591 - val_loss: 0.3664\n",
      "Epoch 51/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3548 - val_loss: 0.3687\n",
      "Epoch 52/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3551 - val_loss: 0.3738\n",
      "Epoch 53/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3580 - val_loss: 0.3682\n",
      "Epoch 54/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3546 - val_loss: 0.3635\n",
      "Epoch 55/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3544 - val_loss: 0.3909\n",
      "Epoch 56/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3543 - val_loss: 0.3652\n",
      "Epoch 57/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3521 - val_loss: 0.3615\n",
      "Epoch 58/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3518 - val_loss: 0.3615\n",
      "Epoch 59/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3502 - val_loss: 0.3666\n",
      "Epoch 60/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3501 - val_loss: 0.3611\n",
      "Epoch 61/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3497 - val_loss: 0.3624\n",
      "Epoch 62/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3479 - val_loss: 0.3712\n",
      "Epoch 63/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3513 - val_loss: 0.3799\n",
      "Epoch 64/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3470 - val_loss: 0.3588\n",
      "Epoch 65/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3446 - val_loss: 0.3652\n",
      "Epoch 66/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3451 - val_loss: 0.3562\n",
      "Epoch 67/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3466 - val_loss: 0.3607\n",
      "Epoch 68/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3443 - val_loss: 0.3523\n",
      "Epoch 69/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3428 - val_loss: 0.3497\n",
      "Epoch 70/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.3426 - val_loss: 0.3596\n",
      "Epoch 71/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3398 - val_loss: 0.3502\n",
      "Epoch 72/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3405 - val_loss: 0.3508\n",
      "Epoch 73/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3392 - val_loss: 0.3594\n",
      "Epoch 74/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3410 - val_loss: 0.3757\n",
      "Epoch 75/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3402 - val_loss: 0.3618\n",
      "Epoch 76/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3378 - val_loss: 0.3478\n",
      "Epoch 77/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.3388 - val_loss: 0.3526\n",
      "Epoch 78/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3353 - val_loss: 0.3556\n",
      "Epoch 79/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3347 - val_loss: 0.3507\n",
      "Epoch 80/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3341 - val_loss: 0.3440\n",
      "Epoch 81/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3336 - val_loss: 0.3413\n",
      "Epoch 82/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3322 - val_loss: 0.3417\n",
      "Epoch 83/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3295 - val_loss: 0.3402\n",
      "Epoch 84/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3303 - val_loss: 0.3394\n",
      "Epoch 85/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3298 - val_loss: 0.3385\n",
      "Epoch 86/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3292 - val_loss: 0.3548\n",
      "Epoch 87/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3267 - val_loss: 0.3400\n",
      "Epoch 88/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3248 - val_loss: 0.3345\n",
      "Epoch 89/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3237 - val_loss: 0.3396\n",
      "Epoch 90/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3225 - val_loss: 0.3327\n",
      "Epoch 91/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3208 - val_loss: 0.3445\n",
      "Epoch 92/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3214 - val_loss: 0.3310\n",
      "Epoch 93/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.3187 - val_loss: 0.3357\n",
      "Epoch 94/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3207 - val_loss: 0.3370\n",
      "Epoch 95/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.3184 - val_loss: 0.3332\n",
      "Epoch 96/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3163 - val_loss: 0.3352\n",
      "Epoch 97/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3158 - val_loss: 0.3291\n",
      "Epoch 98/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3166 - val_loss: 0.3308\n",
      "Epoch 99/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3145 - val_loss: 0.3323\n",
      "Epoch 100/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3142 - val_loss: 0.3279\n",
      "Epoch 101/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3132 - val_loss: 0.3248\n",
      "Epoch 102/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3132 - val_loss: 0.3281\n",
      "Epoch 103/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3111 - val_loss: 0.3445\n",
      "Epoch 104/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3107 - val_loss: 0.3434\n",
      "Epoch 105/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3118 - val_loss: 0.3245\n",
      "Epoch 106/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3119 - val_loss: 0.3247\n",
      "Epoch 107/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.3100 - val_loss: 0.3347\n",
      "Epoch 108/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.3088 - val_loss: 0.3287\n",
      "Epoch 109/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3091 - val_loss: 0.3269\n",
      "Epoch 110/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3091 - val_loss: 0.3259\n",
      "Epoch 111/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3094 - val_loss: 0.3238\n",
      "Epoch 112/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3079 - val_loss: 0.3231\n",
      "Epoch 113/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3079 - val_loss: 0.3285\n",
      "Epoch 114/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3082 - val_loss: 0.3423\n",
      "Epoch 115/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3079 - val_loss: 0.3325\n",
      "Epoch 116/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3068 - val_loss: 0.3213\n",
      "Epoch 117/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3058 - val_loss: 0.3240\n",
      "Epoch 118/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3066 - val_loss: 0.3358\n",
      "Epoch 119/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3066 - val_loss: 0.3204\n",
      "Epoch 120/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.3056 - val_loss: 0.3261\n",
      "Epoch 121/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3062 - val_loss: 0.3191\n",
      "Epoch 122/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3033 - val_loss: 0.3392\n",
      "Epoch 123/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.3065 - val_loss: 0.3207\n",
      "Epoch 124/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3049 - val_loss: 0.3234\n",
      "Epoch 125/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3025 - val_loss: 0.3207\n",
      "Epoch 126/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.3056 - val_loss: 0.3216\n",
      "Epoch 127/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3024 - val_loss: 0.3276\n",
      "Epoch 128/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3047 - val_loss: 0.3181\n",
      "Epoch 129/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3024 - val_loss: 0.3201\n",
      "Epoch 130/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3010 - val_loss: 0.3196\n",
      "Epoch 131/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3028 - val_loss: 0.3187\n",
      "Epoch 132/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2995 - val_loss: 0.3210\n",
      "Epoch 133/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3026 - val_loss: 0.3259\n",
      "Epoch 134/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.3032 - val_loss: 0.3196\n",
      "Epoch 135/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.3017 - val_loss: 0.3311\n",
      "Epoch 136/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2996 - val_loss: 0.3243\n",
      "Epoch 137/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.3008 - val_loss: 0.3190\n",
      "Epoch 138/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.3000 - val_loss: 0.3164\n",
      "Epoch 139/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.3022 - val_loss: 0.3192\n",
      "Epoch 140/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2990 - val_loss: 0.3170\n",
      "Epoch 141/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2986 - val_loss: 0.3181\n",
      "Epoch 142/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2986 - val_loss: 0.3183\n",
      "Epoch 143/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2981 - val_loss: 0.3226\n",
      "Epoch 144/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.3004 - val_loss: 0.3185\n",
      "Epoch 145/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2987 - val_loss: 0.3215\n",
      "Epoch 146/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2989 - val_loss: 0.3161\n",
      "Epoch 147/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2980 - val_loss: 0.3157\n",
      "Epoch 148/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2981 - val_loss: 0.3283\n",
      "Epoch 149/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2962 - val_loss: 0.3196\n",
      "Epoch 150/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2969 - val_loss: 0.3254\n",
      "Epoch 151/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2983 - val_loss: 0.3177\n",
      "Epoch 152/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2981 - val_loss: 0.3163\n",
      "Epoch 153/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2953 - val_loss: 0.3180\n",
      "Epoch 154/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2946 - val_loss: 0.3167\n",
      "Epoch 155/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2948 - val_loss: 0.3247\n",
      "Epoch 156/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2972 - val_loss: 0.3203\n",
      "Epoch 157/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2954 - val_loss: 0.3199\n",
      "Epoch 158/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2967 - val_loss: 0.3180\n",
      "Epoch 159/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2938 - val_loss: 0.3278\n",
      "Epoch 160/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2962 - val_loss: 0.3190\n",
      "Epoch 161/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2942 - val_loss: 0.3156\n",
      "Epoch 162/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2957 - val_loss: 0.3187\n",
      "Epoch 163/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2946 - val_loss: 0.3159\n",
      "Epoch 164/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2946 - val_loss: 0.3150\n",
      "Epoch 165/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2940 - val_loss: 0.3183\n",
      "Epoch 166/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2936 - val_loss: 0.3351\n",
      "Epoch 167/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2924 - val_loss: 0.3200\n",
      "Epoch 168/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2931 - val_loss: 0.3158\n",
      "Epoch 169/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2927 - val_loss: 0.3180\n",
      "Epoch 170/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2917 - val_loss: 0.3134\n",
      "Epoch 171/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2935 - val_loss: 0.3172\n",
      "Epoch 172/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2919 - val_loss: 0.3139\n",
      "Epoch 173/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2931 - val_loss: 0.3213\n",
      "Epoch 174/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2915 - val_loss: 0.3170\n",
      "Epoch 175/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2925 - val_loss: 0.3154\n",
      "Epoch 176/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2908 - val_loss: 0.3119\n",
      "Epoch 177/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2908 - val_loss: 0.3162\n",
      "Epoch 178/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2906 - val_loss: 0.3176\n",
      "Epoch 179/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2900 - val_loss: 0.3136\n",
      "Epoch 180/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2912 - val_loss: 0.3180\n",
      "Epoch 181/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2914 - val_loss: 0.3206\n",
      "Epoch 182/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2920 - val_loss: 0.3130\n",
      "Epoch 183/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2898 - val_loss: 0.3185\n",
      "Epoch 184/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2934 - val_loss: 0.3142\n",
      "Epoch 185/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2912 - val_loss: 0.3208\n",
      "Epoch 186/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2905 - val_loss: 0.3144\n",
      "Epoch 187/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2917 - val_loss: 0.3147\n",
      "Epoch 188/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2907 - val_loss: 0.3177\n",
      "Epoch 189/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2885 - val_loss: 0.3165\n",
      "Epoch 190/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2896 - val_loss: 0.3344\n",
      "Epoch 191/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2912 - val_loss: 0.3130\n",
      "Epoch 192/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2899 - val_loss: 0.3128\n",
      "Epoch 193/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2903 - val_loss: 0.3154\n",
      "Epoch 194/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2880 - val_loss: 0.3278\n",
      "Epoch 195/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2898 - val_loss: 0.3191\n",
      "Epoch 196/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2884 - val_loss: 0.3104\n",
      "Epoch 197/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2895 - val_loss: 0.3129\n",
      "Epoch 198/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2884 - val_loss: 0.3118\n",
      "Epoch 199/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2886 - val_loss: 0.3110\n",
      "Epoch 200/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2888 - val_loss: 0.3139\n",
      "Epoch 201/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2881 - val_loss: 0.3108\n",
      "Epoch 202/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2881 - val_loss: 0.3132\n",
      "Epoch 203/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2881 - val_loss: 0.3142\n",
      "Epoch 204/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2870 - val_loss: 0.3272\n",
      "Epoch 205/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2878 - val_loss: 0.3101\n",
      "Epoch 206/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2871 - val_loss: 0.3275\n",
      "Epoch 207/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2882 - val_loss: 0.3207\n",
      "Epoch 208/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2861 - val_loss: 0.3142\n",
      "Epoch 209/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2881 - val_loss: 0.3141\n",
      "Epoch 210/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2880 - val_loss: 0.3127\n",
      "Epoch 211/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2864 - val_loss: 0.3119\n",
      "Epoch 212/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2875 - val_loss: 0.3159\n",
      "Epoch 213/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2873 - val_loss: 0.3157\n",
      "Epoch 214/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2877 - val_loss: 0.3131\n",
      "Epoch 215/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2865 - val_loss: 0.3148\n",
      "Epoch 216/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2860 - val_loss: 0.3175\n",
      "Epoch 217/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2863 - val_loss: 0.3092\n",
      "Epoch 218/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2848 - val_loss: 0.3163\n",
      "Epoch 219/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2874 - val_loss: 0.3141\n",
      "Epoch 220/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2880 - val_loss: 0.3087\n",
      "Epoch 221/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2853 - val_loss: 0.3166\n",
      "Epoch 222/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2848 - val_loss: 0.3116\n",
      "Epoch 223/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2862 - val_loss: 0.3099\n",
      "Epoch 224/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2844 - val_loss: 0.3090\n",
      "Epoch 225/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2848 - val_loss: 0.3100\n",
      "Epoch 226/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2853 - val_loss: 0.3142\n",
      "Epoch 227/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2860 - val_loss: 0.3166\n",
      "Epoch 228/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2847 - val_loss: 0.3138\n",
      "Epoch 229/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2855 - val_loss: 0.3128\n",
      "Epoch 230/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2860 - val_loss: 0.3123\n",
      "Epoch 231/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2864 - val_loss: 0.3126\n",
      "Epoch 232/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2851 - val_loss: 0.3084\n",
      "Epoch 233/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2840 - val_loss: 0.3128\n",
      "Epoch 234/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2844 - val_loss: 0.3123\n",
      "Epoch 235/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2845 - val_loss: 0.3166\n",
      "Epoch 236/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2826 - val_loss: 0.3131\n",
      "Epoch 237/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2861 - val_loss: 0.3131\n",
      "Epoch 238/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2826 - val_loss: 0.3115\n",
      "Epoch 239/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2843 - val_loss: 0.3096\n",
      "Epoch 240/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2855 - val_loss: 0.3109\n",
      "Epoch 241/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2830 - val_loss: 0.3082\n",
      "Epoch 242/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2830 - val_loss: 0.3096\n",
      "Epoch 243/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2825 - val_loss: 0.3089\n",
      "Epoch 244/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2840 - val_loss: 0.3249\n",
      "Epoch 245/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2830 - val_loss: 0.3126\n",
      "Epoch 246/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2840 - val_loss: 0.3194\n",
      "Epoch 247/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2815 - val_loss: 0.3133\n",
      "Epoch 248/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2827 - val_loss: 0.3148\n",
      "Epoch 249/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2821 - val_loss: 0.3113\n",
      "Epoch 250/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2822 - val_loss: 0.3107\n",
      "Epoch 251/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2828 - val_loss: 0.3076\n",
      "Epoch 252/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2809 - val_loss: 0.3080\n",
      "Epoch 253/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2822 - val_loss: 0.3082\n",
      "Epoch 254/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2820 - val_loss: 0.3161\n",
      "Epoch 255/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2831 - val_loss: 0.3114\n",
      "Epoch 256/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.2818 - val_loss: 0.3278\n",
      "Epoch 257/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2811 - val_loss: 0.3068\n",
      "Epoch 258/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2816 - val_loss: 0.3081\n",
      "Epoch 259/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2813 - val_loss: 0.3148\n",
      "Epoch 260/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2806 - val_loss: 0.3116\n",
      "Epoch 261/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2807 - val_loss: 0.3208\n",
      "Epoch 262/600\n",
      "14448/14448 [==============================] - 1s 63us/step - loss: 0.2813 - val_loss: 0.3069\n",
      "Epoch 263/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2800 - val_loss: 0.3082\n",
      "Epoch 264/600\n",
      "14448/14448 [==============================] - 1s 63us/step - loss: 0.2798 - val_loss: 0.3146\n",
      "Epoch 265/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2809 - val_loss: 0.3107\n",
      "Epoch 266/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2818 - val_loss: 0.3096\n",
      "Epoch 267/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2805 - val_loss: 0.3162\n",
      "Epoch 268/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2813 - val_loss: 0.3121\n",
      "Epoch 269/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2803 - val_loss: 0.3085\n",
      "Epoch 270/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2800 - val_loss: 0.3105\n",
      "Epoch 271/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2801 - val_loss: 0.3060\n",
      "Epoch 272/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2788 - val_loss: 0.3084\n",
      "Epoch 273/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2799 - val_loss: 0.3104\n",
      "Epoch 274/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2785 - val_loss: 0.3171\n",
      "Epoch 275/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2795 - val_loss: 0.3105\n",
      "Epoch 276/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2790 - val_loss: 0.3206\n",
      "Epoch 277/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2789 - val_loss: 0.3099\n",
      "Epoch 278/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2785 - val_loss: 0.3131\n",
      "Epoch 279/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2800 - val_loss: 0.3165\n",
      "Epoch 280/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2789 - val_loss: 0.3088\n",
      "Epoch 281/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2782 - val_loss: 0.3094\n",
      "Epoch 282/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2790 - val_loss: 0.3077\n",
      "Epoch 283/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2793 - val_loss: 0.3078\n",
      "Epoch 284/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2783 - val_loss: 0.3071\n",
      "Epoch 285/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2783 - val_loss: 0.3055\n",
      "Epoch 286/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2779 - val_loss: 0.3118\n",
      "Epoch 287/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2785 - val_loss: 0.3081\n",
      "Epoch 288/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2786 - val_loss: 0.3060\n",
      "Epoch 289/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2775 - val_loss: 0.3086\n",
      "Epoch 290/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2778 - val_loss: 0.3116\n",
      "Epoch 291/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2768 - val_loss: 0.3082\n",
      "Epoch 292/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2775 - val_loss: 0.3099\n",
      "Epoch 293/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2769 - val_loss: 0.3082\n",
      "Epoch 294/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2772 - val_loss: 0.3101\n",
      "Epoch 295/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2780 - val_loss: 0.3074\n",
      "Epoch 296/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2786 - val_loss: 0.3079\n",
      "Epoch 297/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2763 - val_loss: 0.3059\n",
      "Epoch 298/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2773 - val_loss: 0.3055\n",
      "Epoch 299/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2783 - val_loss: 0.3183\n",
      "Epoch 300/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2768 - val_loss: 0.3053\n",
      "Epoch 301/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2759 - val_loss: 0.3059\n",
      "Epoch 302/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2768 - val_loss: 0.3093\n",
      "Epoch 303/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2779 - val_loss: 0.3056\n",
      "Epoch 304/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2771 - val_loss: 0.3091\n",
      "Epoch 305/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2769 - val_loss: 0.3088\n",
      "Epoch 306/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2758 - val_loss: 0.3058\n",
      "Epoch 307/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2757 - val_loss: 0.3127\n",
      "Epoch 308/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2772 - val_loss: 0.3106\n",
      "Epoch 309/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2767 - val_loss: 0.3106\n",
      "Epoch 310/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2756 - val_loss: 0.3057\n",
      "Epoch 311/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2773 - val_loss: 0.3107\n",
      "Epoch 312/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2752 - val_loss: 0.3097\n",
      "Epoch 313/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2766 - val_loss: 0.3094\n",
      "Epoch 314/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2757 - val_loss: 0.3129\n",
      "Epoch 315/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2750 - val_loss: 0.3070\n",
      "Epoch 316/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2762 - val_loss: 0.3041\n",
      "Epoch 317/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2746 - val_loss: 0.3084\n",
      "Epoch 318/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2760 - val_loss: 0.3056\n",
      "Epoch 319/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2735 - val_loss: 0.3082\n",
      "Epoch 320/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2746 - val_loss: 0.3112\n",
      "Epoch 321/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2759 - val_loss: 0.3113\n",
      "Epoch 322/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2748 - val_loss: 0.3073\n",
      "Epoch 323/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2748 - val_loss: 0.3165\n",
      "Epoch 324/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2738 - val_loss: 0.3067\n",
      "Epoch 325/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2758 - val_loss: 0.3183\n",
      "Epoch 326/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2745 - val_loss: 0.3154\n",
      "Epoch 327/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2747 - val_loss: 0.3055\n",
      "Epoch 328/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2740 - val_loss: 0.3037\n",
      "Epoch 329/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2727 - val_loss: 0.3104\n",
      "Epoch 330/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2740 - val_loss: 0.3087\n",
      "Epoch 331/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2727 - val_loss: 0.3054\n",
      "Epoch 332/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2726 - val_loss: 0.3108\n",
      "Epoch 333/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2737 - val_loss: 0.3042\n",
      "Epoch 334/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2731 - val_loss: 0.3042\n",
      "Epoch 335/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2719 - val_loss: 0.3034\n",
      "Epoch 336/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2731 - val_loss: 0.3049\n",
      "Epoch 337/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2727 - val_loss: 0.3036\n",
      "Epoch 338/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2733 - val_loss: 0.3075\n",
      "Epoch 339/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2727 - val_loss: 0.3067\n",
      "Epoch 340/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2733 - val_loss: 0.3087\n",
      "Epoch 341/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2724 - val_loss: 0.3057\n",
      "Epoch 342/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2721 - val_loss: 0.3068\n",
      "Epoch 343/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2717 - val_loss: 0.3058\n",
      "Epoch 344/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2714 - val_loss: 0.3067\n",
      "Epoch 345/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2718 - val_loss: 0.3071\n",
      "Epoch 346/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2718 - val_loss: 0.3050\n",
      "Epoch 347/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2718 - val_loss: 0.3091\n",
      "Epoch 348/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2715 - val_loss: 0.3065\n",
      "Epoch 349/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2722 - val_loss: 0.3040\n",
      "Epoch 350/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2713 - val_loss: 0.3048\n",
      "Epoch 351/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2721 - val_loss: 0.3111\n",
      "Epoch 352/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2715 - val_loss: 0.3046\n",
      "Epoch 353/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2718 - val_loss: 0.3040\n",
      "Epoch 354/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2725 - val_loss: 0.3147\n",
      "Epoch 355/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2715 - val_loss: 0.3043\n",
      "Epoch 356/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2702 - val_loss: 0.3046\n",
      "Epoch 357/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2703 - val_loss: 0.3055\n",
      "Epoch 358/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2709 - val_loss: 0.3066\n",
      "Epoch 359/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2715 - val_loss: 0.3043\n",
      "Epoch 360/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2706 - val_loss: 0.3021\n",
      "Epoch 361/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2698 - val_loss: 0.3041\n",
      "Epoch 362/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2701 - val_loss: 0.3072\n",
      "Epoch 363/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2705 - val_loss: 0.3095\n",
      "Epoch 364/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2708 - val_loss: 0.3050\n",
      "Epoch 365/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2710 - val_loss: 0.3189\n",
      "Epoch 366/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2692 - val_loss: 0.3095\n",
      "Epoch 367/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2703 - val_loss: 0.3067\n",
      "Epoch 368/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2705 - val_loss: 0.3040\n",
      "Epoch 369/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2699 - val_loss: 0.3034\n",
      "Epoch 370/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2710 - val_loss: 0.3043\n",
      "Epoch 371/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2699 - val_loss: 0.3093\n",
      "Epoch 372/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2704 - val_loss: 0.3108\n",
      "Epoch 373/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2693 - val_loss: 0.3052\n",
      "Epoch 374/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2688 - val_loss: 0.3138\n",
      "Epoch 375/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2694 - val_loss: 0.3021\n",
      "Epoch 376/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2694 - val_loss: 0.3048\n",
      "Epoch 377/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2693 - val_loss: 0.3062\n",
      "Epoch 378/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2696 - val_loss: 0.3010\n",
      "Epoch 379/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2693 - val_loss: 0.3066\n",
      "Epoch 380/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2682 - val_loss: 0.3127\n",
      "Epoch 381/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2696 - val_loss: 0.3141\n",
      "Epoch 382/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2688 - val_loss: 0.3044\n",
      "Epoch 383/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2699 - val_loss: 0.3048\n",
      "Epoch 384/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2698 - val_loss: 0.3040\n",
      "Epoch 385/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2687 - val_loss: 0.3039\n",
      "Epoch 386/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2689 - val_loss: 0.3215\n",
      "Epoch 387/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2684 - val_loss: 0.3053\n",
      "Epoch 388/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2695 - val_loss: 0.3009\n",
      "Epoch 389/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2672 - val_loss: 0.3298\n",
      "Epoch 390/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2689 - val_loss: 0.3049\n",
      "Epoch 391/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2687 - val_loss: 0.3030\n",
      "Epoch 392/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2683 - val_loss: 0.3042\n",
      "Epoch 393/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.2689 - val_loss: 0.3024\n",
      "Epoch 394/600\n",
      "14448/14448 [==============================] - 1s 62us/step - loss: 0.2671 - val_loss: 0.3041\n",
      "Epoch 395/600\n",
      "14448/14448 [==============================] - 1s 67us/step - loss: 0.2681 - val_loss: 0.3019\n",
      "Epoch 396/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2698 - val_loss: 0.3013\n",
      "Epoch 397/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2677 - val_loss: 0.3118\n",
      "Epoch 398/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2672 - val_loss: 0.3036\n",
      "Epoch 399/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2682 - val_loss: 0.3103\n",
      "Epoch 400/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2679 - val_loss: 0.3010\n",
      "Epoch 401/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2682 - val_loss: 0.3052\n",
      "Epoch 402/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2683 - val_loss: 0.3019\n",
      "Epoch 403/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2685 - val_loss: 0.3035\n",
      "Epoch 404/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2689 - val_loss: 0.3004\n",
      "Epoch 405/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2673 - val_loss: 0.3033\n",
      "Epoch 406/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2681 - val_loss: 0.3000\n",
      "Epoch 407/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2673 - val_loss: 0.3027\n",
      "Epoch 408/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2677 - val_loss: 0.3009\n",
      "Epoch 409/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2673 - val_loss: 0.3124\n",
      "Epoch 410/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2675 - val_loss: 0.3029\n",
      "Epoch 411/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2663 - val_loss: 0.3014\n",
      "Epoch 412/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2667 - val_loss: 0.3024\n",
      "Epoch 413/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2673 - val_loss: 0.3083\n",
      "Epoch 414/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2676 - val_loss: 0.3021\n",
      "Epoch 415/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2662 - val_loss: 0.3021\n",
      "Epoch 416/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2663 - val_loss: 0.3030\n",
      "Epoch 417/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2663 - val_loss: 0.3054\n",
      "Epoch 418/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2665 - val_loss: 0.3066\n",
      "Epoch 419/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2666 - val_loss: 0.3030\n",
      "Epoch 420/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2666 - val_loss: 0.3049\n",
      "Epoch 421/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2671 - val_loss: 0.3075\n",
      "Epoch 422/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2662 - val_loss: 0.3141\n",
      "Epoch 423/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2670 - val_loss: 0.3081\n",
      "Epoch 424/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2663 - val_loss: 0.3029\n",
      "Epoch 425/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2658 - val_loss: 0.3040\n",
      "Epoch 426/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2661 - val_loss: 0.3096\n",
      "Epoch 427/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2660 - val_loss: 0.3033\n",
      "Epoch 428/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2668 - val_loss: 0.3030\n",
      "Epoch 429/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2651 - val_loss: 0.3100\n",
      "Epoch 430/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2657 - val_loss: 0.3002\n",
      "Epoch 431/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2664 - val_loss: 0.3189\n",
      "Epoch 432/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2661 - val_loss: 0.2983\n",
      "Epoch 433/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2645 - val_loss: 0.3025\n",
      "Epoch 434/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2655 - val_loss: 0.3050\n",
      "Epoch 435/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2656 - val_loss: 0.3093\n",
      "Epoch 436/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2657 - val_loss: 0.3013\n",
      "Epoch 437/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2670 - val_loss: 0.2996\n",
      "Epoch 438/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2658 - val_loss: 0.3003\n",
      "Epoch 439/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2642 - val_loss: 0.3018\n",
      "Epoch 440/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2662 - val_loss: 0.2997\n",
      "Epoch 441/600\n",
      "14448/14448 [==============================] - 1s 60us/step - loss: 0.2648 - val_loss: 0.2992\n",
      "Epoch 442/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2650 - val_loss: 0.3050\n",
      "Epoch 443/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2641 - val_loss: 0.3110\n",
      "Epoch 444/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2652 - val_loss: 0.3001\n",
      "Epoch 445/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2647 - val_loss: 0.2990\n",
      "Epoch 446/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2657 - val_loss: 0.2993\n",
      "Epoch 447/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2646 - val_loss: 0.3047\n",
      "Epoch 448/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2658 - val_loss: 0.3061\n",
      "Epoch 449/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2648 - val_loss: 0.2977\n",
      "Epoch 450/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2655 - val_loss: 0.2995\n",
      "Epoch 451/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2647 - val_loss: 0.3090\n",
      "Epoch 452/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2651 - val_loss: 0.3036\n",
      "Epoch 453/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2648 - val_loss: 0.2989\n",
      "Epoch 454/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2654 - val_loss: 0.3044\n",
      "Epoch 455/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2644 - val_loss: 0.3001\n",
      "Epoch 456/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2652 - val_loss: 0.3008\n",
      "Epoch 457/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2646 - val_loss: 0.2997\n",
      "Epoch 458/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2641 - val_loss: 0.3002\n",
      "Epoch 459/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2641 - val_loss: 0.3023\n",
      "Epoch 460/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2655 - val_loss: 0.3039\n",
      "Epoch 461/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2636 - val_loss: 0.3017\n",
      "Epoch 462/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2634 - val_loss: 0.2977\n",
      "Epoch 463/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2638 - val_loss: 0.3011\n",
      "Epoch 464/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2637 - val_loss: 0.2991\n",
      "Epoch 465/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2633 - val_loss: 0.3054\n",
      "Epoch 466/600\n",
      "14448/14448 [==============================] - 1s 61us/step - loss: 0.2631 - val_loss: 0.3108\n",
      "Epoch 467/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2637 - val_loss: 0.3015\n",
      "Epoch 468/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2646 - val_loss: 0.3134\n",
      "Epoch 469/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2634 - val_loss: 0.3030\n",
      "Epoch 470/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2636 - val_loss: 0.3024\n",
      "Epoch 471/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2636 - val_loss: 0.3040\n",
      "Epoch 472/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2640 - val_loss: 0.2984\n",
      "Epoch 473/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2625 - val_loss: 0.3018\n",
      "Epoch 474/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2630 - val_loss: 0.2994\n",
      "Epoch 475/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2635 - val_loss: 0.2995\n",
      "Epoch 476/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2636 - val_loss: 0.3007\n",
      "Epoch 477/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2634 - val_loss: 0.2988\n",
      "Epoch 478/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2627 - val_loss: 0.3007\n",
      "Epoch 479/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2632 - val_loss: 0.2971\n",
      "Epoch 480/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2636 - val_loss: 0.3052\n",
      "Epoch 481/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2634 - val_loss: 0.3007\n",
      "Epoch 482/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2634 - val_loss: 0.2996\n",
      "Epoch 483/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2629 - val_loss: 0.2971\n",
      "Epoch 484/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2631 - val_loss: 0.2997\n",
      "Epoch 485/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2632 - val_loss: 0.3112\n",
      "Epoch 486/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2626 - val_loss: 0.3014\n",
      "Epoch 487/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2629 - val_loss: 0.2971\n",
      "Epoch 488/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2628 - val_loss: 0.3035\n",
      "Epoch 489/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2623 - val_loss: 0.2995\n",
      "Epoch 490/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2629 - val_loss: 0.2999\n",
      "Epoch 491/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2613 - val_loss: 0.3008\n",
      "Epoch 492/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2633 - val_loss: 0.3049\n",
      "Epoch 493/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2627 - val_loss: 0.3025\n",
      "Epoch 494/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2622 - val_loss: 0.3044\n",
      "Epoch 495/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2640 - val_loss: 0.2976\n",
      "Epoch 496/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2618 - val_loss: 0.2974\n",
      "Epoch 497/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2620 - val_loss: 0.2999\n",
      "Epoch 498/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2624 - val_loss: 0.2979\n",
      "Epoch 499/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2631 - val_loss: 0.3025\n",
      "Epoch 500/600\n",
      "14448/14448 [==============================] - 1s 59us/step - loss: 0.2625 - val_loss: 0.2986\n",
      "Epoch 501/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2624 - val_loss: 0.2968\n",
      "Epoch 502/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2611 - val_loss: 0.2995\n",
      "Epoch 503/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2624 - val_loss: 0.3068\n",
      "Epoch 504/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2614 - val_loss: 0.2988\n",
      "Epoch 505/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2627 - val_loss: 0.2981\n",
      "Epoch 506/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2617 - val_loss: 0.2999\n",
      "Epoch 507/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2618 - val_loss: 0.2977\n",
      "Epoch 508/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2618 - val_loss: 0.3046\n",
      "Epoch 509/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2616 - val_loss: 0.2968\n",
      "Epoch 510/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2616 - val_loss: 0.3040\n",
      "Epoch 511/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2607 - val_loss: 0.2998\n",
      "Epoch 512/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2611 - val_loss: 0.2973\n",
      "Epoch 513/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2615 - val_loss: 0.2997\n",
      "Epoch 514/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2614 - val_loss: 0.3014\n",
      "Epoch 515/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2617 - val_loss: 0.2984\n",
      "Epoch 516/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2617 - val_loss: 0.3013\n",
      "Epoch 517/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2601 - val_loss: 0.2986\n",
      "Epoch 518/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2606 - val_loss: 0.2968\n",
      "Epoch 519/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2602 - val_loss: 0.2990\n",
      "Epoch 520/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2610 - val_loss: 0.2976\n",
      "Epoch 521/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2615 - val_loss: 0.2977\n",
      "Epoch 522/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2604 - val_loss: 0.2978\n",
      "Epoch 523/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2615 - val_loss: 0.3195\n",
      "Epoch 524/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2604 - val_loss: 0.3007\n",
      "Epoch 525/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2606 - val_loss: 0.3086\n",
      "Epoch 526/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2619 - val_loss: 0.3002\n",
      "Epoch 527/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2608 - val_loss: 0.2989\n",
      "Epoch 528/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2609 - val_loss: 0.3007\n",
      "Epoch 529/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2603 - val_loss: 0.2988\n",
      "Epoch 530/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2608 - val_loss: 0.2962\n",
      "Epoch 531/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2611 - val_loss: 0.2980\n",
      "Epoch 532/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2593 - val_loss: 0.3000\n",
      "Epoch 533/600\n",
      "14448/14448 [==============================] - 1s 51us/step - loss: 0.2615 - val_loss: 0.2972\n",
      "Epoch 534/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2605 - val_loss: 0.2968\n",
      "Epoch 535/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2595 - val_loss: 0.2994\n",
      "Epoch 536/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2607 - val_loss: 0.2980\n",
      "Epoch 537/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2601 - val_loss: 0.2981\n",
      "Epoch 538/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2601 - val_loss: 0.3028\n",
      "Epoch 539/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2610 - val_loss: 0.2975\n",
      "Epoch 540/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2597 - val_loss: 0.2984\n",
      "Epoch 541/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2602 - val_loss: 0.2997\n",
      "Epoch 542/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2601 - val_loss: 0.2990\n",
      "Epoch 543/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2594 - val_loss: 0.2979\n",
      "Epoch 544/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2602 - val_loss: 0.3024\n",
      "Epoch 545/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2608 - val_loss: 0.2976\n",
      "Epoch 546/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2605 - val_loss: 0.2984\n",
      "Epoch 547/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2596 - val_loss: 0.2975\n",
      "Epoch 548/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2596 - val_loss: 0.3002\n",
      "Epoch 549/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2597 - val_loss: 0.2973\n",
      "Epoch 550/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2595 - val_loss: 0.2982\n",
      "Epoch 551/600\n",
      "14448/14448 [==============================] - 1s 58us/step - loss: 0.2594 - val_loss: 0.2995\n",
      "Epoch 552/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2593 - val_loss: 0.2960\n",
      "Epoch 553/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2601 - val_loss: 0.2972\n",
      "Epoch 554/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2591 - val_loss: 0.3002\n",
      "Epoch 555/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2599 - val_loss: 0.2975\n",
      "Epoch 556/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2594 - val_loss: 0.3022\n",
      "Epoch 557/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2595 - val_loss: 0.3014\n",
      "Epoch 558/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2594 - val_loss: 0.3043\n",
      "Epoch 559/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2597 - val_loss: 0.2998\n",
      "Epoch 560/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2589 - val_loss: 0.2951\n",
      "Epoch 561/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2589 - val_loss: 0.3045\n",
      "Epoch 562/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2591 - val_loss: 0.3005\n",
      "Epoch 563/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2595 - val_loss: 0.2967\n",
      "Epoch 564/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2598 - val_loss: 0.2985\n",
      "Epoch 565/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2602 - val_loss: 0.2983\n",
      "Epoch 566/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2596 - val_loss: 0.3001\n",
      "Epoch 567/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2591 - val_loss: 0.2959\n",
      "Epoch 568/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2581 - val_loss: 0.2980\n",
      "Epoch 569/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2592 - val_loss: 0.2982\n",
      "Epoch 570/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2586 - val_loss: 0.3016\n",
      "Epoch 571/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2583 - val_loss: 0.2958\n",
      "Epoch 572/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2589 - val_loss: 0.2981\n",
      "Epoch 573/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2581 - val_loss: 0.2946\n",
      "Epoch 574/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2589 - val_loss: 0.2961\n",
      "Epoch 575/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2589 - val_loss: 0.2950\n",
      "Epoch 576/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2591 - val_loss: 0.2954\n",
      "Epoch 577/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2603 - val_loss: 0.2972\n",
      "Epoch 578/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2595 - val_loss: 0.2942\n",
      "Epoch 579/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2588 - val_loss: 0.2973\n",
      "Epoch 580/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2582 - val_loss: 0.3011\n",
      "Epoch 581/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2590 - val_loss: 0.2979\n",
      "Epoch 582/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2581 - val_loss: 0.2999\n",
      "Epoch 583/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2587 - val_loss: 0.2983\n",
      "Epoch 584/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2589 - val_loss: 0.2972\n",
      "Epoch 585/600\n",
      "14448/14448 [==============================] - 1s 56us/step - loss: 0.2589 - val_loss: 0.2971\n",
      "Epoch 586/600\n",
      "14448/14448 [==============================] - 1s 57us/step - loss: 0.2587 - val_loss: 0.2990\n",
      "Epoch 587/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2582 - val_loss: 0.3057\n",
      "Epoch 588/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2584 - val_loss: 0.2952\n",
      "Epoch 589/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2576 - val_loss: 0.3060\n",
      "Epoch 590/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2582 - val_loss: 0.2964\n",
      "Epoch 591/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2582 - val_loss: 0.2974\n",
      "Epoch 592/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2575 - val_loss: 0.2970\n",
      "Epoch 593/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2583 - val_loss: 0.3017\n",
      "Epoch 594/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2582 - val_loss: 0.2957\n",
      "Epoch 595/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2582 - val_loss: 0.2958\n",
      "Epoch 596/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2576 - val_loss: 0.2968\n",
      "Epoch 597/600\n",
      "14448/14448 [==============================] - 1s 54us/step - loss: 0.2570 - val_loss: 0.2971\n",
      "Epoch 598/600\n",
      "14448/14448 [==============================] - 1s 52us/step - loss: 0.2584 - val_loss: 0.2967\n",
      "Epoch 599/600\n",
      "14448/14448 [==============================] - 1s 55us/step - loss: 0.2582 - val_loss: 0.2960\n",
      "Epoch 600/600\n",
      "14448/14448 [==============================] - 1s 53us/step - loss: 0.2577 - val_loss: 0.3051\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAELCAYAAADTK53JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njEVvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(8,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600,validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPx01rPcGAKU"
   },
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxnnQWzPGAKV"
   },
   "source": [
    "## Removing extreme points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684808,
     "status": "ok",
     "timestamp": 1572239194591,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "zt4HdIlUGAKX",
    "outputId": "69b276de-a781-47ff-88ef-e697e8aeb0d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEROOMS: [  4.44071624   5.375        6.05238095 141.90909091]\n",
      "99TH AND 100TH PERCENTILES OF FEATURE AVEROOMS: [ 10.35703302 141.90909091]\n"
     ]
    }
   ],
   "source": [
    "print('25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEROOMS:',np.percentile(da.AveRooms, [25,55,75,100]))\n",
    "print('99TH AND 100TH PERCENTILES OF FEATURE AVEROOMS:',np.percentile(da.AveRooms, [99,100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684786,
     "status": "ok",
     "timestamp": 1572239194594,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "-ZLs5h0wGAKa",
    "outputId": "f3917903-257b-412d-d7db-b8019ccdf1dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEOCCUP: [   2.42974115    2.89550479    3.28226092 1243.33333333]\n",
      "99TH AND 100TH PERCENTILES OF FEATURE AVEOCCUPS: [   5.39481203 1243.33333333]\n"
     ]
    }
   ],
   "source": [
    "print('25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEOCCUP:',np.percentile(da.AveOccup, [25,55,75,100]))\n",
    "print('99TH AND 100TH PERCENTILES OF FEATURE AVEOCCUPS:',np.percentile(da.AveOccup, [99,100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684778,
     "status": "ok",
     "timestamp": 1572239194596,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "5_pD0TsiGAKd",
    "outputId": "796bd326-6612-421f-b51f-d2fe3703fbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE  population: [  787.  1255.  1725. 35682.]\n",
      "99TH AND 100TH PERCENTILES OF FEATURE population: [ 5805.83 35682.  ]\n"
     ]
    }
   ],
   "source": [
    "print('25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE  population:',np.percentile(da. Population, [25,55,75,100]))\n",
    "print('99TH AND 100TH PERCENTILES OF FEATURE population:',np.percentile(da.Population, [99,100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684767,
     "status": "ok",
     "timestamp": 1572239194598,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "ve7rxP2kGAKj",
    "outputId": "bf4ebe4b-b806-4167-bf48-f58b0d0308b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS: [ 1.00607905  1.05712292  1.09952607 34.06666667]\n",
      "99TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS: [ 2.12754082 34.06666667]\n"
     ]
    }
   ],
   "source": [
    "print('25TH,55TH,75TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS:',np.percentile(da.AveBedrms, [25,55,75,100]))\n",
    "print('99TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS:',np.percentile(da.AveBedrms, [99,100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmLZG5aUGAKm"
   },
   "outputs": [],
   "source": [
    "#Selected these limits based on iqr.\n",
    "dy=pd.DataFrame(d['target'])\n",
    "a=np.where(da.AveRooms>=12)\n",
    "da.drop(da.index[a],inplace=True)\n",
    "dy.drop(dy.index[a],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHhUqllbGAKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "b=np.where(da.AveOccup>=6)\n",
    "da.drop(da.index[b],inplace=True)\n",
    "dy.drop(dy.index[b],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFuzzvNdGAKt"
   },
   "outputs": [],
   "source": [
    "c=np.where(da.AveBedrms>=3)\n",
    "da.drop(da.index[c],inplace=True)\n",
    "dy.drop(dy.index[c],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Sp_mgmGAKw"
   },
   "outputs": [],
   "source": [
    "D=np.where(da.Population>=5810)\n",
    "da.drop(da.index[D],inplace=True)\n",
    "dy.drop(dy.index[D],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1685141,
     "status": "ok",
     "timestamp": 1572239195075,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "JWE1NAxLGAK1",
    "outputId": "3a9ecc30-4575-4307-fd9c-8c15dc264cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data after removing extremities/outliers (20180, 8)\n",
      "Total percentage of datapoints removed are: 2.2243713733075436\n"
     ]
    }
   ],
   "source": [
    "print('shape of data after removing extremities/outliers',da.shape)\n",
    "print ('Total percentage of datapoints removed are:',(((20640-20180)/20680)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1685129,
     "status": "ok",
     "timestamp": 1572239195077,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "W2nR_28hGAK6",
    "outputId": "c1801a11-28a1-4950-bea4-fa4779973682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of train : (14126, 8)\n",
      "dimension of test:  (6054, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(da, dy,test_size = 0.30)\n",
    "print('dimension of train :',X_train.shape)\n",
    "print('dimension of test: ',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1685118,
     "status": "ok",
     "timestamp": 1572239195080,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "XpjhrmBkGAK_",
    "outputId": "22f736f7-b2d2-49cc-ad0b-b8581194e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14126, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STANDARDISING \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc=preprocessing.StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIbyrc7NGALE"
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X=X_train, y=y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=True)\n",
    "valid_pred = lm.predict(X_test)\n",
    "train_pred = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1685096,
     "status": "ok",
     "timestamp": 1572239195084,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "qSijh89WGALJ",
    "outputId": "eddf07f4-08f6-44c7-b1f7-1fc0d4689064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on train by using linear regression 0.4567261031040791\n",
      "error on test by using linear regression 0.4187001516635067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('error on train by using linear regression',mean_squared_error(y_train, train_pred))\n",
    "print('error on test by using linear regression',mean_squared_error(y_test,valid_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12T7deiJGALM"
   },
   "source": [
    "##### Training NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1 with relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2075782,
     "status": "ok",
     "timestamp": 1572239585787,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "z50TW6GEGALN",
    "outputId": "c8808da7-0955-480c-8ab8-0902338d1e61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14126 samples, validate on 6054 samples\n",
      "Epoch 1/600\n",
      "14126/14126 [==============================] - 1s 71us/step - loss: 3.6874 - val_loss: 2.5740\n",
      "Epoch 2/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 2.0739 - val_loss: 1.4169\n",
      "Epoch 3/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 1.0929 - val_loss: 0.7245\n",
      "Epoch 4/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.6788 - val_loss: 0.5402\n",
      "Epoch 5/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.5614 - val_loss: 0.4855\n",
      "Epoch 6/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.5145 - val_loss: 0.4569\n",
      "Epoch 7/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4861 - val_loss: 0.4368\n",
      "Epoch 8/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4663 - val_loss: 0.4223\n",
      "Epoch 9/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4540 - val_loss: 0.4124\n",
      "Epoch 10/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4466 - val_loss: 0.4093\n",
      "Epoch 11/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4425 - val_loss: 0.4036\n",
      "Epoch 12/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4394 - val_loss: 0.4047\n",
      "Epoch 13/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4379 - val_loss: 0.3993\n",
      "Epoch 14/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4366 - val_loss: 0.3985\n",
      "Epoch 15/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4353 - val_loss: 0.3976\n",
      "Epoch 16/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4347 - val_loss: 0.3960\n",
      "Epoch 17/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4343 - val_loss: 0.3960\n",
      "Epoch 18/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4336 - val_loss: 0.3946\n",
      "Epoch 19/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4333 - val_loss: 0.3949\n",
      "Epoch 20/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4330 - val_loss: 0.3951\n",
      "Epoch 21/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4328 - val_loss: 0.3939\n",
      "Epoch 22/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4323 - val_loss: 0.3954\n",
      "Epoch 23/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4323 - val_loss: 0.3940\n",
      "Epoch 24/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4322 - val_loss: 0.3933\n",
      "Epoch 25/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3941\n",
      "Epoch 26/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4323 - val_loss: 0.3937\n",
      "Epoch 27/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4321 - val_loss: 0.3935\n",
      "Epoch 28/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4321 - val_loss: 0.3943\n",
      "Epoch 29/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4322 - val_loss: 0.3924\n",
      "Epoch 30/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3927\n",
      "Epoch 31/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3933\n",
      "Epoch 32/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4322 - val_loss: 0.3924\n",
      "Epoch 33/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 34/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 35/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4320 - val_loss: 0.3923\n",
      "Epoch 36/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3945\n",
      "Epoch 37/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4320 - val_loss: 0.3933\n",
      "Epoch 38/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 39/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 40/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 41/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 42/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 43/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3917\n",
      "Epoch 44/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3916\n",
      "Epoch 45/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3916\n",
      "Epoch 46/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3926\n",
      "Epoch 47/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 48/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3914\n",
      "Epoch 49/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4321 - val_loss: 0.3933\n",
      "Epoch 50/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 51/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4321 - val_loss: 0.3937\n",
      "Epoch 52/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3922\n",
      "Epoch 53/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 54/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 55/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 56/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3918\n",
      "Epoch 57/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3924\n",
      "Epoch 58/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4320 - val_loss: 0.3931\n",
      "Epoch 59/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4320 - val_loss: 0.3927\n",
      "Epoch 60/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 61/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 62/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4315 - val_loss: 0.3930\n",
      "Epoch 63/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4316 - val_loss: 0.3929\n",
      "Epoch 64/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.4320 - val_loss: 0.3931\n",
      "Epoch 65/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4314 - val_loss: 0.3919\n",
      "Epoch 66/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 67/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 68/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.4319 - val_loss: 0.3931\n",
      "Epoch 69/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 70/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 71/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 72/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 73/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4318 - val_loss: 0.3935\n",
      "Epoch 74/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3918\n",
      "Epoch 75/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3930\n",
      "Epoch 76/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3921\n",
      "Epoch 77/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3933\n",
      "Epoch 78/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 79/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 80/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3931\n",
      "Epoch 81/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3928\n",
      "Epoch 82/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3941\n",
      "Epoch 83/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4319 - val_loss: 0.3932\n",
      "Epoch 84/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 85/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4320 - val_loss: 0.3915\n",
      "Epoch 86/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3914\n",
      "Epoch 87/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4320 - val_loss: 0.3938\n",
      "Epoch 88/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3913\n",
      "Epoch 89/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 90/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 91/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3932\n",
      "Epoch 92/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4317 - val_loss: 0.3930\n",
      "Epoch 93/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4313 - val_loss: 0.3951\n",
      "Epoch 94/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3936\n",
      "Epoch 95/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 96/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 97/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 98/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4320 - val_loss: 0.3926\n",
      "Epoch 99/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 100/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 101/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 102/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3932\n",
      "Epoch 103/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4320 - val_loss: 0.3920\n",
      "Epoch 104/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 105/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3926\n",
      "Epoch 106/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3936\n",
      "Epoch 107/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 108/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3917\n",
      "Epoch 109/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3942\n",
      "Epoch 110/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 111/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3916\n",
      "Epoch 112/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3920\n",
      "Epoch 113/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3955\n",
      "Epoch 114/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.4322 - val_loss: 0.3928\n",
      "Epoch 115/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 116/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 117/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 118/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3918\n",
      "Epoch 119/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 120/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3917\n",
      "Epoch 121/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 122/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 123/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3923\n",
      "Epoch 124/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 125/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 126/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 127/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 128/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3916\n",
      "Epoch 129/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 130/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 131/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3918\n",
      "Epoch 132/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4315 - val_loss: 0.3930\n",
      "Epoch 133/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 134/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3920\n",
      "Epoch 135/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3949\n",
      "Epoch 136/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 137/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 138/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 139/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3918\n",
      "Epoch 140/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3934\n",
      "Epoch 141/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 142/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3936\n",
      "Epoch 143/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4321 - val_loss: 0.3924\n",
      "Epoch 144/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 145/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3923\n",
      "Epoch 146/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3941\n",
      "Epoch 147/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3925\n",
      "Epoch 148/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4313 - val_loss: 0.3954\n",
      "Epoch 149/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3933\n",
      "Epoch 150/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4321 - val_loss: 0.3917\n",
      "Epoch 151/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 152/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 153/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 154/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3933\n",
      "Epoch 155/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 156/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 157/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 158/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3924\n",
      "Epoch 159/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 160/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 161/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 162/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 163/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 164/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 165/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 166/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 167/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3927\n",
      "Epoch 168/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 169/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 170/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 171/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4320 - val_loss: 0.3925\n",
      "Epoch 172/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3939\n",
      "Epoch 173/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 174/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3925\n",
      "Epoch 175/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3931\n",
      "Epoch 176/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3939\n",
      "Epoch 177/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3941\n",
      "Epoch 178/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3937\n",
      "Epoch 179/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3928\n",
      "Epoch 180/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 181/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3928\n",
      "Epoch 182/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 183/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3925\n",
      "Epoch 184/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3917\n",
      "Epoch 185/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 186/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4320 - val_loss: 0.3926\n",
      "Epoch 187/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3931\n",
      "Epoch 188/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 189/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3928\n",
      "Epoch 190/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 191/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4317 - val_loss: 0.3931\n",
      "Epoch 192/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 193/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 194/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3932\n",
      "Epoch 195/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3916\n",
      "Epoch 196/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4314 - val_loss: 0.3918\n",
      "Epoch 197/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 198/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3925\n",
      "Epoch 199/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 200/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3923\n",
      "Epoch 201/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 202/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3932\n",
      "Epoch 203/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3920\n",
      "Epoch 204/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4321 - val_loss: 0.3925\n",
      "Epoch 205/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 206/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 207/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 208/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3936\n",
      "Epoch 209/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 210/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 211/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 212/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3938\n",
      "Epoch 213/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4319 - val_loss: 0.3930\n",
      "Epoch 214/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4314 - val_loss: 0.3922\n",
      "Epoch 215/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 216/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 217/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 218/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 219/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 220/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4315 - val_loss: 0.3936\n",
      "Epoch 221/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4322 - val_loss: 0.3928\n",
      "Epoch 222/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 223/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3913\n",
      "Epoch 224/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 225/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4321 - val_loss: 0.3926\n",
      "Epoch 226/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 227/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 228/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3941\n",
      "Epoch 229/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 230/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3925\n",
      "Epoch 231/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 232/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3918\n",
      "Epoch 233/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 234/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 235/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3917\n",
      "Epoch 236/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 237/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 238/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3918\n",
      "Epoch 239/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3930\n",
      "Epoch 240/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 241/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 242/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3921\n",
      "Epoch 243/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3943\n",
      "Epoch 244/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 245/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 246/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 247/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4320 - val_loss: 0.3924\n",
      "Epoch 248/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4315 - val_loss: 0.3918\n",
      "Epoch 249/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4317 - val_loss: 0.3916\n",
      "Epoch 250/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 251/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 252/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 253/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 254/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4313 - val_loss: 0.3919\n",
      "Epoch 255/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3917\n",
      "Epoch 256/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 257/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 258/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 259/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3934\n",
      "Epoch 260/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 261/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 262/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 263/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 264/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 265/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3937\n",
      "Epoch 266/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 267/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3921\n",
      "Epoch 268/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 269/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3939\n",
      "Epoch 270/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 271/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4313 - val_loss: 0.3924\n",
      "Epoch 272/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3921\n",
      "Epoch 273/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 274/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 275/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3928\n",
      "Epoch 276/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3940\n",
      "Epoch 277/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 278/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3931\n",
      "Epoch 279/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 280/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 281/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3923\n",
      "Epoch 282/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3924\n",
      "Epoch 283/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 284/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3931\n",
      "Epoch 285/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 286/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 287/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3928\n",
      "Epoch 288/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3926\n",
      "Epoch 289/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3930\n",
      "Epoch 290/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 291/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3920\n",
      "Epoch 292/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 293/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3933\n",
      "Epoch 294/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 295/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3932\n",
      "Epoch 296/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3944\n",
      "Epoch 297/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 298/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3949\n",
      "Epoch 299/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 300/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 301/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3947\n",
      "Epoch 302/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3932\n",
      "Epoch 303/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 304/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 305/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3935\n",
      "Epoch 306/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 307/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3925\n",
      "Epoch 308/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 309/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 310/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3924\n",
      "Epoch 311/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 312/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4321 - val_loss: 0.3924\n",
      "Epoch 313/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3915\n",
      "Epoch 314/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 315/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3918\n",
      "Epoch 316/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 317/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3924\n",
      "Epoch 318/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 319/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 320/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 321/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3925\n",
      "Epoch 322/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3916\n",
      "Epoch 323/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3916\n",
      "Epoch 324/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3951\n",
      "Epoch 325/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3916\n",
      "Epoch 326/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4313 - val_loss: 0.3929\n",
      "Epoch 327/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3929\n",
      "Epoch 328/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3934\n",
      "Epoch 329/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3930\n",
      "Epoch 330/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4315 - val_loss: 0.3922\n",
      "Epoch 331/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3915\n",
      "Epoch 332/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 333/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3937\n",
      "Epoch 334/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3917\n",
      "Epoch 335/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 336/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4314 - val_loss: 0.3926\n",
      "Epoch 337/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3914\n",
      "Epoch 338/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3915\n",
      "Epoch 339/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 340/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 341/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3916\n",
      "Epoch 342/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 343/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 344/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3936\n",
      "Epoch 345/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 346/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 347/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3935\n",
      "Epoch 348/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4314 - val_loss: 0.3929\n",
      "Epoch 349/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4322 - val_loss: 0.3919\n",
      "Epoch 350/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3938\n",
      "Epoch 351/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 352/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 353/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 354/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3939\n",
      "Epoch 355/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3921\n",
      "Epoch 356/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3937\n",
      "Epoch 357/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4320 - val_loss: 0.3920\n",
      "Epoch 358/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 359/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4315 - val_loss: 0.3940\n",
      "Epoch 360/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 361/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 362/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3928\n",
      "Epoch 363/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3915\n",
      "Epoch 364/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3942\n",
      "Epoch 365/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 366/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 367/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3916\n",
      "Epoch 368/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 369/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 370/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3912\n",
      "Epoch 371/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3923\n",
      "Epoch 372/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3915\n",
      "Epoch 373/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3942\n",
      "Epoch 374/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 375/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3920\n",
      "Epoch 376/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3950\n",
      "Epoch 377/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3933\n",
      "Epoch 378/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3922\n",
      "Epoch 379/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3917\n",
      "Epoch 380/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 381/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 382/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 383/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 384/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 385/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4314 - val_loss: 0.3924\n",
      "Epoch 386/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4315 - val_loss: 0.3922\n",
      "Epoch 387/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 388/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3915\n",
      "Epoch 389/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3940\n",
      "Epoch 390/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 391/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 392/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 393/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 394/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 395/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3926\n",
      "Epoch 396/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3928\n",
      "Epoch 397/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3932\n",
      "Epoch 398/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3930\n",
      "Epoch 399/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3919\n",
      "Epoch 400/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3939\n",
      "Epoch 401/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 402/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 403/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3940\n",
      "Epoch 404/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3923\n",
      "Epoch 405/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 406/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 407/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 408/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 409/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3934\n",
      "Epoch 410/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3928\n",
      "Epoch 411/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3916\n",
      "Epoch 412/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3929\n",
      "Epoch 413/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3926\n",
      "Epoch 414/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 415/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3922\n",
      "Epoch 416/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 417/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4314 - val_loss: 0.3930\n",
      "Epoch 418/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 419/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3934\n",
      "Epoch 420/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3918\n",
      "Epoch 421/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3923\n",
      "Epoch 422/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4319 - val_loss: 0.3920\n",
      "Epoch 423/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3927\n",
      "Epoch 424/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3927\n",
      "Epoch 425/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4314 - val_loss: 0.3934\n",
      "Epoch 426/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3925\n",
      "Epoch 427/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 428/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 429/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 430/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 431/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3935\n",
      "Epoch 432/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3921\n",
      "Epoch 433/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 434/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3930\n",
      "Epoch 435/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3917\n",
      "Epoch 436/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3946\n",
      "Epoch 437/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 438/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3923\n",
      "Epoch 439/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4315 - val_loss: 0.3935\n",
      "Epoch 440/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3940\n",
      "Epoch 441/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 442/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4316 - val_loss: 0.3924\n",
      "Epoch 443/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3926\n",
      "Epoch 444/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4314 - val_loss: 0.3929\n",
      "Epoch 445/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3934\n",
      "Epoch 446/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 447/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3923\n",
      "Epoch 448/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4315 - val_loss: 0.3936\n",
      "Epoch 449/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3925\n",
      "Epoch 450/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 451/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 452/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3918\n",
      "Epoch 453/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 454/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 455/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 456/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 457/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3927\n",
      "Epoch 458/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4315 - val_loss: 0.3925\n",
      "Epoch 459/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 460/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3922\n",
      "Epoch 461/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 462/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3924\n",
      "Epoch 463/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3923\n",
      "Epoch 464/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3927\n",
      "Epoch 465/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 466/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 467/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 468/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3930\n",
      "Epoch 469/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3933\n",
      "Epoch 470/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3938\n",
      "Epoch 471/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3936\n",
      "Epoch 472/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3937\n",
      "Epoch 473/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4313 - val_loss: 0.3921\n",
      "Epoch 474/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3914\n",
      "Epoch 475/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 476/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3921\n",
      "Epoch 477/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 478/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4314 - val_loss: 0.3932\n",
      "Epoch 479/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3932\n",
      "Epoch 480/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3924\n",
      "Epoch 481/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 482/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 483/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 484/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3929\n",
      "Epoch 485/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 486/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 487/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3934\n",
      "Epoch 488/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4320 - val_loss: 0.3926\n",
      "Epoch 489/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4313 - val_loss: 0.3923\n",
      "Epoch 490/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 491/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3916\n",
      "Epoch 492/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3925\n",
      "Epoch 493/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 494/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3928\n",
      "Epoch 495/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 496/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3920\n",
      "Epoch 497/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3933\n",
      "Epoch 498/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3924\n",
      "Epoch 499/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3933\n",
      "Epoch 500/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3923\n",
      "Epoch 501/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3926\n",
      "Epoch 502/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3926\n",
      "Epoch 503/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 504/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4316 - val_loss: 0.3925\n",
      "Epoch 505/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4316 - val_loss: 0.3930\n",
      "Epoch 506/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 507/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3917\n",
      "Epoch 508/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3924\n",
      "Epoch 509/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4313 - val_loss: 0.3934\n",
      "Epoch 510/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3916\n",
      "Epoch 511/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3930\n",
      "Epoch 512/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3938\n",
      "Epoch 513/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4315 - val_loss: 0.3953\n",
      "Epoch 514/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3930\n",
      "Epoch 515/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3937\n",
      "Epoch 516/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4319 - val_loss: 0.3925\n",
      "Epoch 517/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 518/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 519/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3932\n",
      "Epoch 520/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3926\n",
      "Epoch 521/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3914\n",
      "Epoch 522/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4314 - val_loss: 0.3939\n",
      "Epoch 523/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3920\n",
      "Epoch 524/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 525/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3918\n",
      "Epoch 526/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3912\n",
      "Epoch 527/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3914\n",
      "Epoch 528/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 529/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 530/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3935\n",
      "Epoch 531/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 532/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3917\n",
      "Epoch 533/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4316 - val_loss: 0.3917\n",
      "Epoch 534/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4317 - val_loss: 0.3927\n",
      "Epoch 535/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3919\n",
      "Epoch 536/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 537/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 538/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4315 - val_loss: 0.3928\n",
      "Epoch 539/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 540/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4316 - val_loss: 0.3946\n",
      "Epoch 541/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4318 - val_loss: 0.3936\n",
      "Epoch 542/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.4316 - val_loss: 0.3924\n",
      "Epoch 543/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 544/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 545/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4318 - val_loss: 0.3927\n",
      "Epoch 546/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 547/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4316 - val_loss: 0.3929\n",
      "Epoch 548/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 549/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4318 - val_loss: 0.3920\n",
      "Epoch 550/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3925\n",
      "Epoch 551/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 552/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.4319 - val_loss: 0.3922\n",
      "Epoch 553/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4319 - val_loss: 0.3924\n",
      "Epoch 554/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4316 - val_loss: 0.3940\n",
      "Epoch 555/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.4318 - val_loss: 0.3922\n",
      "Epoch 556/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3913\n",
      "Epoch 557/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 558/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3922\n",
      "Epoch 559/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3929\n",
      "Epoch 560/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 561/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 562/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3937\n",
      "Epoch 563/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3929\n",
      "Epoch 564/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3932\n",
      "Epoch 565/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3929\n",
      "Epoch 566/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3929\n",
      "Epoch 567/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3928\n",
      "Epoch 568/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4320 - val_loss: 0.3918\n",
      "Epoch 569/600\n",
      "14126/14126 [==============================] - 1s 43us/step - loss: 0.4315 - val_loss: 0.3916\n",
      "Epoch 570/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4314 - val_loss: 0.3931\n",
      "Epoch 571/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 572/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 573/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4314 - val_loss: 0.3939\n",
      "Epoch 574/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3929\n",
      "Epoch 575/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3929\n",
      "Epoch 576/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4314 - val_loss: 0.3925\n",
      "Epoch 577/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 578/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4317 - val_loss: 0.3926\n",
      "Epoch 579/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3922\n",
      "Epoch 580/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3930\n",
      "Epoch 581/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4319 - val_loss: 0.3926\n",
      "Epoch 582/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4315 - val_loss: 0.3933\n",
      "Epoch 583/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3922\n",
      "Epoch 584/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4317 - val_loss: 0.3921\n",
      "Epoch 585/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4319 - val_loss: 0.3916\n",
      "Epoch 586/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4318 - val_loss: 0.3914\n",
      "Epoch 587/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3918\n",
      "Epoch 588/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3915\n",
      "Epoch 589/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4318 - val_loss: 0.3931\n",
      "Epoch 590/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3930\n",
      "Epoch 591/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3931\n",
      "Epoch 592/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4313 - val_loss: 0.3919\n",
      "Epoch 593/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3923\n",
      "Epoch 594/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4316 - val_loss: 0.3917\n",
      "Epoch 595/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4317 - val_loss: 0.3919\n",
      "Epoch 596/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4318 - val_loss: 0.3921\n",
      "Epoch 597/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4319 - val_loss: 0.3934\n",
      "Epoch 598/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3919\n",
      "Epoch 599/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4316 - val_loss: 0.3923\n",
      "Epoch 600/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4315 - val_loss: 0.3955\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAELCAYAAADTK53JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='relu', input_shape=(8,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1 with sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2470460,
     "status": "ok",
     "timestamp": 1572239980488,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "f_Ke4zx7I7pq",
    "outputId": "215a70d9-442f-49b9-9cb2-1450451ce804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14126 samples, validate on 6054 samples\n",
      "Epoch 1/600\n",
      "14126/14126 [==============================] - 1s 78us/step - loss: 3.4936 - val_loss: 2.3429\n",
      "Epoch 2/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 1.8444 - val_loss: 1.2822\n",
      "Epoch 3/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 1.1504 - val_loss: 0.9340\n",
      "Epoch 4/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.9172 - val_loss: 0.8028\n",
      "Epoch 5/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.8034 - val_loss: 0.7134\n",
      "Epoch 6/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.7163 - val_loss: 0.6369\n",
      "Epoch 7/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.6420 - val_loss: 0.5711\n",
      "Epoch 8/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.5809 - val_loss: 0.5192\n",
      "Epoch 9/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.5326 - val_loss: 0.4788\n",
      "Epoch 10/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4960 - val_loss: 0.4479\n",
      "Epoch 11/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.4693 - val_loss: 0.4260\n",
      "Epoch 12/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4503 - val_loss: 0.4102\n",
      "Epoch 13/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4365 - val_loss: 0.3978\n",
      "Epoch 14/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4266 - val_loss: 0.3892\n",
      "Epoch 15/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4193 - val_loss: 0.3823\n",
      "Epoch 16/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4139 - val_loss: 0.3774\n",
      "Epoch 17/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4098 - val_loss: 0.3738\n",
      "Epoch 18/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4066 - val_loss: 0.3708\n",
      "Epoch 19/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.4043 - val_loss: 0.3678\n",
      "Epoch 20/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.4025 - val_loss: 0.3655\n",
      "Epoch 21/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.4010 - val_loss: 0.3643\n",
      "Epoch 22/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3996 - val_loss: 0.3621\n",
      "Epoch 23/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3988 - val_loss: 0.3604\n",
      "Epoch 24/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3980 - val_loss: 0.3610\n",
      "Epoch 25/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3973 - val_loss: 0.3590\n",
      "Epoch 26/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3972 - val_loss: 0.3585\n",
      "Epoch 27/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3966 - val_loss: 0.3577\n",
      "Epoch 28/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3962 - val_loss: 0.3572\n",
      "Epoch 29/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3961 - val_loss: 0.3578\n",
      "Epoch 30/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3957 - val_loss: 0.3576\n",
      "Epoch 31/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3956 - val_loss: 0.3565\n",
      "Epoch 32/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3955 - val_loss: 0.3558\n",
      "Epoch 33/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3953 - val_loss: 0.3557\n",
      "Epoch 34/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3951 - val_loss: 0.3553\n",
      "Epoch 35/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3951 - val_loss: 0.3555\n",
      "Epoch 36/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3949 - val_loss: 0.3551\n",
      "Epoch 37/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3950 - val_loss: 0.3549\n",
      "Epoch 38/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3949 - val_loss: 0.3558\n",
      "Epoch 39/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3948 - val_loss: 0.3558\n",
      "Epoch 40/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3948 - val_loss: 0.3555\n",
      "Epoch 41/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3949 - val_loss: 0.3558\n",
      "Epoch 42/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3948 - val_loss: 0.3554\n",
      "Epoch 43/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3947 - val_loss: 0.3547\n",
      "Epoch 44/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3947 - val_loss: 0.3549\n",
      "Epoch 45/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3947 - val_loss: 0.3546\n",
      "Epoch 46/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3945 - val_loss: 0.3549\n",
      "Epoch 47/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3946 - val_loss: 0.3547\n",
      "Epoch 48/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3946 - val_loss: 0.3552\n",
      "Epoch 49/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3945 - val_loss: 0.3552\n",
      "Epoch 50/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3944 - val_loss: 0.3547\n",
      "Epoch 51/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3945 - val_loss: 0.3562\n",
      "Epoch 52/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3946 - val_loss: 0.3550\n",
      "Epoch 53/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3560\n",
      "Epoch 54/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3945 - val_loss: 0.3549\n",
      "Epoch 55/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3945 - val_loss: 0.3542\n",
      "Epoch 56/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3558\n",
      "Epoch 57/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3944 - val_loss: 0.3561\n",
      "Epoch 58/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3944 - val_loss: 0.3549\n",
      "Epoch 59/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3944 - val_loss: 0.3539\n",
      "Epoch 60/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3943 - val_loss: 0.3548\n",
      "Epoch 61/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 62/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3945 - val_loss: 0.3553\n",
      "Epoch 63/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3943 - val_loss: 0.3538\n",
      "Epoch 64/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3943 - val_loss: 0.3551\n",
      "Epoch 65/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3554\n",
      "Epoch 66/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 67/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3550\n",
      "Epoch 68/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 69/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 70/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3943 - val_loss: 0.3542\n",
      "Epoch 71/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3944 - val_loss: 0.3539\n",
      "Epoch 72/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 73/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 74/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 75/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 76/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 77/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 78/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3552\n",
      "Epoch 79/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 80/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3562\n",
      "Epoch 81/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 82/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 83/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 84/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 85/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 86/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 87/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 88/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 89/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 90/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3943 - val_loss: 0.3544\n",
      "Epoch 91/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 92/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 93/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 94/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 95/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 96/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 97/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 98/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 99/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 100/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 101/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 102/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 103/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 104/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 105/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3553\n",
      "Epoch 106/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 107/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 108/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 109/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3552\n",
      "Epoch 110/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3943 - val_loss: 0.3541\n",
      "Epoch 111/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 112/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3538\n",
      "Epoch 113/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 114/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 115/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3558\n",
      "Epoch 116/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 117/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 118/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 119/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 120/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 121/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 122/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 123/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 124/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 125/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 126/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 127/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 128/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 129/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 130/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 131/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3552\n",
      "Epoch 132/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 133/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 134/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 135/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3546\n",
      "Epoch 136/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 137/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 138/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 139/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 140/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3940 - val_loss: 0.3553\n",
      "Epoch 141/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 142/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3938 - val_loss: 0.3553\n",
      "Epoch 143/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 144/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 145/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 146/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 147/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 148/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 149/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 150/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 151/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 152/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3536\n",
      "Epoch 153/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 154/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 155/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 156/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3552\n",
      "Epoch 157/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3555\n",
      "Epoch 158/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 159/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 160/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 161/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 162/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3552\n",
      "Epoch 163/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 164/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 165/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3545\n",
      "Epoch 166/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 167/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 168/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 169/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 170/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 171/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 172/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 173/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 174/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3552\n",
      "Epoch 175/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 176/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 177/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 178/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 179/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 180/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3547\n",
      "Epoch 181/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3538\n",
      "Epoch 182/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 183/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3938 - val_loss: 0.3556\n",
      "Epoch 184/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 185/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 186/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3938 - val_loss: 0.3546\n",
      "Epoch 187/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3555\n",
      "Epoch 188/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 189/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3551\n",
      "Epoch 190/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 191/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 192/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 193/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 194/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3554\n",
      "Epoch 195/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3557\n",
      "Epoch 196/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3553\n",
      "Epoch 197/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 198/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3552\n",
      "Epoch 199/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 200/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3538\n",
      "Epoch 201/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 202/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3540\n",
      "Epoch 203/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 204/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3551\n",
      "Epoch 205/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3536\n",
      "Epoch 206/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 207/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 208/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 209/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 210/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3561\n",
      "Epoch 211/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 212/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 213/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 214/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3939 - val_loss: 0.3547\n",
      "Epoch 215/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3554\n",
      "Epoch 216/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 217/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 218/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3541\n",
      "Epoch 219/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 220/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 221/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 222/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 223/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 224/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 225/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 226/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 227/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 228/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3541\n",
      "Epoch 229/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 230/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 231/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 232/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 233/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 234/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 235/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 236/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3943 - val_loss: 0.3544\n",
      "Epoch 237/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 238/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3554\n",
      "Epoch 239/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 240/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 241/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 242/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3552\n",
      "Epoch 243/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 244/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 245/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3539\n",
      "Epoch 246/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 247/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 248/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 249/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 250/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 251/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 252/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 253/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 254/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 255/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3538\n",
      "Epoch 256/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3937 - val_loss: 0.3551\n",
      "Epoch 257/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 258/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3542\n",
      "Epoch 259/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 260/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 261/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3939 - val_loss: 0.3541\n",
      "Epoch 262/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 263/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 264/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 265/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 266/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 267/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 268/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 269/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 270/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 271/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 272/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3555\n",
      "Epoch 273/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 274/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3943 - val_loss: 0.3547\n",
      "Epoch 275/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 276/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 277/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 278/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 279/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 280/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 281/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3536\n",
      "Epoch 282/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 283/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 284/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 285/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 286/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 287/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 288/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 289/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 290/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 291/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3540\n",
      "Epoch 292/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3542\n",
      "Epoch 293/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 294/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 295/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3549\n",
      "Epoch 296/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 297/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3554\n",
      "Epoch 298/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 299/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3942 - val_loss: 0.3540\n",
      "Epoch 300/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3561\n",
      "Epoch 301/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 302/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 303/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 304/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3546\n",
      "Epoch 305/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 306/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 307/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 308/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3552\n",
      "Epoch 309/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 310/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 311/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3548\n",
      "Epoch 312/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3556\n",
      "Epoch 313/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3554\n",
      "Epoch 314/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 315/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 316/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 317/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 318/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 319/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 320/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 321/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 322/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3560\n",
      "Epoch 323/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 324/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3939 - val_loss: 0.3541\n",
      "Epoch 325/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3544\n",
      "Epoch 326/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 327/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 328/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 329/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 330/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3552\n",
      "Epoch 331/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 332/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3547\n",
      "Epoch 333/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 334/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 335/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 336/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 337/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 338/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 339/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 340/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3550\n",
      "Epoch 341/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 342/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3536\n",
      "Epoch 343/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 344/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 345/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 346/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 347/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 348/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 349/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3557\n",
      "Epoch 350/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 351/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 352/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 353/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 354/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 355/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 356/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 357/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 358/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 359/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3547\n",
      "Epoch 360/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3938 - val_loss: 0.3544\n",
      "Epoch 361/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 362/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3543\n",
      "Epoch 363/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3550\n",
      "Epoch 364/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 365/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3557\n",
      "Epoch 366/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3938 - val_loss: 0.3544\n",
      "Epoch 367/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 368/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 369/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 370/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 371/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 372/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3551\n",
      "Epoch 373/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 374/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 375/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 376/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3942 - val_loss: 0.3551\n",
      "Epoch 377/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 378/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 379/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3538\n",
      "Epoch 380/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 381/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3550\n",
      "Epoch 382/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 383/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 384/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3559\n",
      "Epoch 385/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 386/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 387/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3553\n",
      "Epoch 388/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3938 - val_loss: 0.3541\n",
      "Epoch 389/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 390/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3538\n",
      "Epoch 391/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 392/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 393/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 394/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 395/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 396/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3539\n",
      "Epoch 397/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 398/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 399/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3555\n",
      "Epoch 400/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3554\n",
      "Epoch 401/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 402/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 403/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 404/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3553\n",
      "Epoch 405/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3538\n",
      "Epoch 406/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3558\n",
      "Epoch 407/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 408/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 409/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 410/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3541\n",
      "Epoch 411/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 412/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3547\n",
      "Epoch 413/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 414/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 415/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 416/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3943 - val_loss: 0.3543\n",
      "Epoch 417/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3939 - val_loss: 0.3555\n",
      "Epoch 418/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 419/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 420/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 421/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 422/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 423/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 424/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3938 - val_loss: 0.3554\n",
      "Epoch 425/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3941 - val_loss: 0.3554\n",
      "Epoch 426/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 427/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3938 - val_loss: 0.3561\n",
      "Epoch 428/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3540\n",
      "Epoch 429/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 430/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 431/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 432/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3548\n",
      "Epoch 433/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3555\n",
      "Epoch 434/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3537\n",
      "Epoch 435/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 436/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 437/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 438/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3546\n",
      "Epoch 439/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3943 - val_loss: 0.3545\n",
      "Epoch 440/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 441/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3553\n",
      "Epoch 442/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 443/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3541\n",
      "Epoch 444/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3541\n",
      "Epoch 445/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3555\n",
      "Epoch 446/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 447/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 448/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3545\n",
      "Epoch 449/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 450/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 451/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 452/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3560\n",
      "Epoch 453/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3943 - val_loss: 0.3547\n",
      "Epoch 454/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3938 - val_loss: 0.3551\n",
      "Epoch 455/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 456/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 457/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3546\n",
      "Epoch 458/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 459/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 460/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 461/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 462/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 463/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 464/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 465/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 466/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 467/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 468/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 469/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 470/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3540\n",
      "Epoch 471/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 472/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 473/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3938 - val_loss: 0.3557\n",
      "Epoch 474/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3558\n",
      "Epoch 475/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3537\n",
      "Epoch 476/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 477/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 478/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 479/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 480/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 481/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 482/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3545\n",
      "Epoch 483/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 484/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 485/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3542\n",
      "Epoch 486/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 487/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 488/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 489/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 490/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3552\n",
      "Epoch 491/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3559\n",
      "Epoch 492/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 493/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3543\n",
      "Epoch 494/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3561\n",
      "Epoch 495/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3552\n",
      "Epoch 496/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3551\n",
      "Epoch 497/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3538\n",
      "Epoch 498/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3559\n",
      "Epoch 499/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 500/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 501/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 502/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3545\n",
      "Epoch 503/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 504/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3549\n",
      "Epoch 505/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3542\n",
      "Epoch 506/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 507/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3943 - val_loss: 0.3544\n",
      "Epoch 508/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 509/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 510/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3554\n",
      "Epoch 511/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3552\n",
      "Epoch 512/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3549\n",
      "Epoch 513/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3546\n",
      "Epoch 514/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3942 - val_loss: 0.3548\n",
      "Epoch 515/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 516/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 517/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3540\n",
      "Epoch 518/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3556\n",
      "Epoch 519/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3939 - val_loss: 0.3543\n",
      "Epoch 520/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 521/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3942 - val_loss: 0.3541\n",
      "Epoch 522/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 523/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 524/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 525/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 526/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 527/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3553\n",
      "Epoch 528/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3549\n",
      "Epoch 529/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 530/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3539\n",
      "Epoch 531/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 532/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 533/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 534/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3542\n",
      "Epoch 535/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3543\n",
      "Epoch 536/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 537/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 538/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3943 - val_loss: 0.3540\n",
      "Epoch 539/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 540/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3543\n",
      "Epoch 541/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 542/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3565\n",
      "Epoch 543/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3543\n",
      "Epoch 544/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3942 - val_loss: 0.3540\n",
      "Epoch 545/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3554\n",
      "Epoch 546/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3553\n",
      "Epoch 547/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3540\n",
      "Epoch 548/600\n",
      "14126/14126 [==============================] - 1s 51us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 549/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3549\n",
      "Epoch 550/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 551/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3550\n",
      "Epoch 552/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 553/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3940 - val_loss: 0.3537\n",
      "Epoch 554/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3940 - val_loss: 0.3553\n",
      "Epoch 555/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3940 - val_loss: 0.3545\n",
      "Epoch 556/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3944 - val_loss: 0.3549\n",
      "Epoch 557/600\n",
      "14126/14126 [==============================] - 1s 50us/step - loss: 0.3940 - val_loss: 0.3551\n",
      "Epoch 558/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 559/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3547\n",
      "Epoch 560/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3537\n",
      "Epoch 561/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 562/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3554\n",
      "Epoch 563/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3542\n",
      "Epoch 564/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 565/600\n",
      "14126/14126 [==============================] - 1s 49us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 566/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3541\n",
      "Epoch 567/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3942 - val_loss: 0.3553\n",
      "Epoch 568/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 569/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3551\n",
      "Epoch 570/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3561\n",
      "Epoch 571/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3549\n",
      "Epoch 572/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 573/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3545\n",
      "Epoch 574/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3539\n",
      "Epoch 575/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3546\n",
      "Epoch 576/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3546\n",
      "Epoch 577/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3939 - val_loss: 0.3553\n",
      "Epoch 578/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3943 - val_loss: 0.3545\n",
      "Epoch 579/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3559\n",
      "Epoch 580/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 581/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3535\n",
      "Epoch 582/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3939 - val_loss: 0.3543\n",
      "Epoch 583/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3551\n",
      "Epoch 584/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3544\n",
      "Epoch 585/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3540\n",
      "Epoch 586/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3548\n",
      "Epoch 587/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3939 - val_loss: 0.3550\n",
      "Epoch 588/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 589/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3940 - val_loss: 0.3550\n",
      "Epoch 590/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 591/600\n",
      "14126/14126 [==============================] - 1s 44us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 592/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3943 - val_loss: 0.3546\n",
      "Epoch 593/600\n",
      "14126/14126 [==============================] - 1s 47us/step - loss: 0.3940 - val_loss: 0.3544\n",
      "Epoch 594/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3941 - val_loss: 0.3560\n",
      "Epoch 595/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3942 - val_loss: 0.3547\n",
      "Epoch 596/600\n",
      "14126/14126 [==============================] - 1s 46us/step - loss: 0.3942 - val_loss: 0.3544\n",
      "Epoch 597/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3943 - val_loss: 0.3546\n",
      "Epoch 598/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3541\n",
      "Epoch 599/600\n",
      "14126/14126 [==============================] - 1s 45us/step - loss: 0.3941 - val_loss: 0.3543\n",
      "Epoch 600/600\n",
      "14126/14126 [==============================] - 1s 48us/step - loss: 0.3941 - val_loss: 0.3537\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAELCAYAAADTK53JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3xT9f0/8FdOLi2F0gu0ECgDxhew\n0gK1DERARwHpFGjZvghDdD+5KAMLojyEKcgol8eKrDoqggJz8/F1TtnkjgwRRSjqA6oiRe5KKfRG\nabn0QpImn98faULSnrZJSUnO2ev5ePTRJOck5/3OObz6yYeTRCOEECAiItWR/F0AERG1DAY8EZFK\nMeCJiFSKAU9EpFIMeCIilWLAExGpFAOeiEilGPDkEzt27MCvf/1rJCQkYOjQoZg+fTqOHj3qt3oW\nLlyIuLg4JCQkOH/GjRvn0X2zsrIwf/78Fq7Qc0lJSTh8+LC/yyAF0vm7AFK+d955B2+//TaWLl2K\noUOHQq/X4+DBg/j0008xYMCAeuvX1NRAp2v5Q2/atGmYN2+ezx9XCAEhBCSJ4yMKbDxC6Y7cvHkT\na9aswSuvvIKHH34YISEh0Ov1SEpKwoIFCwDYR8Rz5szB/Pnzcd9992HLli0wm81YsWIFhg4diqFD\nh2LFihUwm80AgLKyMjzzzDMYMGAABg4ciMmTJ8NmswEA3n77bQwbNgwJCQkYPXo0vvzyS69rvnTp\nEnr37o0tW7bgl7/8JQYNGoR169YBAL744gu89dZb+Pjjj91G/U888QRee+01TJo0Cf369UN+fj6K\ni4sxc+ZMDBw4EKNGjcKHH37o3Iaj5+eeew4JCQkYP348Tp06BQDYuHEj0tLS3Gpavnw5li9f7nUv\nH374IUaNGoWBAwdi5syZKC4uBmD/I7Ry5UoMHjwY9913H8aOHYszZ84AAA4cOIBHHnkECQkJGDZs\nGDZt2uT1dkkhBNEdOHDggIiNjRUWi6XBddasWSPuvfde8cknnwir1Sqqq6vF66+/LiZMmCBKS0vF\n1atXxcSJE8Vrr70mhBBi9erVYvHixcJsNguz2SyOHDkibDabOH/+vHjwwQdFUVGREEKI/Px8kZeX\nJ7vNBQsWiMzMTNll+fn5olevXuLll18W1dXV4uTJk6JPnz7i3LlzznpfeOEFt/tMmTJFPPTQQ+LM\nmTPCYrEIs9ksJk+eLJYsWSJu3bolfvjhBzFo0CBx+PBht54//vhjYTabxcaNG8Xw4cOF2WwWxcXF\nol+/fuL69etCCCEsFou4//77xfHjx2XrHT58uMjOzq53++HDh8XAgQNFbm6uMJlMIj09XUyePFkI\nIcQXX3whxo8fL65fvy5sNps4d+6cKC4uFkIIMWTIEHHkyBEhhBDXrl0Tubm5Dew5UjqO4OmOXLt2\nDREREU1OufTv3x8jR46EJEkIDg7Gjh07MHv2bLRr1w6RkZGYPXs2tm/fDgDQ6XS4cuUKCgoKoNfr\nMWDAAGg0Gmi1WpjNZpw/fx4WiwUxMTH42c9+1uA2//rXv2LAgAHOH8crCodnn30WwcHBuOeee3DP\nPfc4R9gNGT9+PHr27AmdTofS0lJ88803mD9/PoKCghAbG4sJEyZg27ZtzvX79OmD5ORk6PV6PPXU\nUzCbzTh27Biio6MxYMAA7NmzBwBw8OBBREREIC4urtHt17Vjxw785je/QZ8+fWAwGPD888/ju+++\nw6VLl6DT6VBZWYkff/wRQgj06NED0dHRzuf33LlzqKioQFhYGPr06ePVdkk5GPB0R8LDw1FeXo6a\nmppG1+vYsaPb9ZKSEnTq1Ml5vVOnTigpKQFgnzvv2rUrpk6dihEjRuDtt98GAHTt2hUvvfQSsrKy\n8MADD2DevHnOKQk5U6dOxdGjR50/GRkZbsvbt2/vvNyqVStUVVU12oPRaHSrPywsDG3atHHrwbUe\n154lSUKHDh2cPY4fP975B2379u1ISUlpdNtySkpK0LlzZ+f11q1bIzw8HMXFxRg8eDAef/xxpKen\nY/DgwVi8eDEqKioAAGvWrMGBAwcwfPhwTJkyBd9++63X2yZlYMDTHUlISIDBYMC+ffsaXU+j0bhd\nj46ORkFBgfN6YWGhc4TZpk0bLFy4EJ9++inWrVuHd955xznXPnbsWLz//vv47LPPoNFosHr1ah93\nVL9Wudujo6Nx/fp1Z2g6eujQoYPzelFRkfOyzWZDcXGxs8eRI0fi9OnTOHPmDD7//HOMHTvW6zqj\no6Nx+fJl5/Wqqipcu3bNWcOTTz6Jjz76CLt378aFCxewceNGAEDfvn2xbt06HD58GCNHjsRzzz3n\n9bZJGRjwdEdCQ0MxZ84cpKenY9++faiurobFYsGBAwewatWqBu/36KOPYt26dSgrK0NZWRnWrl3r\nDLnPPvsMeXl5EEIgNDQUWq0WGo0GP/74I7788kuYzWYYDAYEBQW1yJks7dq1w+XLl53/sSvHaDQi\nISEBmZmZMJlMOHXqFP71r3+5nYp54sQJ7N27FzU1Nfj73/8Og8GAfv36AQCCgoIwevRovPDCC4iP\nj3d7NSPHYrHAZDI5f2pqajBmzBh89NFHOHnyJMxmMzIzM9G3b1/ExMTg+++/x7Fjx2CxWNCqVSsY\nDAZIkgSz2Yzt27fj5s2b0Ov1aN26Nc8GUjGeJkl3bOrUqWjfvj3efPNNzJ8/H61bt0afPn0wc+bM\nBu8za9YsVFZWOgMxOTkZs2bNAgDk5eVh2bJlKCsrQ9u2bfHb3/4W999/P06dOoU///nPOH/+PPR6\nPRISEpCent7gNjZt2oR3333Xed1gMODrr79usp/k5GRs374dgwYNQkxMDLZs2SK7XmZmJpYsWYJh\nw4ahbdu2SEtLwwMPPOBcPmLECOzevRsLFixA165dkZWVBb1e71yempqKzZs3Y+XKlU3W9PTTT7td\nnzlzJubNm4e5c+ciLS0NN27cQEJCAl577TUAQGVlJVauXIlLly7BYDBg6NChmDZtGgBg27ZtWLZs\nGaxWK7p3745XX321ye2TMmmE4Bd+EPlaVlYW8vLyGp1CKigowK9+9StkZ2e7zeUT+QpfmxH5gc1m\nwzvvvINHHnmE4U4thlM0RHdZVVUVhgwZgk6dOjn/45OoJXCKhohIpThFQ0SkUgExRWOz2VBZWQm9\nXt/gOchEROROCAGLxdLg6a4BEfCVlZXOD0IiIiLv9OrVC6GhofVuD4iAd5wb3KtXLxgMBq/vn5ub\n6/XneAQq9hKY2EvgUUsfQPN7MZvNOHPmjNv7K1wFRMA7pmUc705sjubeLxCxl8DEXgKPWvoA7qyX\nhqa2+Z+sREQqxYAnIlIpBjwRkUp5NAc/a9YsXLp0CZIkISQkBIsXL0ZsbKzbOllZWfjHP/7h/DjU\n++67D0uWLPF9xURE5BGPAj4jI8N5Cs6+ffvw0ksvyX7CXmpqar1vzSEiIv/waIrG9fzKiooKvhmJ\niEgBPP4smpdffhnZ2dkQQmDjxo3o2bOn2/KsrCxs3rwZYWFhiIqKQlpaGhISEjwqwmQyITc31/vq\nARw61BZvvtkZ7757Ek18LSgRkSrFxcXJn2bp7bd0b9myRUyfPr3e7SUlJcJsNgshhDh06JC4//77\nRVlZmUePeevWLXH06FFx69Ytb8sRq1YJAQhx86bXdw1IR48e9XcJPsNeApNaelFLH0I0v5emstPr\ns2hSU1Px9ddfo7y83O32qKgo57uphgwZAqPRiLNnzzbvz5EXHLNF/ExMIiJ3TQZ8ZWUlCgsLndf3\n79+PsLAwhIeHu63n+m3yJ0+exOXLl9G9e3cflirP8fk6DHgiIndNzlpXV1dj7ty5qK6uhiRJCAsL\nw/r166HRaDBjxgzMmTMH8fHxyMzMxIkTJyBJEvR6PVatWoWoqKgWb8Axgm/k+5GJiP4rNRnw7du3\nx4cffii7bMOGDc7LGRkZvqvKC5yiISKSp/h3sjLgiYjkKT7gOQdPRCRP8QHPOXgiInmqCXiO4ImI\n3DHgiYhUSvEBzzl4IiJ5ig94zsETEclTTcBzBE9E5I4BT0SkUooPeM7BExHJU3zAcw6eiEieagKe\nI3giIncMeCIilVJ8wHMOnohInuIDnnPwRETyVBPwHMETEbljwBMRqZTiA55z8ERE8hQf8JyDJyKS\np5qA5wieiMgdA56ISKUUH/CcgycikqfzZKVZs2bh0qVLkCQJISEhWLx4MWJjY93WsVqtWL58OQ4e\nPAiNRoOnn34aEyZMaJGiXXEOnohInkcBn5GRgdDQUADAvn378NJLL2HLli1u6+zYsQMXL17E3r17\nce3aNaSmpmLw4MGIiYnxfdUuOEVDRCTPoykaR7gDQEVFBTSOVHWxe/duTJgwAZIkITIyEiNHjsSe\nPXt8V2kDGPBERPI8GsEDwMsvv4zs7GwIIbBx48Z6ywsLC9GpUyfndaPRiKKiIt9U2QjOwRMRyfM4\n4FesWAEA2Lp1K1atWoUNGzb4vJjc3Fyv73P+fBiA/8GJEydhs1X5vCZ/yMnJ8XcJPsNeApNaelFL\nH0DL9OJxwDukpqbilVdeQXl5OSIiIpy3G41GFBQUoG/fvgDqj+g9ERcXh6CgIK/uU1Bg/33PPbFI\nTPTqrgEpJycHiWpoBOwlUKmlF7X0ATS/F5PJ1OjAuMk5+MrKShQWFjqv79+/H2FhYQgPD3dbLzk5\nGZs3b4bNZkNZWRn27duH0aNHe12wtzgHT0Qkr8kRfHV1NebOnYvq6mpIkoSwsDCsX78eGo0GM2bM\nwJw5cxAfH4+UlBQcO3YMDz/8MABg9uzZ6NKlS4s3wDl4IiJ5TQZ8+/bt8eGHH8ouc52H12q1WLp0\nqe8q8xDPgycikqf4d7JyioaISB4DnohIpRQf8JyDJyKSp/iA5xw8EZE81QQ8R/BERO4Y8EREKqX4\ngOccPBGRPMUHPOfgiYjkqSbgOYInInLHgCciUinFBzzn4ImI5Ck+4DkHT0QkTzUBzxE8EZE7BjwR\nkUopPuA5B09EJE/xAc85eCIieaoJeI7giYjcMeCJiFRK8QHPOXgiInmKD3jOwRMRyVNNwHMET0Tk\njgFPRKRSig94zsETEcnTNbVCeXk5XnzxRVy8eBEGgwFdu3ZFeno6IiMj3dZbuHAhDh8+jIiICABA\ncnIyfv/737dM1S44B09EJK/JgNdoNJg+fToGDRoEAMjIyMDq1auxcuXKeus+/fTTmDJliu+rbLQ+\n+2+O4ImI3DU5RRMeHu4MdwDo378/CgoKWrQobzDgiYjkaYTwPBptNhumTp2KpKQkPPnkk27LFi5c\niCNHjiAkJARdunTBCy+8gB49enj0uCaTCbm5ud5VXuvChSD87//GYfnyH5GcXN6sxyAiUrK4uDgE\nBQXVu73JKRpXy5YtQ0hIiOw0zLx58xAVFQVJkrB161ZMnz4d+/btg1arveMiGxMaav/drdvPkZjo\n1V0DUk5ODhLV0AjYS6BSSy9q6QNofi9NDY49PosmIyMDeXl5eP311yFJ9e/WoUMH5+2pqamoqqpC\nUVGR1wV7i1M0RETyPAr4zMxM5ObmYu3atTAYDLLrFBcXOy8fPHgQkiShQ4cOvqmyEQx4IiJ5TU7R\nnD17Fm+99Ra6deuGSZMmAQBiYmKwdu1apKSk4O2330aHDh2wYMECXL16FRqNBm3atMG6deug03k1\nA9QsPA+eiEhekwncs2dPnD59WnbZtm3bnJf/9re/+awob/A8eCIieYp/JyunaIiI5DHgiYhUSvEB\nzzl4IiJ5ig94rfky/t+D73AOnoioDsUHfOsr/8Q7z0yFJKr8XQoRUUBRfMCjdg5eI6z+rYOIKMAo\nPuA1GsckPOdoiIhcKT7gURvwggFPRORG8QGvYcATEclSfMDfPk+SAU9E5ErxAe8cwfM8SSIiN6oJ\neIABT0TkSvEBzykaIiJ5ig94niZJRCRP+QEv8SwaIiI5ig94x8dJahjwRERuFB/wnKIhIpKn+IC/\n/U5Wfl4wEZErxQe8xDl4IiJZig94cIqGiEiW4gPecRaNhm90IiJyo5qA5wieiMidrqkVysvL8eKL\nL+LixYswGAzo2rUr0tPTERkZ6bZedXU1/vCHP+DEiRPQarVYsGABhg8f3mKFO/DTJImI5DU5gtdo\nNJg+fTr+85//YMeOHejSpQtWr15db71NmzahTZs2+OSTT7B+/XosWrQIlZWVLVK0e4EcwRMRyWky\n4MPDwzFo0CDn9f79+6OgoKDeeh9//DEmTpwIAOjWrRvi4uLwxRdf+LBUec4pGs7BExG58WoO3maz\n4f3330dSUlK9ZQUFBejcubPzutFoRFFR0Z1X2ATHFA3fyUpE5K7JOXhXy5YtQ0hICKZMmdIixeTm\n5np9n7YV59ETwJUrxcjJyfF9UX6glj4A9hKo1NKLWvoAWqYXjwM+IyMDeXl5WL9+vfPNRa46deqE\ny5cvO//ztbCw0G1qxxNxcXEICgry6j4ouAJcBtq3a4fExETv7huAcnJyVNEHwF4ClVp6UUsfQPN7\nMZlMjQ6MPZqiyczMRG5uLtauXQuDwSC7TnJyMj744AMAwIULF3D8+HEMGzbM64K9xi/8ICKS1WTA\nnz17Fm+99RZKSkowadIkpKSkYPbs2QCAlJQUFBcXAwCmTZuGGzduYNSoUXjmmWeQnp6ONm3atGz1\ngDPg+UYnIiJ3TU7R9OzZE6dPn5Zdtm3bNuflkJAQrFmzxneVeYzfyUpEJEfx72TlCJ6ISJ5qAp5z\n8ERE7pQf8OA7WYmI5Cg/4Gu/so8BT0TkTvkBD07REBHJUX7AOz5N0sav7CMicqWigOcInojIlXoC\nnnPwRERulB/wfKMTEZEs5Qc8p2iIiGSpJ+A5RUNE5Eb5Ac8pGiIiWcoPeH4nKxGRLNUEPKdoiIjc\nKT/gOUVDRCRL+QHPKRoiIlmqCXhO0RARuVNNwHMET0TkTvkBz8+DJyKSpfyA5wieiEiWagKec/BE\nRO6UH/CcoiEikqX8gOdX9hERyfIo4DMyMpCUlITevXvjzJkzsutkZWVh8ODBSElJQUpKCpYuXerT\nQhvGETwRkRydJyuNGDECTz75JB5//PFG10tNTcWCBQt8UpjHnHPw/Mo+IiJXHgX8gAEDWrqO5uNZ\nNEREsjwKeE/t2rULhw4dQlRUFNLS0pCQkODV/XNzc73epmStQAIAs/kWcnJyvL5/IFJLHwB7CVRq\n6UUtfQAt04vPAn7SpEmYOXMm9Ho9srOzMWvWLOzevRsREREeP0ZcXByCgoK827ClAjgH6PU6JCYm\nell14MnJyVFFHwB7CVRq6UUtfQDN78VkMjU6MPbZWTRRUVHQ6/UAgCFDhsBoNOLs2bO+eviGcYqG\niEiWzwK+uLjYefnkyZO4fPkyunfv7quHbxgDnohIlkdTNMuXL8fevXtRWlqKp556CuHh4di1axdm\nzJiBOXPmID4+HpmZmThx4gQkSYJer8eqVasQFRXV0vXD8TdKAwY8EZErjwJ+0aJFWLRoUb3bN2zY\n4LyckZHhu6q8wRE8EZEsFbyT1dECA56IyJXyAx78qAIiIjnKD3iNBjah4Rw8EVEdyg94AEJIHMET\nEdWhjoCHBM7BExG5U0XA24TEKRoiojpUEfAcwRMR1aeOgBcaSLD6uwwiooCiioC3Ch00GgY8EZEr\nlQS8HnrJ7O8yiIgCinoCXsuAJyJypY6At9kDnt/aR0R0mzoCXuhh0Jlh5TQ8EZGTKgK+hgFPRFSP\nKgLeKgzQay0MeCIiF6oIeBs4giciqksVAe+Yg7dY/F0JEVHgUEXA2zT2gL91y9+VEBEFDlUEvNDo\nYNCaUV3t70qIiAKHKgIetSN4BjwR0W3qCHiJAU9EVJcqAl5IOgY8EVEdTQZ8RkYGkpKS0Lt3b5w5\nc0Z2HavViqVLl2LkyJEYNWoUNm/e7PNCG6NhwBMR1dNkwI8YMQLvvfceOnfu3OA6O3bswMWLF7F3\n71588MEHyMrKwqVLl3xaaKO0PIuGiKiuJgN+wIABMBqNja6ze/duTJgwAZIkITIyEiNHjsSePXt8\nVmRTNFotR/BERHX4ZA6+sLAQnTp1cl43Go0oKiryxUN7RNLyNEkiorp0/i7AVW5ubrPuF6Gzz8Gf\n/uEicnKu+Liquy8nJ8ffJfgMewlMaulFLX0ALdOLTwLeaDSioKAAffv2BVB/RO+puLg4BAUFeX2/\nCwU66HU1iIqKQWLiz7y+fyDJyclBYmKiv8vwCfYSmNTSi1r6AJrfi8lkanRg7JMpmuTkZGzevBk2\nmw1lZWXYt28fRo8e7YuH9ohWb/87Za7mh9EQETk0GfDLly/Hgw8+iKKiIjz11FN49NFHAQAzZszA\n8ePHAQApKSmIiYnBww8/jMceewyzZ89Gly5dWrZyFxqtHgBgNvFr+4iIHJqcolm0aBEWLVpU7/YN\nGzY4L2u1WixdutS3lXnBKrUCANjMFQBC/VYHEVEgUcU7Wa2SPdRt1df8XAkRUeBQScC3AQBYqhjw\nREQO6gh4be0I/tZ1P1dCRBQ41BHwtVM0sHAET0TkoIqAr6kdwUs1DHgiIgdVBLxjDl4vGPBERA6q\nCHihCYJV6BFiuM7PoyEiqqWKgIdGAxPaISr0CkpK/F0MEVFgUEfAA7DojOgYXoTCQn9XQkQUGFQT\n8AjuCGN4IQoK/F0IEVFgUE3AG9oaYQwv5AieiKiWagI+OMKIDmHFKCyw+rsUIqKAoJqA14QYoZVs\nuFmq/C/8ICLyBdUEPFrZvze25ibnaIiIADUFfLA94EUVA56ICFBTwNeO4PU1d+/LvomIApmKAr4j\nAKCNrhBmfrETEZGKAl4bjFsiEp0jLiM/39/FEBH5n3oCHkCNoRu6R/+Es2f9XQkRkf+pKuB1ET3Q\nI/o8zpzxdyVERP6nqoAPatcD3aN/wrkzNf4uhYjI71QV8JrQHtBra3CtgJPwRESqCniE9gAAWK+f\n93MhRET+51HA//TTT5g4cSJGjx6NiRMn4sKFC/XWycrKwuDBg5GSkoKUlBQsXbrU17U2rY094EM1\n53Hr1t3fPBFRINF5stKSJUswefJkpKSkYNu2bXjllVfw7rvv1lsvNTUVCxYs8HmRHguJgRVB6BF9\nDsePA7/4hf9KISLytyZH8FevXsUPP/yAMWPGAADGjBmDH374AWVlZS1enNc0EqyteyO+y3F89ZW/\niyEi8q8mR/CFhYXo0KEDtFotAECr1SI6OhqFhYWIjIx0W3fXrl04dOgQoqKikJaWhoSEBK+Kyc3N\n9Wp9Vzk5OQCArrpu+EWPw3j1gzI88MBPzX48f3L0ogbsJTCppRe19AG0TC8eTdF4YtKkSZg5cyb0\nej2ys7Mxa9Ys7N69GxERER4/RlxcHIKCgrzedk5ODhITE+1XziYD17ej5MfriItLRDMezq/celE4\n9hKY1NKLWvoAmt+LyWRqdGDc5BSN0WhEcXExrFb7F2lYrVaUlJTAaDS6rRcVFQW9Xg8AGDJkCIxG\nI8764y2l0Q8BAAZ23Y+PPrr7myciChRNBny7du0QGxuLnTt3AgB27tyJ2NjYetMzxcXFzssnT57E\n5cuX0b17dx+X64G2sRDBRkwctgvLloFn0xDRfy2Ppmj++Mc/YuHChXjzzTfRtm1bZGRkAABmzJiB\nOXPmID4+HpmZmThx4gQkSYJer8eqVasQFRXVosXL0mig6ToJI2+tRVlBER57rCP+7/+Atm3vfilE\nRP7kUcD36NEDmzdvrnf7hg0bnJcdoR8Qev4e0tk3sP+1RegzdSN69gQmTQIGDABiY4GoKCA8HGjV\nCtBqAUkCNBp/F01E5Fs++0/WgNK2J9ArDfeeeg3n9v0Sz6+ZgrfeAtasafguGs3tsNdq3S+73iaE\n/acxDf2xaOyPiGOZxRKP2v/K8Pg+3mznbjKb42Ew+LsK35DrpanjoCV4us3GjgGLJR66Bv7lN/T4\njW1Xkuw/3tTnzbquvbhebuz4knvs5v67bWnBwUB6eiu0xP8XqzPgAaBvOlD+LXoUP4FtL3yEmqwZ\nOHdzOE6dDUZ5OVBeDlRXA1YrYLPZf7tervvb8aPR3P6R05x/IK7LSkuvo337KK/u4+l27rbS0hto\n3769v8vwiYZ68UcoNLVN12NAiPrrO3rxdoAgd7tjwOP4t9HYenK3e9uLq6aOL2+253jshupsScHB\nQEREy3xAonoDXtcaeGgXcHIVcOp16C5twT0A7mllBDp3B3p1AwzhgGQADJGArhWg0QPWagAC0GgB\njQTAMX/j+IHLEVD3Ng+XN3SbRsJPFy6ge0wUYDPba5Nqhyg2k30dSW+vDxpA1NQemeL2b7nL+lCX\nnnT2xxBWoKYCqKkCdCH2x7OaAG2QS02Ox4H9eZB09ufIXFa7nmSvUxdS+3hVgM0CaIMBbTAuXMxH\nty6d7Ndt5trHlalPo7ldl0YLCFvtMpt909Zbtc+HHpCCANjstepa1z5u3ee09rpjmbaVfb8Km32f\nQ7I/BuCSGq7pIeotu5B3Ad26dpNd5vxdU2WvNaidfVs2i30fSYbbvTnWbWh/OR/b5XaNFrBW2fvV\n6OzHpUZr356owe1zJWy3nzNdG/t6wmZfx1GLRou8SyXoGtPBfps+tPbYcxyLdWpw1gHUr9eL9Rrr\nUyMBrTrbLzvqdB4DDRzTkh55+ZfRtWvXOvujgX0J2I9dx/MkBd9+voQNzmPKZgb0be3Hs6S3709t\nK8Byo/YY1dofp6bSvj+cz7ujNpe6JS2ga1v7b9dxjFTfPhYknf23rg1yijuhJag34AF7aMcvAe5d\nCBR9ApR9C1T+BFReAEoP23eazWwPugDRHQBU8rWy3QD2EoC6AkBxU2sFPrX0AQChMW8AGOTzx1V3\nwDtog4DOY+w/cqxm+19Zm8X+11qjcf/L7virDLiPUuqOWOourzfKQRO32XD8+PeI7z/IXrPVDAiL\nfV1tkP23YyTvGPW6jb5cLms0cI7sLNdvj4yFBbDV2EdNulD7H8GaKvsybZB9FAPIvAqpHZHaLPZR\nsM1cO0ox2P9AarT2kbw22D7qsd7C8e+/QXzfBPt9JAOcrzzq1gzhMsq01nnlBHufjtG61VQ7+tHb\nt6sNvr0f6z73jlc/1mr7yOVnPTwAAAbvSURBVF+jBczldbZft1e4X65d9v3xXPSNj5dd5uxDF2Lf\npqm0drRnsI/irObaUWmN+/py+8152WU9x6skx6sQYbX/aFvZtyGscLwCtD93ArBU2PeZ66s2jQ4Q\nNfj+26/QN74voG9jH4m6jazlXnXK1FTv1aeX67luw2YGqovstTvq1Eh1HqP2eHD8G9BI+P7Yd+jb\nt5/LLpPZf66XbWbcfkV0q/a50dS+sqp9VaSRapfVPo+Swf4c6dvefu5tZkBbuz/cniPJ/bfNZL+v\n81WxsN/PcZwLa+0rPD1unipFS/jvCPimaA32nwBgNpQ5v0DcZwxhjS/X3+k5pHXmQWtfupr1RUBI\nzB0+dmOacRpucPNO3bXoS4HWP/NsZZ2H63kt3PNVDQ2/g9yi7wi07tLkeneNNrhZx6BFXwCEtMzU\nxt3XMgGvrs+DJyIiJwY8EZFKMeCJiFSKAU9EpFIMeCIilWLAExGpVECcJilqz182m81NrNkwk8nU\n9EoKwV4CE3sJPGrpA2heL47MFG7v5L1NIxpachfdvHkTZ86c8XcZRESK1KtXL4SGhta7PSAC3maz\nobKyEnq9HppA+ShEIqIAJ4SAxWJB69atIUn1Z9wDIuCJiMj3+J+sREQqxYAnIlIpBjwRkUox4ImI\nVIoBT0SkUgx4IiKVYsATEamU4gP+p59+wsSJEzF69GhMnDgRFy5c8HdJDcrIyEBSUhJ69+7t9s7d\nxnoIxP7Ky8sxY8YMjB49GmPHjsWzzz6LsrIyAMB3332HcePGYfTo0Zg6dSquXr3qvF9jy/xp1qxZ\nGDduHFJTUzF58mScPHkSgPL2i6s33njD7ThT4n5JSkpCcnIyUlJSkJKSgoMHDwJQXi8mkwlLlizB\nww8/jLFjx2Lx4sUA7tLxJRTuiSeeEFu3bhVCCLF161bxxBNP+Lmihh05ckQUFBSI4cOHi9OnTztv\nb6yHQOyvvLxcfPXVV87rf/rTn8Qf/vAHYbVaxciRI8WRI0eEEEKsXbtWLFy4UAghGl3mbzdu3HBe\n/uSTT0RqaqoQQnn7xSE3N1dMmzbNeZwpdb/U/XciROP1Bmovy5YtEytWrBA2m00IIcSVK1eEEHfn\n+FJ0wJeWlorExERRU1MjhBCipqZGJCYmiqtXr/q5ssa5HriN9aCU/vbs2SN+97vfiWPHjolHH33U\nefvVq1dF//79hRCi0WWBZMuWLWL8+PGK3S8mk0k89thjIj8/33mcKXW/yAW80nqpqKgQiYmJoqKi\nwu32u3V8BcSnSTZXYWEhOnToAK1WCwDQarWIjo5GYWEhIiMj/VydZxrrQQgR8P3ZbDa8//77SEpK\nQmFhITp1uv0lyJGRkbDZbLh27Vqjy8LDvfgy6Rby8ssvIzs7G0IIbNy4UbH75S9/+QvGjRuHmJjb\nX3au5P0yf/58CCGQmJiI559/XnG95OfnIzw8HG+88Qa+/vprtG7dGnPnzkVwcPBdOb4UPwdP/rVs\n2TKEhIRgypQp/i7ljqxYsQKff/455s2bh1WrVvm7nGb59ttvkZubi8mTJ/u7FJ947733sH37dvz7\n3/+GEALp6en+LslrVqsV+fn5uPfee/HRRx9h/vz5SEtLQ1VV1V3ZvqID3mg0ori4GFarFYD9ySwp\nKYHRaPRzZZ5rrIdA7y8jIwN5eXl4/fXXIUkSjEYjCgoKnMvLysogSRLCw8MbXRZIUlNT8fXXX6Nj\nx46K2y9HjhzB+fPnMWLECCQlJaGoqAjTpk1DXl6eIveL4/k0GAyYPHkyvvnmG8UdY0ajETqdDmPG\njAEA9OvXDxEREQgODr4rx5eiA75du3aIjY3Fzp07AQA7d+5EbGys318me6OxHgK5v8zMTOTm5mLt\n2rUwGAwAgLi4ONy6dQtHjx4FAPzzn/9EcnJyk8v8qbKyEoWFhc7r+/fvR1hYmCL3y9NPP41Dhw5h\n//792L9/Pzp27IhNmzZh+vTpitsvVVVVuHnzJgD7R+Lu3r0bsbGxijvGIiMjMWjQIGRnZwOwnx1z\n9epVdOvW7a4cX4r/uODz589j4cKFuHHjBtq2bYuMjAz8/Oc/93dZspYvX469e/eitLQUERERCA8P\nx65duxrtIRD7O3v2LMaMGYNu3bohODgYABATE4O1a9fim2++wZIlS2AymdC5c2e8+uqraN++PQA0\nusxfSktLMWvWLFRXV0OSJISFhWHBggXo06eP4vZLXUlJSVi/fj169eqluP2Sn5+PtLQ0WK1W2Gw2\n9OjRA4sWLUJ0dLQie3nppZdw7do16HQ6PPfcc3jooYfuyvGl+IAnIiJ5ip6iISKihjHgiYhUigFP\nRKRSDHgiIpViwBMRqRQDnohIpRjwREQqxYAnIlKp/w/GbFNvC7c5xwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(8,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2941859,
     "status": "ok",
     "timestamp": 1572240451910,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "768fuI51JAkL",
    "outputId": "224349f5-2d7d-420a-950f-767bd7568f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14126 samples, validate on 6054 samples\n",
      "Epoch 1/600\n",
      "14126/14126 [==============================] - 1s 101us/step - loss: 0.6139 - val_loss: 0.3837\n",
      "Epoch 2/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3911 - val_loss: 0.3453\n",
      "Epoch 3/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3738 - val_loss: 0.3281\n",
      "Epoch 4/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3676 - val_loss: 0.3281\n",
      "Epoch 5/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3568 - val_loss: 0.3250\n",
      "Epoch 6/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3509 - val_loss: 0.3479\n",
      "Epoch 7/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3456 - val_loss: 0.3052\n",
      "Epoch 8/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3389 - val_loss: 0.3076\n",
      "Epoch 9/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3379 - val_loss: 0.2964\n",
      "Epoch 10/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3323 - val_loss: 0.2993\n",
      "Epoch 11/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3289 - val_loss: 0.3070\n",
      "Epoch 12/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3279 - val_loss: 0.2933\n",
      "Epoch 13/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3244 - val_loss: 0.2880\n",
      "Epoch 14/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3200 - val_loss: 0.2886\n",
      "Epoch 15/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3184 - val_loss: 0.2911\n",
      "Epoch 16/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3182 - val_loss: 0.2857\n",
      "Epoch 17/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3147 - val_loss: 0.2865\n",
      "Epoch 18/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3117 - val_loss: 0.2914\n",
      "Epoch 19/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3078 - val_loss: 0.3059\n",
      "Epoch 20/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3084 - val_loss: 0.2924\n",
      "Epoch 21/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3092 - val_loss: 0.2881\n",
      "Epoch 22/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.3064 - val_loss: 0.2835\n",
      "Epoch 23/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3032 - val_loss: 0.2745\n",
      "Epoch 24/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.3021 - val_loss: 0.2937\n",
      "Epoch 25/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3027 - val_loss: 0.2847\n",
      "Epoch 26/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2989 - val_loss: 0.2764\n",
      "Epoch 27/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3017 - val_loss: 0.2733\n",
      "Epoch 28/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2950 - val_loss: 0.2748\n",
      "Epoch 29/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2958 - val_loss: 0.2701\n",
      "Epoch 30/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2965 - val_loss: 0.2783\n",
      "Epoch 31/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2937 - val_loss: 0.2693\n",
      "Epoch 32/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2896 - val_loss: 0.2700\n",
      "Epoch 33/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2914 - val_loss: 0.2789\n",
      "Epoch 34/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2917 - val_loss: 0.2768\n",
      "Epoch 35/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2912 - val_loss: 0.2683\n",
      "Epoch 36/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2887 - val_loss: 0.2765\n",
      "Epoch 37/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.2884 - val_loss: 0.2819\n",
      "Epoch 38/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2889 - val_loss: 0.2720\n",
      "Epoch 39/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2887 - val_loss: 0.2751\n",
      "Epoch 40/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2869 - val_loss: 0.2679\n",
      "Epoch 41/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2850 - val_loss: 0.2881\n",
      "Epoch 42/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2871 - val_loss: 0.2754\n",
      "Epoch 43/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2853 - val_loss: 0.2808\n",
      "Epoch 44/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2823 - val_loss: 0.2720\n",
      "Epoch 45/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2842 - val_loss: 0.2712\n",
      "Epoch 46/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2838 - val_loss: 0.2718\n",
      "Epoch 47/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2827 - val_loss: 0.2706\n",
      "Epoch 48/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2822 - val_loss: 0.2657\n",
      "Epoch 49/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2825 - val_loss: 0.2659\n",
      "Epoch 50/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2827 - val_loss: 0.2638\n",
      "Epoch 51/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2820 - val_loss: 0.2849\n",
      "Epoch 52/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2810 - val_loss: 0.2648\n",
      "Epoch 53/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2802 - val_loss: 0.2617\n",
      "Epoch 54/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2798 - val_loss: 0.2658\n",
      "Epoch 55/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2803 - val_loss: 0.2618\n",
      "Epoch 56/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2806 - val_loss: 0.2583\n",
      "Epoch 57/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2786 - val_loss: 0.2582\n",
      "Epoch 58/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2797 - val_loss: 0.2585\n",
      "Epoch 59/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2775 - val_loss: 0.2580\n",
      "Epoch 60/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2789 - val_loss: 0.2577\n",
      "Epoch 61/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2769 - val_loss: 0.2641\n",
      "Epoch 62/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2775 - val_loss: 0.2662\n",
      "Epoch 63/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2782 - val_loss: 0.2724\n",
      "Epoch 64/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2756 - val_loss: 0.2624\n",
      "Epoch 65/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2741 - val_loss: 0.2723\n",
      "Epoch 66/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2769 - val_loss: 0.2640\n",
      "Epoch 67/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2759 - val_loss: 0.2562\n",
      "Epoch 68/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2741 - val_loss: 0.2580\n",
      "Epoch 69/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2744 - val_loss: 0.2578\n",
      "Epoch 70/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2727 - val_loss: 0.2540\n",
      "Epoch 71/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2727 - val_loss: 0.2622\n",
      "Epoch 72/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2733 - val_loss: 0.2636\n",
      "Epoch 73/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2731 - val_loss: 0.2563\n",
      "Epoch 74/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2700 - val_loss: 0.2560\n",
      "Epoch 75/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2713 - val_loss: 0.2533\n",
      "Epoch 76/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2701 - val_loss: 0.2626\n",
      "Epoch 77/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2684 - val_loss: 0.2609\n",
      "Epoch 78/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2700 - val_loss: 0.2776\n",
      "Epoch 79/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2698 - val_loss: 0.2651\n",
      "Epoch 80/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2694 - val_loss: 0.2569\n",
      "Epoch 81/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2704 - val_loss: 0.2521\n",
      "Epoch 82/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2685 - val_loss: 0.2523\n",
      "Epoch 83/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2668 - val_loss: 0.2607\n",
      "Epoch 84/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2687 - val_loss: 0.2558\n",
      "Epoch 85/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2666 - val_loss: 0.2552\n",
      "Epoch 86/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2671 - val_loss: 0.2654\n",
      "Epoch 87/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2678 - val_loss: 0.2487\n",
      "Epoch 88/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2651 - val_loss: 0.2506\n",
      "Epoch 89/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2664 - val_loss: 0.2559\n",
      "Epoch 90/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2661 - val_loss: 0.2527\n",
      "Epoch 91/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2663 - val_loss: 0.2594\n",
      "Epoch 92/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2651 - val_loss: 0.2610\n",
      "Epoch 93/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2651 - val_loss: 0.2645\n",
      "Epoch 94/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2660 - val_loss: 0.2526\n",
      "Epoch 95/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2634 - val_loss: 0.2570\n",
      "Epoch 96/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2630 - val_loss: 0.2485\n",
      "Epoch 97/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2630 - val_loss: 0.2620\n",
      "Epoch 98/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2660 - val_loss: 0.2520\n",
      "Epoch 99/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2645 - val_loss: 0.2587\n",
      "Epoch 100/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2619 - val_loss: 0.2590\n",
      "Epoch 101/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2650 - val_loss: 0.2512\n",
      "Epoch 102/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2630 - val_loss: 0.2533\n",
      "Epoch 103/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2620 - val_loss: 0.2615\n",
      "Epoch 104/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2629 - val_loss: 0.2539\n",
      "Epoch 105/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2621 - val_loss: 0.2570\n",
      "Epoch 106/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2630 - val_loss: 0.2613\n",
      "Epoch 107/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2619 - val_loss: 0.2526\n",
      "Epoch 108/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2633 - val_loss: 0.2640\n",
      "Epoch 109/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2620 - val_loss: 0.2568\n",
      "Epoch 110/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2622 - val_loss: 0.2675\n",
      "Epoch 111/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2598 - val_loss: 0.2693\n",
      "Epoch 112/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2611 - val_loss: 0.2611\n",
      "Epoch 113/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2613 - val_loss: 0.2623\n",
      "Epoch 114/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2606 - val_loss: 0.2582\n",
      "Epoch 115/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2608 - val_loss: 0.2526\n",
      "Epoch 116/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2603 - val_loss: 0.2539\n",
      "Epoch 117/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2613 - val_loss: 0.2493\n",
      "Epoch 118/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2590 - val_loss: 0.2696\n",
      "Epoch 119/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2597 - val_loss: 0.2741\n",
      "Epoch 120/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2612 - val_loss: 0.2517\n",
      "Epoch 121/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2616 - val_loss: 0.2589\n",
      "Epoch 122/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2599 - val_loss: 0.2515\n",
      "Epoch 123/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2599 - val_loss: 0.2543\n",
      "Epoch 124/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2601 - val_loss: 0.2504\n",
      "Epoch 125/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2602 - val_loss: 0.2583\n",
      "Epoch 126/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2608 - val_loss: 0.2502\n",
      "Epoch 127/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2587 - val_loss: 0.2590\n",
      "Epoch 128/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2592 - val_loss: 0.2515\n",
      "Epoch 129/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2601 - val_loss: 0.2509\n",
      "Epoch 130/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2613 - val_loss: 0.2602\n",
      "Epoch 131/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2582 - val_loss: 0.2641\n",
      "Epoch 132/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2578 - val_loss: 0.2487\n",
      "Epoch 133/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2573 - val_loss: 0.2541\n",
      "Epoch 134/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2575 - val_loss: 0.2488\n",
      "Epoch 135/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2577 - val_loss: 0.2462\n",
      "Epoch 136/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2565 - val_loss: 0.2533\n",
      "Epoch 137/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2590 - val_loss: 0.2500\n",
      "Epoch 138/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2590 - val_loss: 0.2501\n",
      "Epoch 139/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2579 - val_loss: 0.2479\n",
      "Epoch 140/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2563 - val_loss: 0.2563\n",
      "Epoch 141/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2586 - val_loss: 0.2529\n",
      "Epoch 142/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2567 - val_loss: 0.2526\n",
      "Epoch 143/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2570 - val_loss: 0.2570\n",
      "Epoch 144/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2580 - val_loss: 0.2463\n",
      "Epoch 145/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2575 - val_loss: 0.2542\n",
      "Epoch 146/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2575 - val_loss: 0.2519\n",
      "Epoch 147/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2573 - val_loss: 0.2540\n",
      "Epoch 148/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2558 - val_loss: 0.2490\n",
      "Epoch 149/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2555 - val_loss: 0.2572\n",
      "Epoch 150/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2565 - val_loss: 0.2721\n",
      "Epoch 151/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2574 - val_loss: 0.2485\n",
      "Epoch 152/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2561 - val_loss: 0.2503\n",
      "Epoch 153/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2566 - val_loss: 0.2554\n",
      "Epoch 154/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2547 - val_loss: 0.2516\n",
      "Epoch 155/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2569 - val_loss: 0.2484\n",
      "Epoch 156/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2543 - val_loss: 0.2471\n",
      "Epoch 157/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2550 - val_loss: 0.2558\n",
      "Epoch 158/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2554 - val_loss: 0.2459\n",
      "Epoch 159/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2548 - val_loss: 0.2539\n",
      "Epoch 160/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2544 - val_loss: 0.2473\n",
      "Epoch 161/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2543 - val_loss: 0.2489\n",
      "Epoch 162/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2541 - val_loss: 0.2456\n",
      "Epoch 163/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2565 - val_loss: 0.2592\n",
      "Epoch 164/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2557 - val_loss: 0.2571\n",
      "Epoch 165/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2555 - val_loss: 0.2568\n",
      "Epoch 166/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2538 - val_loss: 0.2423\n",
      "Epoch 167/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2559 - val_loss: 0.2453\n",
      "Epoch 168/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2537 - val_loss: 0.2534\n",
      "Epoch 169/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2549 - val_loss: 0.2480\n",
      "Epoch 170/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2552 - val_loss: 0.2430\n",
      "Epoch 171/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2535 - val_loss: 0.2474\n",
      "Epoch 172/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2551 - val_loss: 0.2507\n",
      "Epoch 173/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2556 - val_loss: 0.2429\n",
      "Epoch 174/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2530 - val_loss: 0.2459\n",
      "Epoch 175/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2551 - val_loss: 0.2559\n",
      "Epoch 176/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2513 - val_loss: 0.2536\n",
      "Epoch 177/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2545 - val_loss: 0.2455\n",
      "Epoch 178/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2535 - val_loss: 0.2485\n",
      "Epoch 179/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2519 - val_loss: 0.2746\n",
      "Epoch 180/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2538 - val_loss: 0.2467\n",
      "Epoch 181/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2537 - val_loss: 0.2489\n",
      "Epoch 182/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2535 - val_loss: 0.2524\n",
      "Epoch 183/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2534 - val_loss: 0.2494\n",
      "Epoch 184/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2531 - val_loss: 0.2528\n",
      "Epoch 185/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2547 - val_loss: 0.2470\n",
      "Epoch 186/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2540 - val_loss: 0.2450\n",
      "Epoch 187/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2525 - val_loss: 0.2477\n",
      "Epoch 188/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2524 - val_loss: 0.2463\n",
      "Epoch 189/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2547 - val_loss: 0.2513\n",
      "Epoch 190/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2533 - val_loss: 0.2548\n",
      "Epoch 191/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2529 - val_loss: 0.2540\n",
      "Epoch 192/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2523 - val_loss: 0.2488\n",
      "Epoch 193/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2522 - val_loss: 0.2482\n",
      "Epoch 194/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2524 - val_loss: 0.2605\n",
      "Epoch 195/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2535 - val_loss: 0.2520\n",
      "Epoch 196/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2536 - val_loss: 0.2478\n",
      "Epoch 197/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2525 - val_loss: 0.2569\n",
      "Epoch 198/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2535 - val_loss: 0.2514\n",
      "Epoch 199/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2539 - val_loss: 0.2493\n",
      "Epoch 200/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2528 - val_loss: 0.2531\n",
      "Epoch 201/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2523 - val_loss: 0.2499\n",
      "Epoch 202/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2512 - val_loss: 0.2458\n",
      "Epoch 203/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2532 - val_loss: 0.2512\n",
      "Epoch 204/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2524 - val_loss: 0.2751\n",
      "Epoch 205/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2541 - val_loss: 0.2504\n",
      "Epoch 206/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2512 - val_loss: 0.2568\n",
      "Epoch 207/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2516 - val_loss: 0.2546\n",
      "Epoch 208/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2510 - val_loss: 0.2499\n",
      "Epoch 209/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2545 - val_loss: 0.2640\n",
      "Epoch 210/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2535 - val_loss: 0.2459\n",
      "Epoch 211/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2526 - val_loss: 0.2510\n",
      "Epoch 212/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2533 - val_loss: 0.2523\n",
      "Epoch 213/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2528 - val_loss: 0.2543\n",
      "Epoch 214/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2526 - val_loss: 0.2520\n",
      "Epoch 215/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2517 - val_loss: 0.2455\n",
      "Epoch 216/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2524 - val_loss: 0.2492\n",
      "Epoch 217/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2520 - val_loss: 0.2516\n",
      "Epoch 218/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2512 - val_loss: 0.2457\n",
      "Epoch 219/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2535 - val_loss: 0.2706\n",
      "Epoch 220/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2511 - val_loss: 0.2472\n",
      "Epoch 221/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2497 - val_loss: 0.2477\n",
      "Epoch 222/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2511 - val_loss: 0.2462\n",
      "Epoch 223/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2511 - val_loss: 0.2458\n",
      "Epoch 224/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2508 - val_loss: 0.2425\n",
      "Epoch 225/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2519 - val_loss: 0.2423\n",
      "Epoch 226/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2506 - val_loss: 0.2472\n",
      "Epoch 227/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2507 - val_loss: 0.2490\n",
      "Epoch 228/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2509 - val_loss: 0.2626\n",
      "Epoch 229/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2502 - val_loss: 0.2432\n",
      "Epoch 230/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2515 - val_loss: 0.2442\n",
      "Epoch 231/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2519 - val_loss: 0.2464\n",
      "Epoch 232/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.2513 - val_loss: 0.2423\n",
      "Epoch 233/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2506 - val_loss: 0.2530\n",
      "Epoch 234/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2505 - val_loss: 0.2511\n",
      "Epoch 235/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2495 - val_loss: 0.2441\n",
      "Epoch 236/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2502 - val_loss: 0.2499\n",
      "Epoch 237/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2503 - val_loss: 0.2446\n",
      "Epoch 238/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2498 - val_loss: 0.2447\n",
      "Epoch 239/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2503 - val_loss: 0.2502\n",
      "Epoch 240/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2523 - val_loss: 0.2684\n",
      "Epoch 241/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2498 - val_loss: 0.2623\n",
      "Epoch 242/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2509 - val_loss: 0.2511\n",
      "Epoch 243/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2509 - val_loss: 0.2460\n",
      "Epoch 244/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2492 - val_loss: 0.2438\n",
      "Epoch 245/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2499 - val_loss: 0.2443\n",
      "Epoch 246/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2495 - val_loss: 0.2492\n",
      "Epoch 247/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2521 - val_loss: 0.2478\n",
      "Epoch 248/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2510 - val_loss: 0.2469\n",
      "Epoch 249/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2501 - val_loss: 0.2536\n",
      "Epoch 250/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2499 - val_loss: 0.2430\n",
      "Epoch 251/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2495 - val_loss: 0.2505\n",
      "Epoch 252/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2502 - val_loss: 0.2482\n",
      "Epoch 253/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2488 - val_loss: 0.2470\n",
      "Epoch 254/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2503 - val_loss: 0.2510\n",
      "Epoch 255/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2500 - val_loss: 0.2522\n",
      "Epoch 256/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2506 - val_loss: 0.2484\n",
      "Epoch 257/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2481 - val_loss: 0.2450\n",
      "Epoch 258/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2499 - val_loss: 0.2429\n",
      "Epoch 259/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2488 - val_loss: 0.2490\n",
      "Epoch 260/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2505 - val_loss: 0.2595\n",
      "Epoch 261/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2501 - val_loss: 0.2450\n",
      "Epoch 262/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2496 - val_loss: 0.2404\n",
      "Epoch 263/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2500 - val_loss: 0.2552\n",
      "Epoch 264/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2490 - val_loss: 0.2621\n",
      "Epoch 265/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2494 - val_loss: 0.2450\n",
      "Epoch 266/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2498 - val_loss: 0.2438\n",
      "Epoch 267/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2490 - val_loss: 0.2503\n",
      "Epoch 268/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2495 - val_loss: 0.2460\n",
      "Epoch 269/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2492 - val_loss: 0.2444\n",
      "Epoch 270/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2492 - val_loss: 0.2508\n",
      "Epoch 271/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2498 - val_loss: 0.2498\n",
      "Epoch 272/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2486 - val_loss: 0.2439\n",
      "Epoch 273/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2483 - val_loss: 0.2466\n",
      "Epoch 274/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2486 - val_loss: 0.2426\n",
      "Epoch 275/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2490 - val_loss: 0.2504\n",
      "Epoch 276/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2491 - val_loss: 0.2433\n",
      "Epoch 277/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2502 - val_loss: 0.2423\n",
      "Epoch 278/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2491 - val_loss: 0.2472\n",
      "Epoch 279/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2478 - val_loss: 0.2446\n",
      "Epoch 280/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2492 - val_loss: 0.2506\n",
      "Epoch 281/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2497 - val_loss: 0.2458\n",
      "Epoch 282/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2489 - val_loss: 0.2575\n",
      "Epoch 283/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2479 - val_loss: 0.2474\n",
      "Epoch 284/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2505 - val_loss: 0.2426\n",
      "Epoch 285/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2500 - val_loss: 0.2488\n",
      "Epoch 286/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2485 - val_loss: 0.2552\n",
      "Epoch 287/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2496 - val_loss: 0.2450\n",
      "Epoch 288/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2498 - val_loss: 0.2527\n",
      "Epoch 289/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2479 - val_loss: 0.2516\n",
      "Epoch 290/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2490 - val_loss: 0.2462\n",
      "Epoch 291/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2477 - val_loss: 0.2464\n",
      "Epoch 292/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2478 - val_loss: 0.2474\n",
      "Epoch 293/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2486 - val_loss: 0.2495\n",
      "Epoch 294/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2499 - val_loss: 0.2511\n",
      "Epoch 295/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2469 - val_loss: 0.2450\n",
      "Epoch 296/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2480 - val_loss: 0.2450\n",
      "Epoch 297/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2467 - val_loss: 0.2471\n",
      "Epoch 298/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2485 - val_loss: 0.2412\n",
      "Epoch 299/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2483 - val_loss: 0.2460\n",
      "Epoch 300/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2478 - val_loss: 0.2472\n",
      "Epoch 301/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2477 - val_loss: 0.2445\n",
      "Epoch 302/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2489 - val_loss: 0.2414\n",
      "Epoch 303/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2481 - val_loss: 0.2497\n",
      "Epoch 304/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2490 - val_loss: 0.2414\n",
      "Epoch 305/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2468 - val_loss: 0.2443\n",
      "Epoch 306/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2474 - val_loss: 0.2395\n",
      "Epoch 307/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2473 - val_loss: 0.2450\n",
      "Epoch 308/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2472 - val_loss: 0.2388\n",
      "Epoch 309/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2488 - val_loss: 0.2563\n",
      "Epoch 310/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2483 - val_loss: 0.2467\n",
      "Epoch 311/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2473 - val_loss: 0.2450\n",
      "Epoch 312/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2477 - val_loss: 0.2426\n",
      "Epoch 313/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2479 - val_loss: 0.2484\n",
      "Epoch 314/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2461 - val_loss: 0.2407\n",
      "Epoch 315/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2475 - val_loss: 0.2495\n",
      "Epoch 316/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2466 - val_loss: 0.2471\n",
      "Epoch 317/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2465 - val_loss: 0.2392\n",
      "Epoch 318/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2480 - val_loss: 0.2484\n",
      "Epoch 319/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2471 - val_loss: 0.2483\n",
      "Epoch 320/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2472 - val_loss: 0.2436\n",
      "Epoch 321/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2461 - val_loss: 0.2465\n",
      "Epoch 322/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2474 - val_loss: 0.2479\n",
      "Epoch 323/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2486 - val_loss: 0.2488\n",
      "Epoch 324/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2468 - val_loss: 0.2446\n",
      "Epoch 325/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2481 - val_loss: 0.2416\n",
      "Epoch 326/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2476 - val_loss: 0.2664\n",
      "Epoch 327/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2473 - val_loss: 0.2445\n",
      "Epoch 328/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2459 - val_loss: 0.2419\n",
      "Epoch 329/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2487 - val_loss: 0.2427\n",
      "Epoch 330/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2486 - val_loss: 0.2435\n",
      "Epoch 331/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2470 - val_loss: 0.2473\n",
      "Epoch 332/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2469 - val_loss: 0.2509\n",
      "Epoch 333/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2472 - val_loss: 0.2555\n",
      "Epoch 334/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2483 - val_loss: 0.2450\n",
      "Epoch 335/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2467 - val_loss: 0.2472\n",
      "Epoch 336/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2473 - val_loss: 0.2418\n",
      "Epoch 337/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2468 - val_loss: 0.2408\n",
      "Epoch 338/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2458 - val_loss: 0.2452\n",
      "Epoch 339/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2468 - val_loss: 0.2576\n",
      "Epoch 340/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2457 - val_loss: 0.2456\n",
      "Epoch 341/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2476 - val_loss: 0.2453\n",
      "Epoch 342/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2473 - val_loss: 0.2427\n",
      "Epoch 343/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2466 - val_loss: 0.2468\n",
      "Epoch 344/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2458 - val_loss: 0.2513\n",
      "Epoch 345/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2485 - val_loss: 0.2464\n",
      "Epoch 346/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2476 - val_loss: 0.2420\n",
      "Epoch 347/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2449 - val_loss: 0.2471\n",
      "Epoch 348/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2453 - val_loss: 0.2467\n",
      "Epoch 349/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2451 - val_loss: 0.2542\n",
      "Epoch 350/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2472 - val_loss: 0.2489\n",
      "Epoch 351/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2471 - val_loss: 0.2424\n",
      "Epoch 352/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2471 - val_loss: 0.2485\n",
      "Epoch 353/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2468 - val_loss: 0.2424\n",
      "Epoch 354/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2466 - val_loss: 0.2487\n",
      "Epoch 355/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2457 - val_loss: 0.2473\n",
      "Epoch 356/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2468 - val_loss: 0.2478\n",
      "Epoch 357/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2453 - val_loss: 0.2428\n",
      "Epoch 358/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2450 - val_loss: 0.2471\n",
      "Epoch 359/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2462 - val_loss: 0.2515\n",
      "Epoch 360/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2448 - val_loss: 0.2444\n",
      "Epoch 361/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2480 - val_loss: 0.2436\n",
      "Epoch 362/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2464 - val_loss: 0.2393\n",
      "Epoch 363/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2474 - val_loss: 0.2526\n",
      "Epoch 364/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2456 - val_loss: 0.2544\n",
      "Epoch 365/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2472 - val_loss: 0.2412\n",
      "Epoch 366/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2450 - val_loss: 0.2434\n",
      "Epoch 367/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2464 - val_loss: 0.2491\n",
      "Epoch 368/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2456 - val_loss: 0.2488\n",
      "Epoch 369/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2475 - val_loss: 0.2411\n",
      "Epoch 370/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2453 - val_loss: 0.2545\n",
      "Epoch 371/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2448 - val_loss: 0.2401\n",
      "Epoch 372/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2459 - val_loss: 0.2430\n",
      "Epoch 373/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2476 - val_loss: 0.2507\n",
      "Epoch 374/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2452 - val_loss: 0.2456\n",
      "Epoch 375/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2464 - val_loss: 0.2482\n",
      "Epoch 376/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2455 - val_loss: 0.2418\n",
      "Epoch 377/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2464 - val_loss: 0.2458\n",
      "Epoch 378/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2472 - val_loss: 0.2542\n",
      "Epoch 379/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2466 - val_loss: 0.2508\n",
      "Epoch 380/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2461 - val_loss: 0.2541\n",
      "Epoch 381/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2456 - val_loss: 0.2550\n",
      "Epoch 382/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2458 - val_loss: 0.2390\n",
      "Epoch 383/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.2458 - val_loss: 0.2693\n",
      "Epoch 384/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2445 - val_loss: 0.2473\n",
      "Epoch 385/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2451 - val_loss: 0.2431\n",
      "Epoch 386/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2468 - val_loss: 0.2576\n",
      "Epoch 387/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2463 - val_loss: 0.2445\n",
      "Epoch 388/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2449 - val_loss: 0.2493\n",
      "Epoch 389/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2458 - val_loss: 0.2422\n",
      "Epoch 390/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2461 - val_loss: 0.2435\n",
      "Epoch 391/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2445 - val_loss: 0.2472\n",
      "Epoch 392/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2456 - val_loss: 0.2411\n",
      "Epoch 393/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2459 - val_loss: 0.2474\n",
      "Epoch 394/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2461 - val_loss: 0.2462\n",
      "Epoch 395/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2455 - val_loss: 0.2570\n",
      "Epoch 396/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2449 - val_loss: 0.2463\n",
      "Epoch 397/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2454 - val_loss: 0.2464\n",
      "Epoch 398/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2438 - val_loss: 0.2416\n",
      "Epoch 399/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2442 - val_loss: 0.2413\n",
      "Epoch 400/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2441 - val_loss: 0.2432\n",
      "Epoch 401/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2451 - val_loss: 0.2415\n",
      "Epoch 402/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2445 - val_loss: 0.2443\n",
      "Epoch 403/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2448 - val_loss: 0.2397\n",
      "Epoch 404/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2450 - val_loss: 0.2448\n",
      "Epoch 405/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2444 - val_loss: 0.2412\n",
      "Epoch 406/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2449 - val_loss: 0.2419\n",
      "Epoch 407/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2449 - val_loss: 0.2404\n",
      "Epoch 408/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2440 - val_loss: 0.2425\n",
      "Epoch 409/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2459 - val_loss: 0.2493\n",
      "Epoch 410/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2454 - val_loss: 0.2496\n",
      "Epoch 411/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2449 - val_loss: 0.2511\n",
      "Epoch 412/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2435 - val_loss: 0.2478\n",
      "Epoch 413/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2458 - val_loss: 0.2427\n",
      "Epoch 414/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2455 - val_loss: 0.2413\n",
      "Epoch 415/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2443 - val_loss: 0.2509\n",
      "Epoch 416/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2451 - val_loss: 0.2452\n",
      "Epoch 417/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2443 - val_loss: 0.2481\n",
      "Epoch 418/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2437 - val_loss: 0.2467\n",
      "Epoch 419/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2431 - val_loss: 0.2404\n",
      "Epoch 420/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2443 - val_loss: 0.2534\n",
      "Epoch 421/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2456 - val_loss: 0.2418\n",
      "Epoch 422/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2445 - val_loss: 0.2462\n",
      "Epoch 423/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2445 - val_loss: 0.2400\n",
      "Epoch 424/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2438 - val_loss: 0.2485\n",
      "Epoch 425/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2436 - val_loss: 0.2443\n",
      "Epoch 426/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2442 - val_loss: 0.2409\n",
      "Epoch 427/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2439 - val_loss: 0.2510\n",
      "Epoch 428/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2435 - val_loss: 0.2457\n",
      "Epoch 429/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2449 - val_loss: 0.2428\n",
      "Epoch 430/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2443 - val_loss: 0.2478\n",
      "Epoch 431/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2443 - val_loss: 0.2432\n",
      "Epoch 432/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2445 - val_loss: 0.2538\n",
      "Epoch 433/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2438 - val_loss: 0.2441\n",
      "Epoch 434/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2435 - val_loss: 0.2404\n",
      "Epoch 435/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2433 - val_loss: 0.2452\n",
      "Epoch 436/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2447 - val_loss: 0.2493\n",
      "Epoch 437/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2434 - val_loss: 0.2464\n",
      "Epoch 438/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2451 - val_loss: 0.2520\n",
      "Epoch 439/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2455 - val_loss: 0.2495\n",
      "Epoch 440/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2435 - val_loss: 0.2472\n",
      "Epoch 441/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2446 - val_loss: 0.2432\n",
      "Epoch 442/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2440 - val_loss: 0.2435\n",
      "Epoch 443/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2439 - val_loss: 0.2431\n",
      "Epoch 444/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2431 - val_loss: 0.2487\n",
      "Epoch 445/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2446 - val_loss: 0.2466\n",
      "Epoch 446/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2435 - val_loss: 0.2521\n",
      "Epoch 447/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2434 - val_loss: 0.2451\n",
      "Epoch 448/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2434 - val_loss: 0.2482\n",
      "Epoch 449/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2440 - val_loss: 0.2455\n",
      "Epoch 450/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2450 - val_loss: 0.2434\n",
      "Epoch 451/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2435 - val_loss: 0.2465\n",
      "Epoch 452/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2442 - val_loss: 0.2511\n",
      "Epoch 453/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2435 - val_loss: 0.2405\n",
      "Epoch 454/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2447 - val_loss: 0.2441\n",
      "Epoch 455/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2446 - val_loss: 0.2577\n",
      "Epoch 456/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2450 - val_loss: 0.2441\n",
      "Epoch 457/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2454 - val_loss: 0.2426\n",
      "Epoch 458/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2440 - val_loss: 0.2579\n",
      "Epoch 459/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2426 - val_loss: 0.2577\n",
      "Epoch 460/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2453 - val_loss: 0.2483\n",
      "Epoch 461/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2433 - val_loss: 0.2447\n",
      "Epoch 462/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2435 - val_loss: 0.2451\n",
      "Epoch 463/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2444 - val_loss: 0.2427\n",
      "Epoch 464/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2425 - val_loss: 0.2422\n",
      "Epoch 465/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2445 - val_loss: 0.2447\n",
      "Epoch 466/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2442 - val_loss: 0.2517\n",
      "Epoch 467/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2452 - val_loss: 0.2407\n",
      "Epoch 468/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2438 - val_loss: 0.2432\n",
      "Epoch 469/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2441 - val_loss: 0.2397\n",
      "Epoch 470/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2432 - val_loss: 0.2536\n",
      "Epoch 471/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2426 - val_loss: 0.2517\n",
      "Epoch 472/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2441 - val_loss: 0.2435\n",
      "Epoch 473/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2434 - val_loss: 0.2480\n",
      "Epoch 474/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2426 - val_loss: 0.2392\n",
      "Epoch 475/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2429 - val_loss: 0.2460\n",
      "Epoch 476/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2431 - val_loss: 0.2401\n",
      "Epoch 477/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2439 - val_loss: 0.2410\n",
      "Epoch 478/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2441 - val_loss: 0.2443\n",
      "Epoch 479/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2436 - val_loss: 0.2427\n",
      "Epoch 480/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2436 - val_loss: 0.2499\n",
      "Epoch 481/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2431 - val_loss: 0.2418\n",
      "Epoch 482/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2441 - val_loss: 0.2464\n",
      "Epoch 483/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2447 - val_loss: 0.2386\n",
      "Epoch 484/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2450 - val_loss: 0.2401\n",
      "Epoch 485/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2428 - val_loss: 0.2441\n",
      "Epoch 486/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2427 - val_loss: 0.2449\n",
      "Epoch 487/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2412 - val_loss: 0.2517\n",
      "Epoch 488/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2436 - val_loss: 0.2464\n",
      "Epoch 489/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2441 - val_loss: 0.2456\n",
      "Epoch 490/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2426 - val_loss: 0.2461\n",
      "Epoch 491/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2427 - val_loss: 0.2479\n",
      "Epoch 492/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2439 - val_loss: 0.2485\n",
      "Epoch 493/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2428 - val_loss: 0.2369\n",
      "Epoch 494/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2440 - val_loss: 0.2421\n",
      "Epoch 495/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2425 - val_loss: 0.2489\n",
      "Epoch 496/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2435 - val_loss: 0.2424\n",
      "Epoch 497/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2440 - val_loss: 0.2468\n",
      "Epoch 498/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2424 - val_loss: 0.2464\n",
      "Epoch 499/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2425 - val_loss: 0.2430\n",
      "Epoch 500/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2434 - val_loss: 0.2461\n",
      "Epoch 501/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2435 - val_loss: 0.2461\n",
      "Epoch 502/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2431 - val_loss: 0.2516\n",
      "Epoch 503/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2422 - val_loss: 0.2429\n",
      "Epoch 504/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2434 - val_loss: 0.2501\n",
      "Epoch 505/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2430 - val_loss: 0.2422\n",
      "Epoch 506/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2429 - val_loss: 0.2447\n",
      "Epoch 507/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2441 - val_loss: 0.2417\n",
      "Epoch 508/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2427 - val_loss: 0.2445\n",
      "Epoch 509/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2448 - val_loss: 0.2443\n",
      "Epoch 510/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2433 - val_loss: 0.2442\n",
      "Epoch 511/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2427 - val_loss: 0.2418\n",
      "Epoch 512/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2425 - val_loss: 0.2530\n",
      "Epoch 513/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2431 - val_loss: 0.2438\n",
      "Epoch 514/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2423 - val_loss: 0.2465\n",
      "Epoch 515/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2429 - val_loss: 0.2429\n",
      "Epoch 516/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2432 - val_loss: 0.2465\n",
      "Epoch 517/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2435 - val_loss: 0.2431\n",
      "Epoch 518/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2429 - val_loss: 0.2454\n",
      "Epoch 519/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2425 - val_loss: 0.2493\n",
      "Epoch 520/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2422 - val_loss: 0.2429\n",
      "Epoch 521/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2436 - val_loss: 0.2458\n",
      "Epoch 522/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2432 - val_loss: 0.2445\n",
      "Epoch 523/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2427 - val_loss: 0.2445\n",
      "Epoch 524/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2431 - val_loss: 0.2423\n",
      "Epoch 525/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2429 - val_loss: 0.2516\n",
      "Epoch 526/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2432 - val_loss: 0.2497\n",
      "Epoch 527/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2442 - val_loss: 0.2433\n",
      "Epoch 528/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2418 - val_loss: 0.2415\n",
      "Epoch 529/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2436 - val_loss: 0.2371\n",
      "Epoch 530/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2425 - val_loss: 0.2422\n",
      "Epoch 531/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2418 - val_loss: 0.2459\n",
      "Epoch 532/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2428 - val_loss: 0.2457\n",
      "Epoch 533/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2422 - val_loss: 0.2481\n",
      "Epoch 534/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2430 - val_loss: 0.2480\n",
      "Epoch 535/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2429 - val_loss: 0.2400\n",
      "Epoch 536/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2422 - val_loss: 0.2490\n",
      "Epoch 537/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2434 - val_loss: 0.2570\n",
      "Epoch 538/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2436 - val_loss: 0.2412\n",
      "Epoch 539/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2425 - val_loss: 0.2420\n",
      "Epoch 540/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2436 - val_loss: 0.2406\n",
      "Epoch 541/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2430 - val_loss: 0.2478\n",
      "Epoch 542/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2430 - val_loss: 0.2439\n",
      "Epoch 543/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2431 - val_loss: 0.2433\n",
      "Epoch 544/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2417 - val_loss: 0.2425\n",
      "Epoch 545/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2418 - val_loss: 0.2405\n",
      "Epoch 546/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2432 - val_loss: 0.2475\n",
      "Epoch 547/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2430 - val_loss: 0.2466\n",
      "Epoch 548/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2423 - val_loss: 0.2470\n",
      "Epoch 549/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2422 - val_loss: 0.2523\n",
      "Epoch 550/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2439 - val_loss: 0.2463\n",
      "Epoch 551/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2419 - val_loss: 0.2465\n",
      "Epoch 552/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2423 - val_loss: 0.2462\n",
      "Epoch 553/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2419 - val_loss: 0.2433\n",
      "Epoch 554/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2417 - val_loss: 0.2529\n",
      "Epoch 555/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2421 - val_loss: 0.2477\n",
      "Epoch 556/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2431 - val_loss: 0.2455\n",
      "Epoch 557/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2427 - val_loss: 0.2431\n",
      "Epoch 558/600\n",
      "14126/14126 [==============================] - 1s 52us/step - loss: 0.2420 - val_loss: 0.2434\n",
      "Epoch 559/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2423 - val_loss: 0.2470\n",
      "Epoch 560/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2437 - val_loss: 0.2413\n",
      "Epoch 561/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2410 - val_loss: 0.2468\n",
      "Epoch 562/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2424 - val_loss: 0.2474\n",
      "Epoch 563/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2420 - val_loss: 0.2423\n",
      "Epoch 564/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2420 - val_loss: 0.2409\n",
      "Epoch 565/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2414 - val_loss: 0.2490\n",
      "Epoch 566/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2409 - val_loss: 0.2483\n",
      "Epoch 567/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2413 - val_loss: 0.2434\n",
      "Epoch 568/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2421 - val_loss: 0.2481\n",
      "Epoch 569/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2440 - val_loss: 0.2420\n",
      "Epoch 570/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2412 - val_loss: 0.2645\n",
      "Epoch 571/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2427 - val_loss: 0.2427\n",
      "Epoch 572/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2411 - val_loss: 0.2515\n",
      "Epoch 573/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2428 - val_loss: 0.2422\n",
      "Epoch 574/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2425 - val_loss: 0.2428\n",
      "Epoch 575/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2407 - val_loss: 0.2411\n",
      "Epoch 576/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2427 - val_loss: 0.2445\n",
      "Epoch 577/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2414 - val_loss: 0.2422\n",
      "Epoch 578/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2433 - val_loss: 0.2412\n",
      "Epoch 579/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2429 - val_loss: 0.2449\n",
      "Epoch 580/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2420 - val_loss: 0.2466\n",
      "Epoch 581/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2416 - val_loss: 0.2475\n",
      "Epoch 582/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2420 - val_loss: 0.2499\n",
      "Epoch 583/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2411 - val_loss: 0.2502\n",
      "Epoch 584/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2425 - val_loss: 0.2416\n",
      "Epoch 585/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2414 - val_loss: 0.2438\n",
      "Epoch 586/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2421 - val_loss: 0.2440\n",
      "Epoch 587/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2430 - val_loss: 0.2407\n",
      "Epoch 588/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2416 - val_loss: 0.2402\n",
      "Epoch 589/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2422 - val_loss: 0.2523\n",
      "Epoch 590/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2419 - val_loss: 0.2439\n",
      "Epoch 591/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2423 - val_loss: 0.2445\n",
      "Epoch 592/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2413 - val_loss: 0.2440\n",
      "Epoch 593/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2419 - val_loss: 0.2502\n",
      "Epoch 594/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2423 - val_loss: 0.2443\n",
      "Epoch 595/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2416 - val_loss: 0.2475\n",
      "Epoch 596/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2411 - val_loss: 0.2553\n",
      "Epoch 597/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2421 - val_loss: 0.2431\n",
      "Epoch 598/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2421 - val_loss: 0.2512\n",
      "Epoch 599/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2410 - val_loss: 0.2422\n",
      "Epoch 600/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2427 - val_loss: 0.2464\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAELCAYAAAAx94awAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(8,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 with sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 485433,
     "status": "ok",
     "timestamp": 1572241882849,
     "user": {
      "displayName": "santhosh ketha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCHZoev1Su01c3YSBYi638WPWZaktYecdyTuh8ZMQ=s64",
      "userId": "06963422152123978380"
     },
     "user_tz": -330
    },
    "id": "TBtEiQbAGALS",
    "outputId": "98608592-b505-45b0-92c0-90d41f6beb6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14126 samples, validate on 6054 samples\n",
      "Epoch 1/600\n",
      "14126/14126 [==============================] - 1s 106us/step - loss: 0.5973 - val_loss: 0.4182\n",
      "Epoch 2/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.4635 - val_loss: 0.4164\n",
      "Epoch 3/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.4523 - val_loss: 0.4164\n",
      "Epoch 4/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.4488 - val_loss: 0.3998\n",
      "Epoch 5/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.4432 - val_loss: 0.3943\n",
      "Epoch 6/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.4419 - val_loss: 0.3918\n",
      "Epoch 7/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.4326 - val_loss: 0.3975\n",
      "Epoch 8/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.4295 - val_loss: 0.3819\n",
      "Epoch 9/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.4245 - val_loss: 0.3837\n",
      "Epoch 10/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.4219 - val_loss: 0.4059\n",
      "Epoch 11/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.4176 - val_loss: 0.3748\n",
      "Epoch 12/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.4120 - val_loss: 0.3755\n",
      "Epoch 13/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.4138 - val_loss: 0.3686\n",
      "Epoch 14/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.4084 - val_loss: 0.3746\n",
      "Epoch 15/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.4044 - val_loss: 0.3651\n",
      "Epoch 16/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.4050 - val_loss: 0.3675\n",
      "Epoch 17/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3968 - val_loss: 0.3583\n",
      "Epoch 18/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3895 - val_loss: 0.3544\n",
      "Epoch 19/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3872 - val_loss: 0.3521\n",
      "Epoch 20/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.3821 - val_loss: 0.3411\n",
      "Epoch 21/600\n",
      "14126/14126 [==============================] - 1s 65us/step - loss: 0.3796 - val_loss: 0.3423\n",
      "Epoch 22/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3767 - val_loss: 0.3458\n",
      "Epoch 23/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.3739 - val_loss: 0.3682\n",
      "Epoch 24/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.3705 - val_loss: 0.3327\n",
      "Epoch 25/600\n",
      "14126/14126 [==============================] - 1s 69us/step - loss: 0.3690 - val_loss: 0.3323\n",
      "Epoch 26/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3637 - val_loss: 0.3335\n",
      "Epoch 27/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3613 - val_loss: 0.3249\n",
      "Epoch 28/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3595 - val_loss: 0.3461\n",
      "Epoch 29/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.3625 - val_loss: 0.3423\n",
      "Epoch 30/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3575 - val_loss: 0.3213\n",
      "Epoch 31/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3550 - val_loss: 0.3270\n",
      "Epoch 32/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3553 - val_loss: 0.3230\n",
      "Epoch 33/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3571 - val_loss: 0.3187\n",
      "Epoch 34/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3501 - val_loss: 0.3198\n",
      "Epoch 35/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.3501 - val_loss: 0.3603\n",
      "Epoch 36/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3538 - val_loss: 0.3526\n",
      "Epoch 37/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3502 - val_loss: 0.3236\n",
      "Epoch 38/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.3495 - val_loss: 0.3234\n",
      "Epoch 39/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3467 - val_loss: 0.3183\n",
      "Epoch 40/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3466 - val_loss: 0.3176\n",
      "Epoch 41/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3457 - val_loss: 0.3311\n",
      "Epoch 42/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3450 - val_loss: 0.3190\n",
      "Epoch 43/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3427 - val_loss: 0.3157\n",
      "Epoch 44/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3438 - val_loss: 0.3132\n",
      "Epoch 45/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3438 - val_loss: 0.3149\n",
      "Epoch 46/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3436 - val_loss: 0.3109\n",
      "Epoch 47/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3414 - val_loss: 0.3121\n",
      "Epoch 48/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3405 - val_loss: 0.3128\n",
      "Epoch 49/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3380 - val_loss: 0.3227\n",
      "Epoch 50/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3394 - val_loss: 0.3069\n",
      "Epoch 51/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3400 - val_loss: 0.3220\n",
      "Epoch 52/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3367 - val_loss: 0.3043\n",
      "Epoch 53/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3366 - val_loss: 0.3133\n",
      "Epoch 54/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3341 - val_loss: 0.3040\n",
      "Epoch 55/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3327 - val_loss: 0.3111\n",
      "Epoch 56/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3312 - val_loss: 0.3103\n",
      "Epoch 57/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3302 - val_loss: 0.3113\n",
      "Epoch 58/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3312 - val_loss: 0.3029\n",
      "Epoch 59/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3284 - val_loss: 0.3009\n",
      "Epoch 60/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3276 - val_loss: 0.3056\n",
      "Epoch 61/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3282 - val_loss: 0.3118\n",
      "Epoch 62/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3259 - val_loss: 0.2961\n",
      "Epoch 63/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3252 - val_loss: 0.2958\n",
      "Epoch 64/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3238 - val_loss: 0.2996\n",
      "Epoch 65/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3243 - val_loss: 0.2989\n",
      "Epoch 66/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3229 - val_loss: 0.2988\n",
      "Epoch 67/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3210 - val_loss: 0.3031\n",
      "Epoch 68/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3202 - val_loss: 0.2987\n",
      "Epoch 69/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3202 - val_loss: 0.2912\n",
      "Epoch 70/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3200 - val_loss: 0.2955\n",
      "Epoch 71/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3185 - val_loss: 0.2931\n",
      "Epoch 72/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.3168 - val_loss: 0.2956\n",
      "Epoch 73/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3179 - val_loss: 0.2877\n",
      "Epoch 74/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3153 - val_loss: 0.2879\n",
      "Epoch 75/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3157 - val_loss: 0.2916\n",
      "Epoch 76/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3172 - val_loss: 0.2923\n",
      "Epoch 77/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3149 - val_loss: 0.3112\n",
      "Epoch 78/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3160 - val_loss: 0.2905\n",
      "Epoch 79/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.3124 - val_loss: 0.3015\n",
      "Epoch 80/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.3120 - val_loss: 0.2869\n",
      "Epoch 81/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.3141 - val_loss: 0.3021\n",
      "Epoch 82/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3121 - val_loss: 0.2831\n",
      "Epoch 83/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3110 - val_loss: 0.2963\n",
      "Epoch 84/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3097 - val_loss: 0.2909\n",
      "Epoch 85/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3103 - val_loss: 0.2877\n",
      "Epoch 86/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3122 - val_loss: 0.2842\n",
      "Epoch 87/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3080 - val_loss: 0.2872\n",
      "Epoch 88/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3092 - val_loss: 0.2832\n",
      "Epoch 89/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3096 - val_loss: 0.2833\n",
      "Epoch 90/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3094 - val_loss: 0.2805\n",
      "Epoch 91/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3084 - val_loss: 0.2853\n",
      "Epoch 92/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3092 - val_loss: 0.2828\n",
      "Epoch 93/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3078 - val_loss: 0.2867\n",
      "Epoch 94/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3080 - val_loss: 0.2825\n",
      "Epoch 95/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3068 - val_loss: 0.2918\n",
      "Epoch 96/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3074 - val_loss: 0.2806\n",
      "Epoch 97/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3058 - val_loss: 0.2851\n",
      "Epoch 98/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3064 - val_loss: 0.2823\n",
      "Epoch 99/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3066 - val_loss: 0.2783\n",
      "Epoch 100/600\n",
      "14126/14126 [==============================] - 1s 66us/step - loss: 0.3068 - val_loss: 0.2817\n",
      "Epoch 101/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.3041 - val_loss: 0.2858\n",
      "Epoch 102/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.3052 - val_loss: 0.2824\n",
      "Epoch 103/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.3035 - val_loss: 0.2786\n",
      "Epoch 104/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.3057 - val_loss: 0.2906\n",
      "Epoch 105/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3048 - val_loss: 0.2814\n",
      "Epoch 106/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3038 - val_loss: 0.2867\n",
      "Epoch 107/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3024 - val_loss: 0.2774\n",
      "Epoch 108/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3023 - val_loss: 0.2779\n",
      "Epoch 109/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3026 - val_loss: 0.3034\n",
      "Epoch 110/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3042 - val_loss: 0.2797\n",
      "Epoch 111/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.3019 - val_loss: 0.2875\n",
      "Epoch 112/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3017 - val_loss: 0.2781\n",
      "Epoch 113/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3021 - val_loss: 0.2797\n",
      "Epoch 114/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.3034 - val_loss: 0.2808\n",
      "Epoch 115/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3009 - val_loss: 0.2864\n",
      "Epoch 116/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3010 - val_loss: 0.2741\n",
      "Epoch 117/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3006 - val_loss: 0.2757\n",
      "Epoch 118/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.3008 - val_loss: 0.2837\n",
      "Epoch 119/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3001 - val_loss: 0.2784\n",
      "Epoch 120/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.3021 - val_loss: 0.3016\n",
      "Epoch 121/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3007 - val_loss: 0.2789\n",
      "Epoch 122/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.3002 - val_loss: 0.2758\n",
      "Epoch 123/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2998 - val_loss: 0.2876\n",
      "Epoch 124/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2984 - val_loss: 0.3064\n",
      "Epoch 125/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.3006 - val_loss: 0.2728\n",
      "Epoch 126/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2980 - val_loss: 0.2739\n",
      "Epoch 127/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2976 - val_loss: 0.2732\n",
      "Epoch 128/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2972 - val_loss: 0.2804\n",
      "Epoch 129/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2968 - val_loss: 0.2815\n",
      "Epoch 130/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2982 - val_loss: 0.2755\n",
      "Epoch 131/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2975 - val_loss: 0.2739\n",
      "Epoch 132/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2978 - val_loss: 0.2732\n",
      "Epoch 133/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2970 - val_loss: 0.2711\n",
      "Epoch 134/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2973 - val_loss: 0.2749\n",
      "Epoch 135/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2968 - val_loss: 0.2734\n",
      "Epoch 136/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2952 - val_loss: 0.2708\n",
      "Epoch 137/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2949 - val_loss: 0.2709\n",
      "Epoch 138/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2957 - val_loss: 0.2720\n",
      "Epoch 139/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2964 - val_loss: 0.2730\n",
      "Epoch 140/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2958 - val_loss: 0.2717\n",
      "Epoch 141/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2974 - val_loss: 0.2800\n",
      "Epoch 142/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2959 - val_loss: 0.2752\n",
      "Epoch 143/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2953 - val_loss: 0.2691\n",
      "Epoch 144/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2942 - val_loss: 0.2776\n",
      "Epoch 145/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2941 - val_loss: 0.2730\n",
      "Epoch 146/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2937 - val_loss: 0.2742\n",
      "Epoch 147/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2927 - val_loss: 0.2714\n",
      "Epoch 148/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2931 - val_loss: 0.2717\n",
      "Epoch 149/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2927 - val_loss: 0.2746\n",
      "Epoch 150/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2931 - val_loss: 0.2758\n",
      "Epoch 151/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2919 - val_loss: 0.2802\n",
      "Epoch 152/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2924 - val_loss: 0.2690\n",
      "Epoch 153/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2929 - val_loss: 0.2764\n",
      "Epoch 154/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2936 - val_loss: 0.2693\n",
      "Epoch 155/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2910 - val_loss: 0.2790\n",
      "Epoch 156/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2949 - val_loss: 0.2680\n",
      "Epoch 157/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2914 - val_loss: 0.2690\n",
      "Epoch 158/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2934 - val_loss: 0.2697\n",
      "Epoch 159/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2911 - val_loss: 0.2770\n",
      "Epoch 160/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2906 - val_loss: 0.2692\n",
      "Epoch 161/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2920 - val_loss: 0.2701\n",
      "Epoch 162/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2907 - val_loss: 0.2817\n",
      "Epoch 163/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2896 - val_loss: 0.2781\n",
      "Epoch 164/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2909 - val_loss: 0.2742\n",
      "Epoch 165/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2896 - val_loss: 0.2862\n",
      "Epoch 166/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2904 - val_loss: 0.2784\n",
      "Epoch 167/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2909 - val_loss: 0.2772\n",
      "Epoch 168/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2920 - val_loss: 0.2725\n",
      "Epoch 169/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2887 - val_loss: 0.2736\n",
      "Epoch 170/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2882 - val_loss: 0.2730\n",
      "Epoch 171/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2888 - val_loss: 0.2728\n",
      "Epoch 172/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2892 - val_loss: 0.2717\n",
      "Epoch 173/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2878 - val_loss: 0.2768\n",
      "Epoch 174/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2891 - val_loss: 0.2793\n",
      "Epoch 175/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2893 - val_loss: 0.2680\n",
      "Epoch 176/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2883 - val_loss: 0.2708\n",
      "Epoch 177/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2878 - val_loss: 0.2700\n",
      "Epoch 178/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2867 - val_loss: 0.2762\n",
      "Epoch 179/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2871 - val_loss: 0.2670\n",
      "Epoch 180/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2887 - val_loss: 0.2664\n",
      "Epoch 181/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2871 - val_loss: 0.2686\n",
      "Epoch 182/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2875 - val_loss: 0.2665\n",
      "Epoch 183/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2880 - val_loss: 0.2721\n",
      "Epoch 184/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2874 - val_loss: 0.2726\n",
      "Epoch 185/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2861 - val_loss: 0.2662\n",
      "Epoch 186/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2866 - val_loss: 0.2701\n",
      "Epoch 187/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2869 - val_loss: 0.2690\n",
      "Epoch 188/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2857 - val_loss: 0.2719\n",
      "Epoch 189/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2859 - val_loss: 0.2673\n",
      "Epoch 190/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2869 - val_loss: 0.2652\n",
      "Epoch 191/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2855 - val_loss: 0.2709\n",
      "Epoch 192/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2866 - val_loss: 0.2714\n",
      "Epoch 193/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2858 - val_loss: 0.2746\n",
      "Epoch 194/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2861 - val_loss: 0.2710\n",
      "Epoch 195/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2852 - val_loss: 0.2742\n",
      "Epoch 196/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2849 - val_loss: 0.2673\n",
      "Epoch 197/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2847 - val_loss: 0.2682\n",
      "Epoch 198/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2853 - val_loss: 0.2713\n",
      "Epoch 199/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2849 - val_loss: 0.2653\n",
      "Epoch 200/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2855 - val_loss: 0.2670\n",
      "Epoch 201/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2858 - val_loss: 0.2659\n",
      "Epoch 202/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2838 - val_loss: 0.2664\n",
      "Epoch 203/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2845 - val_loss: 0.2643\n",
      "Epoch 204/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2836 - val_loss: 0.2878\n",
      "Epoch 205/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2846 - val_loss: 0.2643\n",
      "Epoch 206/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2843 - val_loss: 0.2655\n",
      "Epoch 207/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2827 - val_loss: 0.2656\n",
      "Epoch 208/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2830 - val_loss: 0.2648\n",
      "Epoch 209/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2839 - val_loss: 0.2663\n",
      "Epoch 210/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2830 - val_loss: 0.2657\n",
      "Epoch 211/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2834 - val_loss: 0.2758\n",
      "Epoch 212/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2839 - val_loss: 0.2643\n",
      "Epoch 213/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2835 - val_loss: 0.2727\n",
      "Epoch 214/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2830 - val_loss: 0.2631\n",
      "Epoch 215/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2833 - val_loss: 0.2660\n",
      "Epoch 216/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2835 - val_loss: 0.2650\n",
      "Epoch 217/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2828 - val_loss: 0.2644\n",
      "Epoch 218/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2821 - val_loss: 0.2678\n",
      "Epoch 219/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2822 - val_loss: 0.2731\n",
      "Epoch 220/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2820 - val_loss: 0.2685\n",
      "Epoch 221/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2812 - val_loss: 0.2634\n",
      "Epoch 222/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2821 - val_loss: 0.2654\n",
      "Epoch 223/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2807 - val_loss: 0.2770\n",
      "Epoch 224/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2808 - val_loss: 0.2662\n",
      "Epoch 225/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2817 - val_loss: 0.2647\n",
      "Epoch 226/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2814 - val_loss: 0.2655\n",
      "Epoch 227/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2797 - val_loss: 0.2636\n",
      "Epoch 228/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2807 - val_loss: 0.2633\n",
      "Epoch 229/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2801 - val_loss: 0.2659\n",
      "Epoch 230/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2800 - val_loss: 0.2692\n",
      "Epoch 231/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2799 - val_loss: 0.2647\n",
      "Epoch 232/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2805 - val_loss: 0.2723\n",
      "Epoch 233/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2792 - val_loss: 0.2647\n",
      "Epoch 234/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2791 - val_loss: 0.2629\n",
      "Epoch 235/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2805 - val_loss: 0.2734\n",
      "Epoch 236/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2799 - val_loss: 0.2618\n",
      "Epoch 237/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2798 - val_loss: 0.2642\n",
      "Epoch 238/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2791 - val_loss: 0.2624\n",
      "Epoch 239/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2787 - val_loss: 0.2634\n",
      "Epoch 240/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2794 - val_loss: 0.2634\n",
      "Epoch 241/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2775 - val_loss: 0.2812\n",
      "Epoch 242/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2806 - val_loss: 0.2632\n",
      "Epoch 243/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2802 - val_loss: 0.2636\n",
      "Epoch 244/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2798 - val_loss: 0.2637\n",
      "Epoch 245/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2800 - val_loss: 0.2656\n",
      "Epoch 246/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2786 - val_loss: 0.2681\n",
      "Epoch 247/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2780 - val_loss: 0.2707\n",
      "Epoch 248/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2799 - val_loss: 0.2803\n",
      "Epoch 249/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2784 - val_loss: 0.2893\n",
      "Epoch 250/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2787 - val_loss: 0.2602\n",
      "Epoch 251/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2781 - val_loss: 0.2646\n",
      "Epoch 252/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2765 - val_loss: 0.2638\n",
      "Epoch 253/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2778 - val_loss: 0.2648\n",
      "Epoch 254/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2786 - val_loss: 0.2613\n",
      "Epoch 255/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2783 - val_loss: 0.2622\n",
      "Epoch 256/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2771 - val_loss: 0.2614\n",
      "Epoch 257/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2762 - val_loss: 0.2688\n",
      "Epoch 258/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2768 - val_loss: 0.2655\n",
      "Epoch 259/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2786 - val_loss: 0.2611\n",
      "Epoch 260/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2755 - val_loss: 0.2901\n",
      "Epoch 261/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2772 - val_loss: 0.2664\n",
      "Epoch 262/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2767 - val_loss: 0.2637\n",
      "Epoch 263/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2771 - val_loss: 0.2628\n",
      "Epoch 264/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2751 - val_loss: 0.2685\n",
      "Epoch 265/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2761 - val_loss: 0.2691\n",
      "Epoch 266/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2763 - val_loss: 0.2626\n",
      "Epoch 267/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2769 - val_loss: 0.2643\n",
      "Epoch 268/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2753 - val_loss: 0.2670\n",
      "Epoch 269/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2756 - val_loss: 0.2651\n",
      "Epoch 270/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2768 - val_loss: 0.2743\n",
      "Epoch 271/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2760 - val_loss: 0.2607\n",
      "Epoch 272/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2756 - val_loss: 0.2598\n",
      "Epoch 273/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2760 - val_loss: 0.2672\n",
      "Epoch 274/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2765 - val_loss: 0.2606\n",
      "Epoch 275/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2752 - val_loss: 0.2646\n",
      "Epoch 276/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2753 - val_loss: 0.2658\n",
      "Epoch 277/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2755 - val_loss: 0.2611\n",
      "Epoch 278/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2765 - val_loss: 0.2640\n",
      "Epoch 279/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2758 - val_loss: 0.2749\n",
      "Epoch 280/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2758 - val_loss: 0.2652\n",
      "Epoch 281/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2743 - val_loss: 0.2610\n",
      "Epoch 282/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2751 - val_loss: 0.2714\n",
      "Epoch 283/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2753 - val_loss: 0.2597\n",
      "Epoch 284/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2745 - val_loss: 0.2638\n",
      "Epoch 285/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2742 - val_loss: 0.2733\n",
      "Epoch 286/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2766 - val_loss: 0.2620\n",
      "Epoch 287/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2739 - val_loss: 0.2644\n",
      "Epoch 288/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2731 - val_loss: 0.2669\n",
      "Epoch 289/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2747 - val_loss: 0.2603\n",
      "Epoch 290/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2741 - val_loss: 0.2600\n",
      "Epoch 291/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2740 - val_loss: 0.2611\n",
      "Epoch 292/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2747 - val_loss: 0.2613\n",
      "Epoch 293/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2744 - val_loss: 0.2616\n",
      "Epoch 294/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2740 - val_loss: 0.2594\n",
      "Epoch 295/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2732 - val_loss: 0.2589\n",
      "Epoch 296/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2726 - val_loss: 0.2635\n",
      "Epoch 297/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2732 - val_loss: 0.2628\n",
      "Epoch 298/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2738 - val_loss: 0.2629\n",
      "Epoch 299/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2747 - val_loss: 0.2690\n",
      "Epoch 300/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2729 - val_loss: 0.2692\n",
      "Epoch 301/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2744 - val_loss: 0.2648\n",
      "Epoch 302/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2736 - val_loss: 0.2610\n",
      "Epoch 303/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2729 - val_loss: 0.2617\n",
      "Epoch 304/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2740 - val_loss: 0.2749\n",
      "Epoch 305/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2734 - val_loss: 0.2624\n",
      "Epoch 306/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2738 - val_loss: 0.2602\n",
      "Epoch 307/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2734 - val_loss: 0.2600\n",
      "Epoch 308/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2715 - val_loss: 0.2645\n",
      "Epoch 309/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2718 - val_loss: 0.2587\n",
      "Epoch 310/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2724 - val_loss: 0.2618\n",
      "Epoch 311/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2728 - val_loss: 0.2715\n",
      "Epoch 312/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2727 - val_loss: 0.2624\n",
      "Epoch 313/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2730 - val_loss: 0.2670\n",
      "Epoch 314/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2725 - val_loss: 0.2612\n",
      "Epoch 315/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2723 - val_loss: 0.2621\n",
      "Epoch 316/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2723 - val_loss: 0.2592\n",
      "Epoch 317/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2716 - val_loss: 0.2614\n",
      "Epoch 318/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2717 - val_loss: 0.2641\n",
      "Epoch 319/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2722 - val_loss: 0.2588\n",
      "Epoch 320/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2721 - val_loss: 0.2635\n",
      "Epoch 321/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2719 - val_loss: 0.2596\n",
      "Epoch 322/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2719 - val_loss: 0.2627\n",
      "Epoch 323/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2724 - val_loss: 0.2653\n",
      "Epoch 324/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2720 - val_loss: 0.2695\n",
      "Epoch 325/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2712 - val_loss: 0.2612\n",
      "Epoch 326/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2714 - val_loss: 0.2759\n",
      "Epoch 327/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2726 - val_loss: 0.2611\n",
      "Epoch 328/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2711 - val_loss: 0.2594\n",
      "Epoch 329/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2703 - val_loss: 0.2642\n",
      "Epoch 330/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2722 - val_loss: 0.2600\n",
      "Epoch 331/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2707 - val_loss: 0.2612\n",
      "Epoch 332/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2728 - val_loss: 0.2662\n",
      "Epoch 333/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2722 - val_loss: 0.2617\n",
      "Epoch 334/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2714 - val_loss: 0.2607\n",
      "Epoch 335/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2721 - val_loss: 0.2633\n",
      "Epoch 336/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2710 - val_loss: 0.2636\n",
      "Epoch 337/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2707 - val_loss: 0.2586\n",
      "Epoch 338/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2704 - val_loss: 0.2592\n",
      "Epoch 339/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2718 - val_loss: 0.2614\n",
      "Epoch 340/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2703 - val_loss: 0.2604\n",
      "Epoch 341/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2700 - val_loss: 0.2741\n",
      "Epoch 342/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2704 - val_loss: 0.2641\n",
      "Epoch 343/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2703 - val_loss: 0.2597\n",
      "Epoch 344/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2708 - val_loss: 0.2734\n",
      "Epoch 345/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2705 - val_loss: 0.2582\n",
      "Epoch 346/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2700 - val_loss: 0.2582\n",
      "Epoch 347/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2702 - val_loss: 0.2580\n",
      "Epoch 348/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2704 - val_loss: 0.2594\n",
      "Epoch 349/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2700 - val_loss: 0.2677\n",
      "Epoch 350/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2702 - val_loss: 0.2582\n",
      "Epoch 351/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2699 - val_loss: 0.2610\n",
      "Epoch 352/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2692 - val_loss: 0.2616\n",
      "Epoch 353/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2697 - val_loss: 0.2598\n",
      "Epoch 354/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2702 - val_loss: 0.2570\n",
      "Epoch 355/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2679 - val_loss: 0.2644\n",
      "Epoch 356/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2701 - val_loss: 0.2584\n",
      "Epoch 357/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2693 - val_loss: 0.2608\n",
      "Epoch 358/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2698 - val_loss: 0.2592\n",
      "Epoch 359/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2693 - val_loss: 0.2599\n",
      "Epoch 360/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2688 - val_loss: 0.2588\n",
      "Epoch 361/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2698 - val_loss: 0.2574\n",
      "Epoch 362/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2689 - val_loss: 0.2582\n",
      "Epoch 363/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2687 - val_loss: 0.2580\n",
      "Epoch 364/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2683 - val_loss: 0.2599\n",
      "Epoch 365/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2683 - val_loss: 0.2684\n",
      "Epoch 366/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2683 - val_loss: 0.2654\n",
      "Epoch 367/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2690 - val_loss: 0.2597\n",
      "Epoch 368/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2688 - val_loss: 0.2714\n",
      "Epoch 369/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2702 - val_loss: 0.2680\n",
      "Epoch 370/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2682 - val_loss: 0.2684\n",
      "Epoch 371/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2682 - val_loss: 0.2578\n",
      "Epoch 372/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2687 - val_loss: 0.2588\n",
      "Epoch 373/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2684 - val_loss: 0.2587\n",
      "Epoch 374/600\n",
      "14126/14126 [==============================] - 1s 53us/step - loss: 0.2687 - val_loss: 0.2583\n",
      "Epoch 375/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2689 - val_loss: 0.2643\n",
      "Epoch 376/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2678 - val_loss: 0.2651\n",
      "Epoch 377/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2676 - val_loss: 0.2581\n",
      "Epoch 378/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2681 - val_loss: 0.2573\n",
      "Epoch 379/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2687 - val_loss: 0.2600\n",
      "Epoch 380/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2686 - val_loss: 0.2571\n",
      "Epoch 381/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2684 - val_loss: 0.2627\n",
      "Epoch 382/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2680 - val_loss: 0.2641\n",
      "Epoch 383/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2677 - val_loss: 0.2601\n",
      "Epoch 384/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2683 - val_loss: 0.2620\n",
      "Epoch 385/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2681 - val_loss: 0.2650\n",
      "Epoch 386/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2673 - val_loss: 0.2609\n",
      "Epoch 387/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2677 - val_loss: 0.2596\n",
      "Epoch 388/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2668 - val_loss: 0.2661\n",
      "Epoch 389/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2678 - val_loss: 0.2607\n",
      "Epoch 390/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2668 - val_loss: 0.2615\n",
      "Epoch 391/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2674 - val_loss: 0.2558\n",
      "Epoch 392/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2670 - val_loss: 0.2594\n",
      "Epoch 393/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2687 - val_loss: 0.2581\n",
      "Epoch 394/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2668 - val_loss: 0.2609\n",
      "Epoch 395/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2667 - val_loss: 0.2606\n",
      "Epoch 396/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2663 - val_loss: 0.2600\n",
      "Epoch 397/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2665 - val_loss: 0.2583\n",
      "Epoch 398/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2673 - val_loss: 0.2557\n",
      "Epoch 399/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2682 - val_loss: 0.2586\n",
      "Epoch 400/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2672 - val_loss: 0.2614\n",
      "Epoch 401/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2662 - val_loss: 0.2572\n",
      "Epoch 402/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2666 - val_loss: 0.2588\n",
      "Epoch 403/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2658 - val_loss: 0.2616\n",
      "Epoch 404/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2661 - val_loss: 0.2559\n",
      "Epoch 405/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2671 - val_loss: 0.2624\n",
      "Epoch 406/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2671 - val_loss: 0.2627\n",
      "Epoch 407/600\n",
      "14126/14126 [==============================] - 1s 65us/step - loss: 0.2670 - val_loss: 0.2583\n",
      "Epoch 408/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2662 - val_loss: 0.2575\n",
      "Epoch 409/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2669 - val_loss: 0.2569\n",
      "Epoch 410/600\n",
      "14126/14126 [==============================] - 1s 65us/step - loss: 0.2661 - val_loss: 0.2579\n",
      "Epoch 411/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2671 - val_loss: 0.2566\n",
      "Epoch 412/600\n",
      "14126/14126 [==============================] - 1s 66us/step - loss: 0.2655 - val_loss: 0.2571\n",
      "Epoch 413/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2660 - val_loss: 0.2653\n",
      "Epoch 414/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2649 - val_loss: 0.2644\n",
      "Epoch 415/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2651 - val_loss: 0.2589\n",
      "Epoch 416/600\n",
      "14126/14126 [==============================] - 1s 66us/step - loss: 0.2660 - val_loss: 0.2573\n",
      "Epoch 417/600\n",
      "14126/14126 [==============================] - 1s 69us/step - loss: 0.2661 - val_loss: 0.2661\n",
      "Epoch 418/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2671 - val_loss: 0.2588\n",
      "Epoch 419/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2656 - val_loss: 0.2587\n",
      "Epoch 420/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2656 - val_loss: 0.2579\n",
      "Epoch 421/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2649 - val_loss: 0.2551\n",
      "Epoch 422/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2656 - val_loss: 0.2575\n",
      "Epoch 423/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2659 - val_loss: 0.2567\n",
      "Epoch 424/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2652 - val_loss: 0.2641\n",
      "Epoch 425/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2643 - val_loss: 0.2628\n",
      "Epoch 426/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2652 - val_loss: 0.2631\n",
      "Epoch 427/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2661 - val_loss: 0.2616\n",
      "Epoch 428/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2649 - val_loss: 0.2585\n",
      "Epoch 429/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2648 - val_loss: 0.2583\n",
      "Epoch 430/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2649 - val_loss: 0.2603\n",
      "Epoch 431/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2653 - val_loss: 0.2686\n",
      "Epoch 432/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2656 - val_loss: 0.2592\n",
      "Epoch 433/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2645 - val_loss: 0.2578\n",
      "Epoch 434/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2644 - val_loss: 0.2571\n",
      "Epoch 435/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2645 - val_loss: 0.2563\n",
      "Epoch 436/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2645 - val_loss: 0.2565\n",
      "Epoch 437/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2643 - val_loss: 0.2614\n",
      "Epoch 438/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2636 - val_loss: 0.2581\n",
      "Epoch 439/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2647 - val_loss: 0.2568\n",
      "Epoch 440/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2646 - val_loss: 0.2594\n",
      "Epoch 441/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2648 - val_loss: 0.2628\n",
      "Epoch 442/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2644 - val_loss: 0.2572\n",
      "Epoch 443/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2646 - val_loss: 0.2583\n",
      "Epoch 444/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2645 - val_loss: 0.2612\n",
      "Epoch 445/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2643 - val_loss: 0.2645\n",
      "Epoch 446/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2639 - val_loss: 0.2687\n",
      "Epoch 447/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2646 - val_loss: 0.2670\n",
      "Epoch 448/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2632 - val_loss: 0.2572\n",
      "Epoch 449/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2641 - val_loss: 0.2559\n",
      "Epoch 450/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2640 - val_loss: 0.2563\n",
      "Epoch 451/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2641 - val_loss: 0.2560\n",
      "Epoch 452/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2635 - val_loss: 0.2829\n",
      "Epoch 453/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2649 - val_loss: 0.2622\n",
      "Epoch 454/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2635 - val_loss: 0.2558\n",
      "Epoch 455/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2650 - val_loss: 0.2565\n",
      "Epoch 456/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2645 - val_loss: 0.2587\n",
      "Epoch 457/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2648 - val_loss: 0.2575\n",
      "Epoch 458/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2631 - val_loss: 0.2693\n",
      "Epoch 459/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2630 - val_loss: 0.2573\n",
      "Epoch 460/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2643 - val_loss: 0.2585\n",
      "Epoch 461/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2621 - val_loss: 0.2578\n",
      "Epoch 462/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2631 - val_loss: 0.2560\n",
      "Epoch 463/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2643 - val_loss: 0.2581\n",
      "Epoch 464/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2624 - val_loss: 0.2575\n",
      "Epoch 465/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2628 - val_loss: 0.2579\n",
      "Epoch 466/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2626 - val_loss: 0.2601\n",
      "Epoch 467/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2630 - val_loss: 0.2606\n",
      "Epoch 468/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2634 - val_loss: 0.2568\n",
      "Epoch 469/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2619 - val_loss: 0.2655\n",
      "Epoch 470/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2638 - val_loss: 0.2579\n",
      "Epoch 471/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2624 - val_loss: 0.2606\n",
      "Epoch 472/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2634 - val_loss: 0.2594\n",
      "Epoch 473/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2624 - val_loss: 0.2588\n",
      "Epoch 474/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2633 - val_loss: 0.2637\n",
      "Epoch 475/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2623 - val_loss: 0.2598\n",
      "Epoch 476/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2627 - val_loss: 0.2565\n",
      "Epoch 477/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2622 - val_loss: 0.2595\n",
      "Epoch 478/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2626 - val_loss: 0.2644\n",
      "Epoch 479/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2629 - val_loss: 0.2612\n",
      "Epoch 480/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2630 - val_loss: 0.2587\n",
      "Epoch 481/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2628 - val_loss: 0.2574\n",
      "Epoch 482/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2628 - val_loss: 0.2725\n",
      "Epoch 483/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2630 - val_loss: 0.2615\n",
      "Epoch 484/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2625 - val_loss: 0.2567\n",
      "Epoch 485/600\n",
      "14126/14126 [==============================] - 1s 63us/step - loss: 0.2624 - val_loss: 0.2582\n",
      "Epoch 486/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2617 - val_loss: 0.2593\n",
      "Epoch 487/600\n",
      "14126/14126 [==============================] - 1s 65us/step - loss: 0.2629 - val_loss: 0.2569\n",
      "Epoch 488/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2635 - val_loss: 0.2630\n",
      "Epoch 489/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2623 - val_loss: 0.2561\n",
      "Epoch 490/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2628 - val_loss: 0.2619\n",
      "Epoch 491/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2612 - val_loss: 0.2647\n",
      "Epoch 492/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2623 - val_loss: 0.2576\n",
      "Epoch 493/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2629 - val_loss: 0.2558\n",
      "Epoch 494/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2619 - val_loss: 0.2578\n",
      "Epoch 495/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2630 - val_loss: 0.2574\n",
      "Epoch 496/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2601 - val_loss: 0.2617\n",
      "Epoch 497/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2617 - val_loss: 0.2579\n",
      "Epoch 498/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2629 - val_loss: 0.2597\n",
      "Epoch 499/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2617 - val_loss: 0.2564\n",
      "Epoch 500/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2613 - val_loss: 0.2613\n",
      "Epoch 501/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2617 - val_loss: 0.2668\n",
      "Epoch 502/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2620 - val_loss: 0.2573\n",
      "Epoch 503/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2622 - val_loss: 0.2581\n",
      "Epoch 504/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2614 - val_loss: 0.2638\n",
      "Epoch 505/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2614 - val_loss: 0.2552\n",
      "Epoch 506/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2612 - val_loss: 0.2687\n",
      "Epoch 507/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2613 - val_loss: 0.2567\n",
      "Epoch 508/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2606 - val_loss: 0.2579\n",
      "Epoch 509/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2613 - val_loss: 0.2596\n",
      "Epoch 510/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2609 - val_loss: 0.2559\n",
      "Epoch 511/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2610 - val_loss: 0.2580\n",
      "Epoch 512/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2608 - val_loss: 0.2597\n",
      "Epoch 513/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2618 - val_loss: 0.2606\n",
      "Epoch 514/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2606 - val_loss: 0.2613\n",
      "Epoch 515/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2615 - val_loss: 0.2587\n",
      "Epoch 516/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2608 - val_loss: 0.2556\n",
      "Epoch 517/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2619 - val_loss: 0.2590\n",
      "Epoch 518/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2613 - val_loss: 0.2556\n",
      "Epoch 519/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2607 - val_loss: 0.2569\n",
      "Epoch 520/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2603 - val_loss: 0.2587\n",
      "Epoch 521/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2613 - val_loss: 0.2576\n",
      "Epoch 522/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2605 - val_loss: 0.2568\n",
      "Epoch 523/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2600 - val_loss: 0.2565\n",
      "Epoch 524/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2619 - val_loss: 0.2555\n",
      "Epoch 525/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2606 - val_loss: 0.2576\n",
      "Epoch 526/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2609 - val_loss: 0.2618\n",
      "Epoch 527/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2624 - val_loss: 0.2636\n",
      "Epoch 528/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2599 - val_loss: 0.2577\n",
      "Epoch 529/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2604 - val_loss: 0.2551\n",
      "Epoch 530/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2608 - val_loss: 0.2576\n",
      "Epoch 531/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2603 - val_loss: 0.2568\n",
      "Epoch 532/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2607 - val_loss: 0.2581\n",
      "Epoch 533/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2608 - val_loss: 0.2559\n",
      "Epoch 534/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2600 - val_loss: 0.2565\n",
      "Epoch 535/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2603 - val_loss: 0.2588\n",
      "Epoch 536/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2602 - val_loss: 0.2560\n",
      "Epoch 537/600\n",
      "14126/14126 [==============================] - 1s 61us/step - loss: 0.2596 - val_loss: 0.2545\n",
      "Epoch 538/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2608 - val_loss: 0.2572\n",
      "Epoch 539/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2606 - val_loss: 0.2612\n",
      "Epoch 540/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2603 - val_loss: 0.2590\n",
      "Epoch 541/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2602 - val_loss: 0.2602\n",
      "Epoch 542/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2593 - val_loss: 0.2556\n",
      "Epoch 543/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2599 - val_loss: 0.2585\n",
      "Epoch 544/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2608 - val_loss: 0.2625\n",
      "Epoch 545/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2600 - val_loss: 0.2622\n",
      "Epoch 546/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2594 - val_loss: 0.2592\n",
      "Epoch 547/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2598 - val_loss: 0.2586\n",
      "Epoch 548/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2597 - val_loss: 0.2570\n",
      "Epoch 549/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2595 - val_loss: 0.2588\n",
      "Epoch 550/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2600 - val_loss: 0.2603\n",
      "Epoch 551/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2595 - val_loss: 0.2558\n",
      "Epoch 552/600\n",
      "14126/14126 [==============================] - 1s 54us/step - loss: 0.2597 - val_loss: 0.2674\n",
      "Epoch 553/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2602 - val_loss: 0.2571\n",
      "Epoch 554/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2594 - val_loss: 0.2548\n",
      "Epoch 555/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2594 - val_loss: 0.2610\n",
      "Epoch 556/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2586 - val_loss: 0.2619\n",
      "Epoch 557/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2598 - val_loss: 0.2569\n",
      "Epoch 558/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2591 - val_loss: 0.2578\n",
      "Epoch 559/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2601 - val_loss: 0.2614\n",
      "Epoch 560/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2589 - val_loss: 0.2602\n",
      "Epoch 561/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2599 - val_loss: 0.2551\n",
      "Epoch 562/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2591 - val_loss: 0.2555\n",
      "Epoch 563/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2586 - val_loss: 0.2567\n",
      "Epoch 564/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2591 - val_loss: 0.2575\n",
      "Epoch 565/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2590 - val_loss: 0.2691\n",
      "Epoch 566/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2587 - val_loss: 0.2579\n",
      "Epoch 567/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2587 - val_loss: 0.2558\n",
      "Epoch 568/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2596 - val_loss: 0.2596\n",
      "Epoch 569/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2585 - val_loss: 0.2562\n",
      "Epoch 570/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2592 - val_loss: 0.2568\n",
      "Epoch 571/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2587 - val_loss: 0.2561\n",
      "Epoch 572/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2582 - val_loss: 0.2593\n",
      "Epoch 573/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2591 - val_loss: 0.2568\n",
      "Epoch 574/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2602 - val_loss: 0.2556\n",
      "Epoch 575/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2585 - val_loss: 0.2549\n",
      "Epoch 576/600\n",
      "14126/14126 [==============================] - 1s 64us/step - loss: 0.2588 - val_loss: 0.2568\n",
      "Epoch 577/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2580 - val_loss: 0.2640\n",
      "Epoch 578/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2581 - val_loss: 0.2647\n",
      "Epoch 579/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2610 - val_loss: 0.2618\n",
      "Epoch 580/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2583 - val_loss: 0.2578\n",
      "Epoch 581/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2588 - val_loss: 0.2610\n",
      "Epoch 582/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2599 - val_loss: 0.2565\n",
      "Epoch 583/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2585 - val_loss: 0.2561\n",
      "Epoch 584/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2582 - val_loss: 0.2596\n",
      "Epoch 585/600\n",
      "14126/14126 [==============================] - 1s 62us/step - loss: 0.2585 - val_loss: 0.2559\n",
      "Epoch 586/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2583 - val_loss: 0.2564\n",
      "Epoch 587/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2586 - val_loss: 0.2595\n",
      "Epoch 588/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2581 - val_loss: 0.2804\n",
      "Epoch 589/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2576 - val_loss: 0.2627\n",
      "Epoch 590/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2583 - val_loss: 0.2568\n",
      "Epoch 591/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2579 - val_loss: 0.2565\n",
      "Epoch 592/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2592 - val_loss: 0.2552\n",
      "Epoch 593/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2574 - val_loss: 0.2642\n",
      "Epoch 594/600\n",
      "14126/14126 [==============================] - 1s 57us/step - loss: 0.2582 - val_loss: 0.2654\n",
      "Epoch 595/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2578 - val_loss: 0.2562\n",
      "Epoch 596/600\n",
      "14126/14126 [==============================] - 1s 55us/step - loss: 0.2582 - val_loss: 0.2560\n",
      "Epoch 597/600\n",
      "14126/14126 [==============================] - 1s 59us/step - loss: 0.2577 - val_loss: 0.2758\n",
      "Epoch 598/600\n",
      "14126/14126 [==============================] - 1s 60us/step - loss: 0.2580 - val_loss: 0.2550\n",
      "Epoch 599/600\n",
      "14126/14126 [==============================] - 1s 56us/step - loss: 0.2585 - val_loss: 0.2610\n",
      "Epoch 600/600\n",
      "14126/14126 [==============================] - 1s 58us/step - loss: 0.2581 - val_loss: 0.2577\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAELCAYAAAAx94awAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngr8QQrig/wcZv5tTiSTcrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(8,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))\n",
    "summarize_diagnostics(history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Untitled1.ipynb",
   "provenance": [
    {
     "file_id": "1oFtqCo79iZXLsi-Ub8tdx-4PL9vxwLmX",
     "timestamp": 1572341199940
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
